from PIL import Image
from jet.llm.mlx.adapters.mlx_llama_index_llm_adapter import MLXLlamaIndexLLMAdapter
from jet.logger import CustomLogger
from jet.models.config import MODELS_CACHE_DIR
from llama_index.core import VectorStoreIndex
from llama_index.core.node_parser import UnstructuredElementNodeParser
from llama_index.core.schema import ImageDocument
from llama_index.core.settings import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.multi_modal_llms.replicate import ReplicateMultiModal
from llama_index.multi_modal_llms.replicate.base import (
REPLICATE_MULTI_MODAL_LLM_MODELS,
)
from llama_index.readers.file import FlatReader
from pathlib import Path
from unstructured.partition.html import partition_html
import matplotlib.pyplot as plt
import openai
import os
import pandas as pd
import pickle
import shutil


OUTPUT_DIR = os.path.join(
    os.path.dirname(__file__), "generated", os.path.splitext(os.path.basename(__file__))[0])
shutil.rmtree(OUTPUT_DIR, ignore_errors=True)
log_file = os.path.join(OUTPUT_DIR, "main.log")
logger = CustomLogger(log_file, overwrite=True)
logger.info(f"Logs: {log_file}")

model_name = "sentence-transformers/all-MiniLM-L6-v2"
Settings.embed_model = HuggingFaceEmbedding(
    model_name=model_name,
    cache_folder=MODELS_CACHE_DIR,
)


"""
<a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/multi_modal/llava_multi_modal_tesla_10q.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
# Retrieval-Augmented Image Captioning

In this example, we show how to leverage [LLaVa + Replicate](https://replicate.com/yorickvp/llava-13b) for image understanding/captioning and retrieve relevant unstructured text and embedded tables from Tesla 10K file according to the image understanding.

1. LlaVa can provide image understanding based on user prompt.
2. We use Unstructured to parse out the tables, and use LlamaIndex recursive retrieval to index/retrieve tables and texts.
3. We can leverage the image understanding from Step 1 to retrieve relevant information from knowledge base generated by Step 2 (which is indexed by LlamaIndex)

Context for LLaVA: Large Language and Vision Assistant
* [Website](https://llava-vl.github.io/)
* [Paper](https://arxiv.org/abs/2304.08485)
* [Github](https://github.com/haotian-liu/LLaVA)
* LLaVA is now supported in llama.cpp with 4-bit / 5-bit quantization support: [See here.](https://github.com/ggerganov/llama.cpp/pull/3436) [Deprecated]
* LLaVA 13b is now supported in Replicate: [See here.](https://replicate.com/yorickvp/llava-13b)

For LlamaIndex:
LlaVa+Replicate enables us to run image understanding locally and combine the multi-modal knowledge with our RAG knowledge base system.

TODO:
Waiting for [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) supporting LlaVa model in python wrapper.
So LlamaIndex can leverage `LlamaCPP` class for serving LlaVa model directly/locally.

## Using Replicate serving LLaVa model through LlamaIndex

### Build and Run LLaVa models locally through Llama.cpp (Deprecated)

1. git clone [https://github.com/ggerganov/llama.cpp.git](https://github.com/ggerganov/llama.cpp.git)
2. `cd llama.cpp`. Checkout llama.cpp repo for more details.
3. `make`
4. Download Llava models including `ggml-model-*` and `mmproj-model-*` from [this Hugging Face repo](https://huggingface.co/mys/ggml_llava-v1.5-7b/tree/main). Please select one model based on your own local configuration
5. `./llava` for checking whether llava is running locally
"""
logger.info("# Retrieval-Augmented Image Captioning")

# %pip install llama-index-readers-file
# %pip install llama-index-multi-modal-llms-replicate

# %load_ext autoreload
# % autoreload 2

# !pip install unstructured


pd.set_option("display.max_rows", None)
pd.set_option("display.max_columns", None)
pd.set_option("display.width", None)
pd.set_option("display.max_colwidth", None)

"""
## Perform Data Extraction from Tesla 10K file


In these sections we use Unstructured to parse out the table and non-table elements.

### Extract Elements

We use Unstructured to extract table and non-table elements from the 10-K filing.
"""
logger.info("## Perform Data Extraction from Tesla 10K file")

# !wget "https://www.dropbox.com/scl/fi/mlaymdy1ni1ovyeykhhuk/tesla_2021_10k.htm?rlkey=qf9k4zn0ejrbm716j0gg7r802&dl=1" -O tesla_2021_10k.htm
# !wget "https://docs.google.com/uc?export=download&id=1THe1qqM61lretr9N3BmINc_NWDvuthYf" -O shanghai.jpg
# !wget "https://docs.google.com/uc?export=download&id=1PDVCf_CzLWXNnNoRV8CFgoJxv6U0sHAO" -O tesla_supercharger.jpg


reader = FlatReader()
docs_2021 = reader.load_data(Path("tesla_2021_10k.htm"))


node_parser = UnstructuredElementNodeParser()


REPLICATE_API_TOKEN = "..."  # Your Relicate API token here
os.environ["REPLICATE_API_TOKEN"] = REPLICATE_API_TOKEN


# OPENAI_API_KEY = "sk-..."
# openai.api_key = OPENAI_API_KEY  # add your openai api key here
# os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY


if not os.path.exists("2021_nodes.pkl"):
    raw_nodes_2021 = node_parser.get_nodes_from_documents(docs_2021)
    pickle.dump(raw_nodes_2021, open("2021_nodes.pkl", "wb"))
else:
    raw_nodes_2021 = pickle.load(open("2021_nodes.pkl", "rb"))

nodes_2021, objects_2021 = node_parser.get_nodes_and_objects(raw_nodes_2021)

"""
## Setup Composable Retriever

Now that we've extracted tables and their summaries, we can setup a composable retriever in LlamaIndex to query these tables.

### Construct Retrievers
"""
logger.info("## Setup Composable Retriever")


vector_index = VectorStoreIndex(nodes=nodes_2021, objects=objects_2021)
query_engine = vector_index.as_query_engine(similarity_top_k=2, verbose=True)


imageUrl = "./tesla_supercharger.jpg"
image = Image.open(imageUrl).convert("RGB")

plt.figure(figsize=(16, 5))
plt.imshow(image)

"""
### Running LLaVa model using Replicate through LlamaIndex for image understanding
"""
logger.info("### Running LLaVa model using Replicate through LlamaIndex for image understanding")


multi_modal_llm = ReplicateMultiModal(
    model=REPLICATE_MULTI_MODAL_LLM_MODELS["llava-13b"],
    max_new_tokens=200,
    temperature=0.1,
)

prompt = "what is the main object for tesla in the image?"

llava_response = multi_modal_llm.complete(
    prompt=prompt,
    image_documents=[ImageDocument(image_path=imageUrl)],
)

"""
### Retrieve relevant information from LlamaIndex knowledge base according to LLaVa image understanding
"""
logger.info("### Retrieve relevant information from LlamaIndex knowledge base according to LLaVa image understanding")

prompt_template = "please provide relevant information about: "
rag_response = query_engine.query(prompt_template + llava_response.text)

"""
### Showing final RAG image caption results from LlamaIndex
"""
logger.info("### Showing final RAG image caption results from LlamaIndex")

logger.debug(str(rag_response))


imageUrl = "./shanghai.jpg"
image = Image.open(imageUrl).convert("RGB")

plt.figure(figsize=(16, 5))
plt.imshow(image)

"""
### Retrieve relevant information from LlamaIndex for a new image
"""
logger.info("### Retrieve relevant information from LlamaIndex for a new image")

prompt = "which Tesla factory is shown in the image?"

llava_response = multi_modal_llm.complete(
    prompt=prompt,
    image_documents=[ImageDocument(image_path=imageUrl)],
)

prompt_template = "please provide relevant information about: "
rag_response = query_engine.query(prompt_template + llava_response.text)

"""
### Showing final RAG image caption results from LlamaIndex
"""
logger.info("### Showing final RAG image caption results from LlamaIndex")

logger.debug(rag_response)

logger.info("\n\n[DONE]", bright=True)