from jet.logger import CustomLogger
from llama_index.llms.cleanlab import CleanlabTLM
import os
import shutil


OUTPUT_DIR = os.path.join(
    os.path.dirname(__file__), "generated", os.path.splitext(os.path.basename(__file__))[0])
shutil.rmtree(OUTPUT_DIR, ignore_errors=True)
log_file = os.path.join(OUTPUT_DIR, "main.log")
logger = CustomLogger(log_file, overwrite=True)
logger.info(f"Logs: {log_file}")

"""
<a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/cleanlab.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Cleanlab Trustworthy Language Model

Cleanlab‚Äôs [Trustworthy Language Model](https://help.cleanlab.ai/tlm/) scores the trustworthiness of every LLM response in real-time, using state-of-the-art uncertainty estimates for LLMs.  Trust scoring is crucial for applications where unchecked hallucinations and other LLM errors are a show-stopper.

This page demonstrates how to use TLM in place of your own LLM, to both generate responses and score their trustworthiness. That‚Äôs **not** the only way to use TLM though.
To add trust scoring to your existing unmodified RAG application, you can instead see [this Trustworthy RAG tutorial](https://docs.llamaindex.ai/en/stable/examples/evaluation/Cleanlab/).
Beyond RAG applications, you can score the trustworthiness of responses already generated from any LLM via `TLM.get_trustworthiness_score()`.

Learn more in the Cleanlab [documentation](https://help.cleanlab.ai/tlm/).

## Setup

If you're opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô.
"""
logger.info("# Cleanlab Trustworthy Language Model")

# %pip install llama-index-llms-cleanlab

# %pip install llama-index


llm = CleanlabTLM(api_key="your_api_key")

resp = llm.complete("Who is Paul Graham?")

logger.debug(resp)

"""
You also get the trustworthiness score of the above response in `additional_kwargs`. TLM automatically computes this score for all the <prompt, response> pair.
"""
logger.info("You also get the trustworthiness score of the above response in `additional_kwargs`. TLM automatically computes this score for all the <prompt, response> pair.")

logger.debug(resp.additional_kwargs)

"""
A high score indicates that LLM's response can be trusted. Let's take another example here.
"""
logger.info("A high score indicates that LLM's response can be trusted. Let's take another example here.")

resp = llm.complete(
    "What was the horsepower of the first automobile engine used in a commercial truck in the United States?"
)

logger.debug(resp)

logger.debug(resp.additional_kwargs)

"""
A low score indicates that the LLM's response shouldn't be trusted.

From these 2 straightforward examples, we can observe that the LLM's responses with the highest scores are direct, accurate, and appropriately detailed.<br />
On the other hand, LLM's responses with low trustworthiness score convey unhelpful or factually inaccurate answers, sometimes referred to as hallucinations.

### Streaming

Cleanlab‚Äôs TLM does not natively support streaming both the response and the trustworthiness score. However, there is an alternative approach available to achieve low-latency, streaming responses that can be used for your application.<br>
Detailed information about the approach, along with example code, is available [here](https://help.cleanlab.ai/tlm/use-cases/tlm_rag/#alternate-low-latencystreaming-approach-use-tlm-to-assess-responses-from-an-existing-rag-system).

## Advance use of TLM

TLM can be configured with the following options:
- **model**: underlying LLM to use
- **max_tokens**: maximum number of tokens to generate in the response
- **num_candidate_responses**: number of alternative candidate responses internally generated by TLM
- **num_consistency_samples**: amount of internal sampling to evaluate LLM-response-consistency
- **use_self_reflection**: whether the LLM is asked to self-reflect upon the response it generated and self-evaluate this response
- **log**: specify additional metadata to return. include ‚Äúexplanation‚Äù here to get explanations of why a response is scored with low trustworthiness

These configurations are passed as a dictionary to the `CleanlabTLM` object during initialization. <br />
More details about these options can be referred from [Cleanlab's API documentation](https://help.cleanlab.ai/tlm/api/python/tlm/#class-tlmoptions) and a few use-cases of these options are explored in [this notebook](https://help.cleanlab.ai/tlm/tutorials/tlm_advanced/).

Let's consider an example where the application requires `gpt-4` model with `128` output tokens.
"""
logger.info("### Streaming")

options = {
    "model": "gpt-4",
    "max_tokens": 128,
}
llm = CleanlabTLM(api_key="your_api_key", options=options)

resp = llm.complete("Who is Paul Graham?")

logger.debug(resp)

"""
To understand why the TLM estimated low trustworthiness for the previous horsepower related question, specify the `"explanation"` flag when initializing the TLM.
"""
logger.info("To understand why the TLM estimated low trustworthiness for the previous horsepower related question, specify the `"explanation"` flag when initializing the TLM.")

options = {
    "log": ["explanation"],
}
llm = CleanlabTLM(api_key="your_api_key", options=options)

resp = llm.complete(
    "What was the horsepower of the first automobile engine used in a commercial truck in the United States?"
)

logger.debug(resp)

logger.debug(resp.additional_kwargs["explanation"])

logger.info("\n\n[DONE]", bright=True)