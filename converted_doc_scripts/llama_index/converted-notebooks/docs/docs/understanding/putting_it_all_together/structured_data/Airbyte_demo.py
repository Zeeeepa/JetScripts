from IPython.display import Image
from IPython.display import Markdown, display
from jet.llm.mlx.adapters.mlx_llama_index_llm_adapter import MLXLlamaIndexLLMAdapter
from jet.logger import CustomLogger
from jet.models.config import MODELS_CACHE_DIR
from llama_index import SQLDatabase
from llama_index import VectorStoreIndex
from llama_index.core.settings import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.indices.struct_store.sql_query import (
SQLTableRetrieverQueryEngine,
)
from llama_index.indices.struct_store.sql_query import NLSQLTableQueryEngine
from llama_index.objects import (
SQLTableNodeMapping,
ObjectIndex,
SQLTableSchema,
)
from sqlalchemy import select, create_engine, MetaData, Table
import os
import shutil
import snowflake.sqlalchemy.snowdialect
import sqlalchemy.util.compat


OUTPUT_DIR = os.path.join(
    os.path.dirname(__file__), "generated", os.path.splitext(os.path.basename(__file__))[0])
shutil.rmtree(OUTPUT_DIR, ignore_errors=True)
log_file = os.path.join(OUTPUT_DIR, "main.log")
logger = CustomLogger(log_file, overwrite=True)
logger.info(f"Logs: {log_file}")

model_name = "sentence-transformers/all-MiniLM-L6-v2"
Settings.embed_model = HuggingFaceEmbedding(
    model_name=model_name,
    cache_folder=MODELS_CACHE_DIR,
)


"""
# Airbyte SQL Index Guide

We will show how to generate SQL queries on a Snowflake db generated by Airbyte.
"""
logger.info("# Airbyte SQL Index Guide")



"""
### Airbyte ingestion

Here we show how to ingest data from Github into a Snowflake db using Airbyte.
"""
logger.info("### Airbyte ingestion")


Image(filename="img/airbyte_1.png")

"""
Let's create a new connection. Here we will be dumping our Zendesk tickets into a Snowflake db.
"""
logger.info("Let's create a new connection. Here we will be dumping our Zendesk tickets into a Snowflake db.")

Image(filename="img/github_1.png")

Image(filename="img/github_2.png")

Image(filename="img/snowflake_1.png")

Image(filename="img/snowflake_2.png")

"""
Choose the streams you want to sync.
"""
logger.info("Choose the streams you want to sync.")

Image(filename="img/airbyte_7.png")

Image(filename="img/github_3.png")

"""
Sync your data.
"""
logger.info("Sync your data.")

Image(filename="img/airbyte_9.png")

Image(filename="img/airbyte_8.png")

"""
### Snowflake-SQLAlchemy version fix

Hack to make snowflake-sqlalchemy work despite incompatible sqlalchemy versions

Taken from https://github.com/snowflakedb/snowflake-sqlalchemy/issues/380#issuecomment-1470762025
"""
logger.info("### Snowflake-SQLAlchemy version fix")

def snowflake_sqlalchemy_20_monkey_patches():

    sqlalchemy.util.compat.string_types = (str,)
    sqlalchemy.types.String.RETURNS_UNICODE = True


    snowflake.sqlalchemy.snowdialect.SnowflakeDialect.returns_unicode_strings = (
        True
    )


    def has_table(self, connection, table_name, schema=None, info_cache=None):
        """
        Checks if the table exists.
        """
        return self._has_object(connection, "TABLE", table_name, schema)

    snowflake.sqlalchemy.snowdialect.SnowflakeDialect.has_table = has_table


try:
    snowflake_sqlalchemy_20_monkey_patches()
except Exception as e:
    raise ValueError("Please run `pip install snowflake-sqlalchemy`")

"""
### Define database

We pass the Snowflake uri to the SQL db constructor
"""
logger.info("### Define database")

snowflake_uri = "snowflake://<user_login_name>:<password>@<account_identifier>/<database_name>/<schema_name>?warehouse=<warehouse_name>&role=<role_name>"

"""
First we try connecting with sqlalchemy to check the db works.
"""
logger.info("First we try connecting with sqlalchemy to check the db works.")


engine = create_engine(snowflake_uri)
metadata = MetaData(bind=None)
table = Table("ZENDESK_TICKETS", metadata, autoload=True, autoload_with=engine)
stmt = select(table.columns)


with engine.connect() as connection:
    results = connection.execute(stmt).fetchone()
    logger.debug(results)
    logger.debug(results.keys())

"""
### Define SQL DB

Once we have defined the SQLDatabase, we can wrap it in a query engine to query it.
If we know what tables we want to use we can use `NLSQLTableQueryEngine`.
This will generate a SQL query on the specified tables.
"""
logger.info("### Define SQL DB")



sql_database = SQLDatabase(engine)

"""
### Synthesize Query

We then show a natural language query, which is translated to a SQL query under the hood with our text-to-SQL prompt.
"""
logger.info("### Synthesize Query")


query_engine = NLSQLTableQueryEngine(
    sql_database=sql_database,
    tables=["github_issues", "github_comments", "github_users"],
)
query_str = "Which issues have the most comments? Give the top 10 and use a join on url."
response = query_engine.query(query_str)
display(Markdown(f"<b>{response}</b>"))

query_engine = NLSQLTableQueryEngine(
    sql_database=sql_database,
    synthesize_response=False,
    tables=["github_issues", "github_comments", "github_users"],
)
response = query_engine.query(query_str)
display(Markdown(f"<b>{response}</b>"))

sql_query = response.metadata["sql_query"]
display(Markdown(f"<b>{sql_query}</b>"))

"""
We can also use LLM prediction to figure out what tables to use.

We first need to create an ObjectIndex of SQLTableSchema. In this case we only pass in the table names.
The query engine will fetch the relevant table schema at query time.
"""
logger.info("We can also use LLM prediction to figure out what tables to use.")


table_node_mapping = SQLTableNodeMapping(sql_database)
all_table_names = sql_database.get_usable_table_names()
table_schema_objs = []
for table_name in all_table_names:
    table_schema_objs.append(SQLTableSchema(table_name=table_name))

obj_index = ObjectIndex.from_objects(
    table_schema_objs,
    table_node_mapping,
    VectorStoreIndex,
)
table_retriever_query_engine = SQLTableRetrieverQueryEngine(
    sql_database, obj_index.as_retriever(similarity_top_k=1)
)
response = query_engine.query(query_str)

display(Markdown(f"<b>{response}</b>"))
sql_query = response.metadata["sql_query"]
display(Markdown(f"<b>{sql_query}</b>"))

logger.info("\n\n[DONE]", bright=True)