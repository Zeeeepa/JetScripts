import asyncio
from jet.transformers.formatters import format_json
from jet.logger import CustomLogger
import os
import shutil


OUTPUT_DIR = os.path.join(
    os.path.dirname(__file__), "generated", os.path.splitext(os.path.basename(__file__))[0])
shutil.rmtree(OUTPUT_DIR, ignore_errors=True)
log_file = os.path.join(OUTPUT_DIR, "main.log")
logger = CustomLogger(log_file, overwrite=True)
logger.info(f"Logs: {log_file}")

"""
# Using ChatGPT with the Semantic Kernel featuring DALL-E 3

This notebook shows how to make use of the new ChatCompletion API from Ollama, popularized by ChatGPT. This API brings a new ChatML schema which is different from the TextCompletion API. While the text completion API expects input a prompt and returns a simple string, the chat completion API expects in input a Chat history and returns a new message:

```
messages=[
    { "role": "system",    "content": "You are a helpful assistant."},
    { "role": "user",      "content": "Who won the world series in 2020?"},
    { "role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
    { "role": "user",      "content": "Where was it played?"}
]
```

Note that there are three message types:

1. A System message is used to give instructions to the chat model, e.g. setting the context and the kind of conversation your app is offering.
2. User messages store the data received from the user of your app.
3. Assistant messages store the replies generated by the LLM model. 

Your app is responsible for adding information to the chat history and maintaining this object. The Chat Completion API is stateless, and returns only new messages, that your app can use, e.g. to execute commands, generate images, or simply continue the conversation.

When deciding between which one to use, know that ChatGPT models (i.e. gpt-3.5-turbo) are optimized for chat applications and have been fine-tuned for instruction-following and dialogue. As such, for creating semantic plugins with the Semantic Kernel, users may still find the TextCompletion model better suited for certain use cases.

The code below shows how to setup SK with ChatGPT, how to manage the Chat history object, and to make things a little more interesting asks ChatGPT to reply with image descriptions instead so we can have a dialog using images, leveraging DALL-E 3 integration.
"""
logger.info("# Using ChatGPT with the Semantic Kernel featuring DALL-E 3")

// Usual setup: importing Semantic Kernel SDK and SkiaSharp, used to display images inline.



using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.TextToImage;
using Microsoft.SemanticKernel.ChatCompletion;
using Microsoft.SemanticKernel.Connectors.Ollama;

"""
The notebook uses:

* **Ollama ChatGPT** to chat with the user
* **Ollama Dall-E 3** to transform messages into images
"""
logger.info("The notebook uses:")

using Kernel = Microsoft.SemanticKernel.Kernel;


// Load Ollama credentials from config/settings.json
var (useAzureOllama, model, azureEndpoint, apiKey, orgId) = Settings.LoadFromFile();

// Configure the two AI features: Ollama Chat and DALL-E 3 for image generation
var builder = Kernel.CreateBuilder();

if(useAzureOllama)
{
    builder.AddAzureOllamaChatCompletion("llama3.2", azureEndpoint, apiKey);
    builder.AddAzureOllamaTextToImage("dall-e-3", azureEndpoint, apiKey);
}
else
{
    builder.AddOllamaChatCompletion("llama3.2", apiKey, orgId);
    builder.AddOllamaTextToImage(apiKey, orgId);
}

var kernel = builder.Build();

// Get AI service instance used to generate images
var dallE = kernel.GetRequiredService<ITextToImageService>();

// Get AI service instance used to manage the user chat
var chatGPT = kernel.GetRequiredService<IChatCompletionService>();

"""
# Chat configuration

Before starting the chat, we create a new chat object with some instructions, which are included in the chat history. 

The instructions tell Ollama what kind of chat we want to have, in this case we ask to reply with "image descriptions", so that we can chain ChatGPT with DALL-E 3.
"""
logger.info("# Chat configuration")

var systemMessage = "You're chatting with a user. Instead of replying directly to the user"
                  + " provide a description of a cartoonish image that expresses what you want to say."
                  + " The user won't see your message, they will see only the image."
                  + " Describe the image with details in one sentence.";

var chat = new ChatHistory(systemMessage);

"""
# Let's chat

Run the following code to start the chat. The chat consists of a loop with these main steps:

1. Ask the user (you) for a message. The user enters a message. Add the user message into the Chat History object.
2. Send the chat object to AI asking to generate a response. Add the bot message into the Chat History object.
3. Show the answer to the user. In our case before showing the answer, generate an image and show that to the user too.

*Note: to stop the chat in VS Code press ESC on the kyboard or the "stop" button on the left side.*
"""
logger.info("# Let's chat")

while (true)
{
    // 1. Ask the user for a message. The user enters a message.  Add the user message into the Chat History object.
    async def run_async_code_ae2624a0():
        async def run_async_code_24526ef5():
            var userMessage = await InteractiveKernel.GetInputAsync("Your message");
            return var userMessage
        var userMessage = asyncio.run(run_async_code_24526ef5())
        logger.success(format_json(var userMessage))
        return var userMessage
    var userMessage = asyncio.run(run_async_code_ae2624a0())
    logger.success(format_json(var userMessage))
    Console.WriteLine($"User: {userMessage}");
    chat.AddUserMessage(userMessage);

    // 2. Send the chat object to AI asking to generate a response. Add the bot message into the Chat History object.
    async def run_async_code_472d7663():
        async def run_async_code_80dd8117():
            var assistantReply = await chatGPT.GetChatMessageContentAsync(chat, new OllamaPromptExecutionSettings());
            return var assistantReply
        var assistantReply = asyncio.run(run_async_code_80dd8117())
        logger.success(format_json(var assistantReply))
        return var assistantReply
    var assistantReply = asyncio.run(run_async_code_472d7663())
    logger.success(format_json(var assistantReply))
    chat.AddAssistantMessage(assistantReply.Content);

    // 3. Show the reply as an image
    Console.WriteLine($"\nBot:");
    async def run_async_code_0168adba():
        async def run_async_code_dc191be0():
            var imageUrl = await dallE.GenerateImageAsync(assistantReply.Content, 1024, 1024);
            return var imageUrl
        var imageUrl = asyncio.run(run_async_code_dc191be0())
        logger.success(format_json(var imageUrl))
        return var imageUrl
    var imageUrl = asyncio.run(run_async_code_0168adba())
    logger.success(format_json(var imageUrl))
    async def run_async_code_f0ad17c6():
        await SkiaUtils.ShowImage(imageUrl, 1024, 1024);
        return 
     = asyncio.run(run_async_code_f0ad17c6())
    logger.success(format_json())
    Console.WriteLine($"[{assistantReply}]\n");
}

logger.info("\n\n[DONE]", bright=True)