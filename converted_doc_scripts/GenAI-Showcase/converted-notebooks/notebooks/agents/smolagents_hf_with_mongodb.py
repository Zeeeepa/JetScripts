from google.colab import userdata
from jet.logger import CustomLogger
from litellm import embedding
from pymongo import MongoClient
from pymongo.operations import SearchIndexModel
from smolagents import LiteLLMModel, tool
from smolagents import tool
from smolagents.agents import ToolCallingAgent
import json
import os
import shutil
import time


OUTPUT_DIR = os.path.join(
    os.path.dirname(__file__), "generated", os.path.splitext(os.path.basename(__file__))[0])
shutil.rmtree(OUTPUT_DIR, ignore_errors=True)
LOG_DIR = f"{OUTPUT_DIR}/logs"

log_file = os.path.join(LOG_DIR, "main.log")
logger = CustomLogger(log_file, overwrite=True)
logger.orange(f"Logs: {log_file}")

"""
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/GenAI-Showcase/blob/main/notebooks/agents/smolagents_hf_with_mongodb.ipynb)

# Using Smolagents with MongoDB Atlas

This notebook demonstrates how to use [Smolagents](https://github.com/huggingface/smolagents) to interact with MongoDB Atlas for building AI-powered applications. We'll explore how to create tools that leverage MongoDB's aggregation capabilities to analyze and extract insights from data.

## Prerequisites

Before running this notebook, you'll need:

1. A MongoDB Atlas account and cluster
2. Python environment with required packages
3. Ollama API key for GPT-4 access

## Setting Up MongoDB Atlas

1. Create a free MongoDB Atlas account at [https://www.mongodb.com/cloud/atlas/register](https://www.mongodb.com/cloud/atlas/register)
2. Create a new cluster (free tier is sufficient)
3. Configure network access by adding your IP address
4. Create a database user with read/write permissions
5. Get your connection string from Atlas UI (Click "Connect" > "Connect your application")
6. Replace `<password>` in the connection string with your database user's password
7. Enable network access from your IP address in the Network Access settings

## Observations

In this notebook, we:
- Define tools that interact with MongoDB Atlas using pymongo
- Use aggregation pipelines to analyze data
- Sample documents to understand schema structure
- Demonstrate how LLMs can generate and execute MongoDB queries

The tools showcase how to:
1. Execute aggregation pipelines generated by the LLM
2. Sample documents to understand collection structure
3. Handle errors and provide meaningful feedback

### Security Considerations

When working with MongoDB Atlas:
- Never commit connection strings with credentials to version control
- Use environment variables or secure secret management
- Restrict database user permissions to only what's needed
- Enable IP allowlist in Atlas Network Access settings
"""
logger.info("# Using Smolagents with MongoDB Atlas")

pip install pymongo smolagents

# import getpass

# MONGODB_URI = getpass.getpass("Enter your MongoDB Atlas URI: ")
os.environ["MONGODB_URI"] = MONGODB_URI

"""
## Loading the dataset

In this example I am using the airbnb data set from https://huggingface.co/datasets/MongoDB/airbnb_embeddings .

- Database : ai_airbnb
- Collection : rentals

## Defining the tools

We'll create two main tools for interacting with MongoDB:

1. **Aggregation Tool**: Executes aggregation pipelines generated by the LLM to analyze data
   - Takes a pipeline as input
   - Handles complex data transformations
   - Returns aggregated results

2. **Sampling Tool**: Helps understand collection structure
   - Randomly samples documents
   - Provides schema insights
   - Useful for data exploration

Both tools automatically exclude embedding fields to reduce response size and improve readability.
"""
logger.info("## Loading the dataset")

# import getpass


# os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

model = LiteLLMModel(model_id="gpt-4o")

client = MongoClient(MONGODB_URI, appname="devrel.showcase.smolagents")


@tool
def get_aggregated_docs(pipeline: str) -> list:
    """
    Gets a generated pipeline as 'pipeline' by the LLM and provide the context documents

     Args:
        pipeline: An array List with the current stages from the LLM # Added (list) and a description after the argument name
    """
    db = client["ai_airbnb"]
    collection = db["rentals"]
    pipeline = json.loads(pipeline)
    pipeline.insert(
        0, {"$project": {"text_embeddings": 0, "image_embeddings": 0}}
    )  # Use insert to add at the beginning
    docs = list(collection.aggregate(pipeline))
    return docs


@tool
def sample_documents(collection_name: str) -> str:
    """
    Use $sample to sample the collection docs

    Args:
      collection_name: The name of the collection to sample from
    """
    db = client["ai_airbnb"]
    try:
        collection = db[collection_name]
        sample = list(
            collection.aggregate(
                [
                    {"$project": {"text_embeddings": 0, "image_embeddings": 0}},
                    {"$sample": {"size": 5}},
                ]
            )
        )  # Sample 5 documents
        return sample
    except Exception as e:
        return f"Error: {e}"


agent = ToolCallingAgent(tools=[get_aggregated_docs, sample_documents], model=model)

user_query = "What are the supported countries in our 'rentals' collection, sample for structre and then  aggregate how many are in each country"
response = agent.run(user_query)

"""
## Vector Search based RAG with Atlas Search

Vector search allows us to find relevant documents based on the semantic meaning of the query rather than just keyword matching. In this section, we demonstrate how to build a Retrieval-Augmented Generation (RAG) agent that leverages MongoDB Atlas Search's vector search capabilities.

The RAG agent uses the `vector_search_rentals` tool to find relevant documents based on the query's embeddings. This approach enhances the search results by considering the context and meaning of the query, providing more accurate and relevant results.

We define the `vector_search_rentals` tool to perform the vector search and integrate it with the `ToolCallingAgent` to handle user queries effectively. The agent processes the query, performs the vector search, and returns the most relevant documents from the rentals collection.

### Create the vector search index if it does not exists

To create the vector search index, we define a search index model with the necessary configuration for vector search. This includes specifying the number of dimensions and the similarity metric. The index is then created on the text_embeddings field of the rentals collection. We also include a polling mechanism to ensure the index is ready for querying before proceeding.
"""
logger.info("## Vector Search based RAG with Atlas Search")



db = client["ai_airbnb"]
collection = db["rentals"]


search_index_model = SearchIndexModel(
    definition={
        "fields": [
            {
                "type": "vector",
                "numDimensions": 1536,
                "path": "text_embeddings",
                "similarity": "cosine",
            },
        ]
    },
    name="vector_index",
    type="vectorSearch",
)
result = collection.create_search_index(model=search_index_model)
logger.debug("New search index named " + result + " is building.")
logger.debug("Polling to check if the index is ready. This may take up to a minute.")


def check_queryable(index):
    """Check if the index is queryable."""
    return index.get("queryable") is True


predicate = None
if predicate is None:
    predicate = check_queryable
while True:
    indices = list(collection.list_search_indexes(result))
    if len(indices) and predicate(indices[0]):
        break
    time.sleep(5)

logger.debug(result + " is ready for querying.")
client.close()





@tool
def vector_search_rentals(query: str) -> list:
    """
    Gets a query , generates embeddings and locate vector store relavant documents

    Args:
      query: The query to search for

    Returns:
      A list of documents that are relavant to the query

    """
    response = embedding(model="mxbai-embed-large", input=[query])
    query_embedding = response["data"][0]["embedding"]

    pipeline = [
        {
            "$vectorSearch": {
                "index": "vector_index",
                "queryVector": query_embedding,
                "path": "text_embeddings",
                "numCandidates": 100,
                "limit": 5,
            }
        },
        {
            "$project": {
                "text_embeddings": 0,
                "image_embeddings": 0,
                "_id": 0,
                "score": {"$meta": "searchScore"},
            }
        },
    ]

    results = list(collection.aggregate(pipeline))
    return results


user_query: str = "Show me apartments in London"
search_results = vector_search_rentals(user_query)

logger.debug(search_results)



user_query = "Near parks and in brooklyn"

rag_agent = ToolCallingAgent(tools=[vector_search_rentals], model=model)

response = rag_agent.run(user_query)  # Pass context to agent.run()

"""
## Conclusions

This notebook successfully demonstrates the integration of Smolagents with MongoDB Atlas, enabling effective data analysis through an AI agent.  The defined tools, `get_aggregated_docs` and `sample_documents`, effectively interact with the Airbnb dataset stored in MongoDB Atlas.  The agent, powered by a chosen LLM (in this case, GPT-4o), successfully translates user queries into both data sampling and aggregation pipelines executed against the MongoDB database.

Key improvements and observations include:

* **Robust Tool Design:** The tools now incorporate error handling, providing more informative feedback to the user in case of issues.  The exclusion of embedding fields from queries enhances performance and readability of results.
* **Enhanced Query Handling:**  The inclusion of an initial projection stage in the aggregation pipeline, specifically designed to remove embedding fields (`text_embeddings` and `image_embeddings`) prior to other stages, ensures more efficient query execution and smaller response sizes.  The use of `json.loads()` ensures that the pipeline string received from the LLM is correctly parsed.
Atlas Search excels at finding relevant documents quickly, thanks to its vector search capabilities.  This is particularly beneficial for large datasets where traditional keyword search may be insufficient.
* **Improved User Experience:** Clearer tool documentation and example usage further enhance the user's ability to interact with the agent and interpret results.
* **Practical Application:** The demonstration showcases a practical application for analyzing data within a MongoDB Atlas database using an LLM-powered agent.

Future development could explore:

* **Expanded Toolset:** Implementing additional tools for data manipulation, filtering, and more complex analytics.
* **Advanced Query Generation:** Exploring methods to refine the LLM's ability to generate accurate and efficient MongoDB queries.
* **Visualization Capabilities:** Integrating data visualization libraries to present the analysis results more effectively.
* **Security Enhancements:** Further solidifying security practices, potentially incorporating environment variable management for sensitive credentials.
"""
logger.info("## Conclusions")

logger.info("\n\n[DONE]", bright=True)