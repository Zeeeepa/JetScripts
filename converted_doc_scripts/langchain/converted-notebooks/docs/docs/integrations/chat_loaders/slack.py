from jet.adapters.langchain.chat_ollama import ChatOllama
from jet.logger import logger
from langchain_community.chat_loaders.slack import SlackChatLoader
from langchain_community.chat_loaders.utils import (
map_ai_messages,
merge_chat_runs,
)
from langchain_core.chat_sessions import ChatSession
from typing import List
import os
import requests
import shutil


OUTPUT_DIR = os.path.join(
    os.path.dirname(__file__), "generated", os.path.splitext(os.path.basename(__file__))[0])
shutil.rmtree(OUTPUT_DIR, ignore_errors=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
log_file = os.path.join(OUTPUT_DIR, "main.log")
logger.basicConfig(filename=log_file)
logger.info(f"Logs: {log_file}")

PERSIST_DIR = f"{OUTPUT_DIR}/chroma"
os.makedirs(PERSIST_DIR, exist_ok=True)

"""
# Slack

This notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages.

The process has three steps:
1. Export the desired conversation thread by following the [instructions here](https://slack.com/help/articles/1500001548241-Request-to-export-all-conversations).
2. Create the `SlackChatLoader` with the file path pointed to the json file or directory of JSON files
3. Call `loader.load()` (or `loader.lazy_load()`) to perform the conversion. Optionally use `merge_chat_runs` to combine message from the same sender in sequence, and/or `map_ai_messages` to convert messages from the specified sender to the "AIMessage" class.

## 1. Create message dump

Currently (2023/08/23), this loader best supports a zip directory of files in the format generated by exporting your a direct message conversation from Slack. Follow the up-to-date instructions from slack on how to do so.

We have an example in the LangChain repo.
"""
logger.info("# Slack")


permalink = "https://raw.githubusercontent.com/langchain-ai/langchain/342087bdfa3ac31d622385d0f2d09cf5e06c8db3/libs/langchain/tests/integration_tests/examples/slack_export.zip"
response = requests.get(permalink)
with open("slack_dump.zip", "wb") as f:
    f.write(response.content)

"""
## 2. Create the Chat Loader

Provide the loader with the file path to the zip directory. You can optionally specify the user id that maps to an ai message as well as configure whether to merge message runs.
"""
logger.info("## 2. Create the Chat Loader")


loader = SlackChatLoader(
    path="slack_dump.zip",
)

"""
## 3. Load messages

The `load()` (or `lazy_load`) methods return a list of "ChatSessions" that currently just contain a list of messages per loaded conversation.
"""
logger.info("## 3. Load messages")



raw_messages = loader.lazy_load()
merged_messages = merge_chat_runs(raw_messages)
messages: List[ChatSession] = list(
    map_ai_messages(merged_messages, sender="U0500003428")
)

"""
### Next Steps

You can then use these messages how you see fit, such as fine-tuning a model, few-shot example selection, or directly make predictions for the next message.
"""
logger.info("### Next Steps")


llm = ChatOllama(model="llama3.2")

for chunk in llm.stream(messages[1]["messages"]):
    logger.debug(chunk.content, end="", flush=True)

logger.info("\n\n[DONE]", bright=True)