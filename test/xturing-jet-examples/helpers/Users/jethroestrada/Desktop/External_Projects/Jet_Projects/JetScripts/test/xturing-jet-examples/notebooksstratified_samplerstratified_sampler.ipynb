{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from jet.logger import time_it\n",
    "from words import get_unique_words, get_words\n",
    "from typing import List, Optional\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedData():\n",
    "    source: str\n",
    "    target: Optional[str]\n",
    "    category_values: List[str]\n",
    "    score: Optional[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n=1):\n",
    "    # Tokenize and filter out punctuation in one step\n",
    "    words = get_words(text, n)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_weight(all_ngrams, sentence_ngrams, previous_ngrams):\n",
    "    # Calculate weight of the sentence based on n-gram frequency\n",
    "    # Introduce penalty for shared n-grams with the previous sentence\n",
    "    penalty = sum(ngram in previous_ngrams for ngram in sentence_ngrams)\n",
    "    return sum(1 / all_ngrams[ngram] for ngram in sentence_ngrams if ngram in all_ngrams) + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ad012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_sentences(sentences, n):\n",
    "    all_ngrams = Counter()\n",
    "    sentence_ngrams_dict = {}\n",
    "\n",
    "    # Precompute n-grams for each sentence\n",
    "    for sentence in tqdm(sentences, desc=\"Precomputing n-grams\"):\n",
    "        ngram_list = get_ngrams(sentence, n)\n",
    "        all_ngrams.update(ngram_list)\n",
    "        sentence_ngrams_dict[sentence] = ngram_list\n",
    "\n",
    "    sorted_sentences = []\n",
    "\n",
    "    # Adding tqdm progress bar\n",
    "    for _ in tqdm(range(len(sentences)), desc=\"Sorting sentences\"):\n",
    "        if sorted_sentences:\n",
    "            previous_ngrams = set(get_ngrams(sorted_sentences[-1], n))\n",
    "        else:\n",
    "            previous_ngrams = set()\n",
    "\n",
    "        # Sort remaining sentences based on n-gram weights and penalties\n",
    "        sentences.sort(key=lambda sentence: get_ngram_weight(\n",
    "            all_ngrams, sentence_ngrams_dict[sentence], previous_ngrams),\n",
    "            reverse=False\n",
    "        )\n",
    "\n",
    "        # Add the best sentence to the sorted list and remove it from the original list\n",
    "        sorted_sentences.append(sentences.pop(0))\n",
    "\n",
    "    return sorted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_frequency(sentence, n=2):\n",
    "    \"\"\" Calculate the frequency of n-grams in a sentence \"\"\"\n",
    "    n_grams = [sentence[i:i+n] for i in range(len(sentence) - n + 1)]\n",
    "    return Counter(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_n_gram_diversity(freq):\n",
    "    \"\"\" Calculate diversity based on the count of unique n-grams \"\"\"\n",
    "    return len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_sort_sentences_by_ngrams(sentences: List[str], n: int = 2, top_n: int = 2, is_start_ngrams=True) -> List[str]:\n",
    "    sentence_ngrams = defaultdict(list)\n",
    "    all_ngrams = Counter()\n",
    "\n",
    "    # Combine grouping and ngram counting in a single loop\n",
    "    for sentence in tqdm(sentences, desc=\"Grouping sentences\"):\n",
    "        ngrams_list = get_ngrams(sentence, n)\n",
    "        all_ngrams.update(ngrams_list)\n",
    "\n",
    "        if is_start_ngrams and ngrams_list:\n",
    "            sentence_ngrams[\" \".join(ngrams_list[0])].append(sentence)\n",
    "        elif not is_start_ngrams:\n",
    "            for ngram in set(ngrams_list):\n",
    "                sentence_ngrams[\" \".join(ngram)].append(sentence)\n",
    "\n",
    "    # Optimizing groups without a secondary sorting loop\n",
    "    optimized_groups = {ngram: group_sentences[:top_n]\n",
    "                        for ngram, group_sentences in sentence_ngrams.items()}\n",
    "\n",
    "    # Flatten the dictionary of grouped sentences\n",
    "    flattened_sentences = set(\n",
    "        itertools.chain.from_iterable(optimized_groups.values()))\n",
    "\n",
    "    # Sort sentences by unique ngram weights\n",
    "    sorted_sentences = sort_sentences(list(flattened_sentences), n)\n",
    "\n",
    "    return sorted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedDataString():\n",
    "    source: str\n",
    "    category_values: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2751653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedData():\n",
    "    def __init__(self, source: str, target: str, score: float):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.score = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedSampler():\n",
    "    def __init__(self, data: List[ProcessedData or str], num_samples=0.8):\n",
    "        # Assuming get_unique_words is defined elsewhere\n",
    "        unique_words = get_unique_words(data)\n",
    "        self.data = unique_words\n",
    "\n",
    "        # Calculate the number of samples\n",
    "        if isinstance(num_samples, float) and 0.0 < num_samples < 1.0:\n",
    "            final_num_samples = int(num_samples * len(data))\n",
    "        elif isinstance(num_samples, int) and num_samples > 0:\n",
    "            final_num_samples = min(num_samples, len(data))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"num_samples must be a float in the range (0.0, 1.0) or a positive integer\")\n",
    "\n",
    "        self.num_samples = final_num_samples\n",
    "\n",
    "    def filter_strings(self, n=2, top_n=2) -> List[str]:\n",
    "        filtered_data = filter_and_sort_sentences_by_ngrams(\n",
    "            self.data, n, top_n, is_start_ngrams=True)\n",
    "\n",
    "        return filtered_data[:self.num_samples]\n",
    "\n",
    "    @time_it\n",
    "    def get_samples(self) -> List[StratifiedData]:\n",
    "        # Create a dictionary to map (source, target) tuples to their scores\n",
    "        score_map = {(item.source, item.target): item.score for item in self.data}\n",
    "\n",
    "        # Unpack the data into features, targets, and labels for stratification\n",
    "        features_targets = [(item.source, item.target) for item in self.data]\n",
    "        labels = [item.category_values for item in self.data]\n",
    "\n",
    "        # Use labels for stratification\n",
    "        features_targets_sample, _, labels_sample, _ = train_test_split(\n",
    "            features_targets, labels, train_size=self.num_samples, stratify=labels\n",
    "        )\n",
    "\n",
    "        # Construct StratifiedData objects from the sampled data, including the score\n",
    "        stratified_samples = [\n",
    "            StratifiedData(source=ft[0], target=ft[1], score=score_map[ft])\n",
    "            for ft, lbl in zip(features_targets_sample, labels_sample)\n",
    "        ]\n",
    "\n",
    "        # Convert all items to dict\n",
    "        stratified_samples = [item.__dict__ for item in stratified_samples]\n",
    "\n",
    "        return stratified_samples\n",
    "\n",
    "    @time_it\n",
    "    def get_unique_strings(self) -> List[str]:\n",
    "        data_with_labels = self.load_data_with_labels()\n",
    "\n",
    "        # Unpack the data into features, targets, and labels for stratification\n",
    "        features_targets = [item.source for item in data_with_labels]\n",
    "        # Now includes n-gram categories\n",
    "        labels = [item.category_values for item in data_with_labels]\n",
    "\n",
    "        features_targets_sample, _, labels_sample, _ = train_test_split(\n",
    "            features_targets, labels, train_size=self.num_samples, stratify=labels\n",
    "        )\n",
    "\n",
    "        return features_targets_sample\n",
    "\n",
    "    @time_it\n",
    "    def load_data_with_labels(\n",
    "        self,\n",
    "        max_q=2,\n",
    "    ) -> List[ProcessedDataString]:\n",
    "        data = self.data\n",
    "\n",
    "        def calculate_ttr(sentence):\n",
    "            words = sentence.split()\n",
    "            unique_words = set(words)\n",
    "            return len(unique_words)\n",
    "\n",
    "        def calculate_ttr_class(ttr, ttr_quantiles):\n",
    "            for i, q in enumerate(ttr_quantiles):\n",
    "                if ttr <= q:\n",
    "                    return f'ttr_q{i+1}'\n",
    "            return f'ttr_q{len(ttr_quantiles)+1}'\n",
    "\n",
    "        def categorize_sentence_length(sentence, length_quantiles):\n",
    "            word_count = len(sentence.split())\n",
    "            for i, q in enumerate(length_quantiles):\n",
    "                if word_count <= q:\n",
    "                    return f'q{i+1}'\n",
    "            return f'q{len(length_quantiles)+1}'\n",
    "\n",
    "        def categorize_n_gram_diversity(diversity, diversity_quantiles):\n",
    "            \"\"\" Categorize based on n-gram diversity quantiles \"\"\"\n",
    "            for i, q in enumerate(diversity_quantiles):\n",
    "                if diversity <= q:\n",
    "                    return f'ngram_q{i+1}'\n",
    "            return f'ngram_q{len(diversity_quantiles)+1}'\n",
    "\n",
    "        def get_starting_n_gram(sentence, n=5):\n",
    "            \"\"\" Extract the starting n-gram of a sentence (consider increasing 'n' for more granularity) \"\"\"\n",
    "            words = get_words(sentence)\n",
    "            return ' '.join(words[:n]) if len(words) >= n else sentence\n",
    "\n",
    "        def categorize_based_on_quantiles(value, quantiles):\n",
    "            \"\"\" Categorize a value based on quantiles \"\"\"\n",
    "            for i, q in enumerate(quantiles):\n",
    "                if value <= q:\n",
    "                    return f'q{i+1}'\n",
    "            return f'q{len(quantiles)+1}'\n",
    "\n",
    "        def determine_quantiles(values, num_quantiles):\n",
    "            \"\"\" Determine dynamic quantile values based on the data distribution \"\"\"\n",
    "            quantile_values = np.linspace(0, 1, num_quantiles + 2)[1:-1]\n",
    "            return np.quantile(values, quantile_values)\n",
    "\n",
    "        # Compute quantiles for dynamic categorization\n",
    "        sentence_counts = [len(item.split()) for item in data]\n",
    "        ttrs = [calculate_ttr(item) for item in data]\n",
    "\n",
    "        # Determine number of quantiles based on data diversity and max_q\n",
    "        num_length_quantiles = min(max_q, min(\n",
    "            5, len(set(sentence_counts)) // 20))\n",
    "        num_ttr_quantiles = min(max_q, min(5, len(set(ttrs)) // 20))\n",
    "\n",
    "        length_quantiles = determine_quantiles(\n",
    "            sentence_counts, num_length_quantiles)\n",
    "        ttr_quantiles = determine_quantiles(ttrs, num_ttr_quantiles)\n",
    "\n",
    "        # Compute n-gram frequencies and determine their diversity\n",
    "        ngram_diversities = [calculate_n_gram_diversity(\n",
    "            n_gram_frequency(item)) for item in data]\n",
    "\n",
    "        # Determine number of quantiles for n-gram diversity\n",
    "        num_ngram_quantiles = min(max_q, min(\n",
    "            5, len(set(ngram_diversities)) // 20))\n",
    "        ngram_quantiles = determine_quantiles(\n",
    "            ngram_diversities, num_ngram_quantiles)\n",
    "\n",
    "       # Compute starting n-gram frequencies\n",
    "        starting_ngrams = [get_starting_n_gram(item) for item in data]\n",
    "        starting_ngram_freq = Counter(starting_ngrams)\n",
    "        starting_ngram_counts = list(starting_ngram_freq.values())\n",
    "\n",
    "        # Determine dynamic quantiles for starting n-gram frequencies\n",
    "        num_starting_ngram_quantiles = min(max_q, min(\n",
    "            5, len(set(starting_ngram_counts)) // 20))\n",
    "        starting_ngram_quantiles = determine_quantiles(\n",
    "            starting_ngram_counts, num_starting_ngram_quantiles)\n",
    "\n",
    "        # Categorize each starting n-gram based on quantiles\n",
    "        starting_ngram_categories = {}\n",
    "        for ngram in starting_ngram_freq:\n",
    "            ngram_count = starting_ngram_freq[ngram]\n",
    "            starting_ngram_category = categorize_based_on_quantiles(\n",
    "                ngram_count, starting_ngram_quantiles)\n",
    "            starting_ngram_categories[ngram] = starting_ngram_category\n",
    "\n",
    "        processed_data = []\n",
    "        ttr_class_distribution = Counter()\n",
    "        sentence_length_distribution = Counter()\n",
    "        n_gram_diversity_distribution = Counter()\n",
    "        starting_ngram_distribution = Counter()\n",
    "\n",
    "        for item in data:\n",
    "            source_sentence = item\n",
    "\n",
    "            ttr = calculate_ttr(source_sentence)\n",
    "            ttr_class = calculate_ttr_class(ttr, ttr_quantiles)\n",
    "            sentence_length = categorize_sentence_length(\n",
    "                source_sentence, length_quantiles)\n",
    "\n",
    "            n_gram_diversity = calculate_n_gram_diversity(\n",
    "                n_gram_frequency(source_sentence))\n",
    "            n_gram_diversity_class = categorize_n_gram_diversity(\n",
    "                n_gram_diversity, ngram_quantiles)\n",
    "\n",
    "            # Get starting n-gram category\n",
    "            starting_ngram = get_starting_n_gram(source_sentence)\n",
    "            starting_ngram_category = starting_ngram_categories[starting_ngram]\n",
    "\n",
    "            # Update distributions\n",
    "            ttr_class_distribution[ttr_class] += 1\n",
    "            sentence_length_distribution[sentence_length] += 1\n",
    "            n_gram_diversity_distribution[n_gram_diversity_class] += 1\n",
    "            starting_ngram_distribution[starting_ngram_category] += 1\n",
    "\n",
    "            # Create a new processed data item\n",
    "            processed_item = ProcessedDataString()\n",
    "            processed_item.source = source_sentence\n",
    "            processed_item.category_values = [\n",
    "                ttr_class, sentence_length, n_gram_diversity_class, starting_ngram_category]\n",
    "\n",
    "            processed_data.append(processed_item)\n",
    "\n",
    "        # Print out the distributions\n",
    "        print(\"TTR Class Distribution:\", dict(ttr_class_distribution))\n",
    "        print(\"Sentence Length Distribution:\",\n",
    "              dict(sentence_length_distribution))\n",
    "        print(\"N-Gram Diversity Distribution:\",\n",
    "              dict(n_gram_diversity_distribution))\n",
    "        print(\"Starting N-Gram Distribution:\",\n",
    "              dict(starting_ngram_distribution))\n",
    "\n",
    "        return processed_data"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
