{
  "structure": "Keyword Extraction",
  "system": "You are an expert Python developer tasked with generating complete, functional, and well-documented Python code. Based on the provided example, create a Python script that implements the specified structure. The code should:\n- Be syntactically correct and follow PEP 8 style guidelines.\n- Include necessary imports and type hints where applicable.\n- Handle errors gracefully with try-except blocks.\n- Include docstrings and comments for clarity.\n- Be compatible with the existing MLX framework (e.g., use jet.llm.mlx modules).\n- Produce output matching the example structure exactly.\n\nGenerate a complete Python script that implements the provided info. Do not include markdown code fences or any non-Python content. Ensure the script can be saved and run directly.",
  "query": "Generate a Python script for the Keyword Extraction structure.",
  "code": "from typing import List, Dict, Optional\nfrom jet.llm.mlx.config import DEFAULT_MODEL\nfrom jet.llm.mlx.mlx_types import ModelType\nfrom jet.llm.mlx.models import resolve_model\nfrom jet.llm.mlx.token_utils import tokenize_strings\nfrom jet.logger import logger\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom mlx_lm.generate import stream_generate, generate_step\nfrom mlx_lm.sample_utils import make_sampler, make_logits_processors\nfrom mlx_lm.utils import TokenizerWrapper\n\n# Custom exceptions for specific error cases\n\n\nclass ModelLoadError(Exception):\n    pass\n\n\nclass InvalidMethodError(Exception):\n    pass\n\n\nclass InvalidOutputError(Exception):\n    pass\n\n# Type definitions for structured data\n\n\nclass ChatMessage(TypedDict):\n    role: str\n    content: str\n\n\nclass AnswerResult(TypedDict):\n    answer: str\n    token_id: int\n    is_valid: bool\n    method: str\n    error: Optional[str]\n\n\ndef keyword_extraction(\n    question: str,\n    model_path: ModelType = DEFAULT_MODEL,\n    method: str = \"stream_generate\",\n    max_tokens: int = 10,\n    temperature: float = 0.0,\n    top_p: float = 0.9\n) -> AnswerResult:\n    \"\"\"\n    Extracts keywords from a given question using a language model.\n\n    Args:\n        question: The question to be answered.\n        model_path: Path to the model (defaults to DEFAULT_MODEL).\n        method: Generation method (\"stream_generate\" or \"generate_step\").\n        max_tokens: Maximum number of tokens to generate.\n        temperature: Sampling temperature.\n        top_p: Top-p sampling parameter.\n\n    Returns:\n        AnswerResult containing the extracted keywords, token ID, validity, method, and any error.\n    \"\"\"\n    try:\n        # Validate inputs\n        validate_method(method)\n\n        # Load model and tokenizer\n        model_components = load_model_components(model_path)\n\n        # Create and log prompt\n        system_prompt = create_system_prompt(question)\n        log_prompt_details(system_prompt, question, model_path)\n\n        # Format messages and apply chat template\n        messages = format_chat_messages(system_prompt, question)\n        formatted_prompt = model_components.tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        # Encode choices and setup generation parameters\n        choice_token",
  "error": null
}