{
  "structure": "Text Segmentation",
  "system": "You are an expert Python developer tasked with generating complete, functional, and well-documented Python code. Based on the provided example, create a Python script that implements the specified structure. The code should:\n- Be syntactically correct and follow PEP 8 style guidelines.\n- Include necessary imports and type hints where applicable.\n- Handle errors gracefully with try-except blocks.\n- Include docstrings and comments for clarity.\n- Be compatible with the existing MLX framework (e.g., use jet.llm.mlx modules).\n- Produce output matching the example structure exactly.\n\nGenerate a complete Python script that implements the provided info. Do not include markdown code fences or any non-Python content. Ensure the script can be saved and run directly.",
  "query": "Generate a Python script for the Text Segmentation structure.",
  "code": "from typing import List, Dict\nfrom jet.llm.mlx.config import DEFAULT_MODEL\nfrom jet.llm.mlx.mlx_types import ModelType\nfrom jet.llm.mlx.models import resolve_model\nfrom jet.llm.mlx.token_utils import tokenize_strings\nfrom jet.logger import logger\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom mlx_lm.generate import stream_generate, generate_step\nfrom mlx_lm.sample_utils import make_sampler, make_logits_processors\nfrom mlx_lm.utils import TokenizerWrapper\n\n# Custom exceptions for specific error cases\n\n\nclass ModelLoadError(Exception):\n    pass\n\n\nclass InvalidMethodError(Exception):\n    pass\n\n\nclass InvalidOutputError(Exception):\n    pass\n\n\n# Type definitions for structured data\n\n\nclass ChatMessage(TypedDict):\n    role: str\n    content: str\n\n\nclass AnswerResult(TypedDict):\n    answer: str\n    token_id: int\n    is_valid: bool\n    method: str\n    error: Optional[str]\n\n\ndef segment_text(text: str, max_length: int = 1000) -> ChatMessage:\n    \"\"\"\n    Segments the input text into smaller chunks.\n\n    Args:\n        text: The input text to be segmented.\n        max_length: The maximum length of each chunk.\n\n    Returns:\n        ChatMessage containing the segmented text.\n    \"\"\"\n    # Split the text into sentences\n    sentences = tokenize_strings(text, max_length)\n\n    # Segment each sentence into words\n    words = []\n    for sentence in sentences:\n        words.extend(tokenize_strings(sentence, max_length))\n\n    # Join the words back into a text\n    text = ' '.join(words)\n\n    return ChatMessage(text, text)\n\n\ndef generate_answer_stream(\n    model_components: ModelComponents,\n    formatted_prompt: str,\n    max_tokens: int = 1000,\n    logits_processors: list,\n    sampler,\n    stop_tokens: List[int],\n    choices: List[str]\n) -> tuple[str, int, List[int]]:\n    \"\"\"\n    Generates an answer using the provided inputs.\n\n    Args:\n        model_components: The model and tokenizer components.\n        formatted_prompt: The formatted prompt.\n        max_tokens: The maximum number of tokens to generate.\n        logits_processors: The logits processor.\n        sampler: The sampler.\n        stop_tokens: The stop tokens.\n        choices: The possible answer choices.\n\n    Returns:\n        tuple[str, int",
  "error": null
}