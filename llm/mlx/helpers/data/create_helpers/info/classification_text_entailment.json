{
  "structure": "Text Entailment",
  "system": "You are an expert Python developer tasked with generating complete, functional, and well-documented Python code. Based on the provided example, create a Python script that implements the specified structure. The code should:\n- Be syntactically correct and follow PEP 8 style guidelines.\n- Include necessary imports and type hints where applicable.\n- Handle errors gracefully with try-except blocks.\n- Include docstrings and comments for clarity.\n- Be compatible with the existing MLX framework (e.g., use jet.llm.mlx modules).\n- Produce output matching the example structure exactly.\n\nGenerate a complete Python script that implements the provided info. Do not include markdown code fences or any non-Python content. Ensure the script can be saved and run directly.",
  "query": "Generate a Python script for the Text Entailment structure.",
  "code": "from typing import List, Dict, Optional, TypedDict\nfrom jet.llm.mlx.config import DEFAULT_MODEL\nfrom jet.llm.mlx.mlx_types import ModelType\nfrom jet.llm.mlx.models import resolve_model\nfrom jet.llm.mlx.token_utils import tokenize_strings\nfrom jet.logger import logger\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom mlx_lm.generate import stream_generate, generate_step\nfrom mlx_lm.sample_utils import make_sampler, make_logits_processors\nfrom mlx_lm.utils import TokenizerWrapper\n\n# Set the seed for reproducibility\nmx.random.seed(42)\n\n\n# Custom exceptions for specific error cases\nclass ModelLoadError(Exception):\n    pass\n\n\nclass InvalidMethodError(Exception):\n    pass\n\n\nclass InvalidOutputError(Exception):\n    pass\n\n\n# Type definitions for structured data\nclass ChatMessage(TypedDict):\n    role: str\n    content: str\n\n\nclass AnswerResult(TypedDict):\n    answer: str\n    token_id: int\n    is_valid: bool\n    method: str\n    error: Optional[str]\n\n\ndef answer_yes_no(\n    question: str,\n    model_path: ModelType = DEFAULT_MODEL,\n    method: str = \"stream_generate\",\n    max_tokens: int = 1,\n    temperature: float = 0.1,\n    top_p: float = 0.1\n) -> AnswerResult:\n    model_path = resolve_model(model_path)\n\n    try:\n        try:\n            model, tokenizer = load(model_path)\n        except Exception as e:\n            raise ModelLoadError(f\"Error loading model or tokenizer: {e}\")\n\n        if method not in [\"stream_generate\", \"generate_step\"]:\n            raise InvalidMethodError(\n                f\"Invalid method specified: {method}. Valid methods: {['stream_generate', 'generate_step']}\"\n            )\n\n        messages: List[ChatMessage] = [\n            {\"role\": \"system\", \"content\": \"Answer the following question with only 'Yes' or 'No'. Ensure accuracy.\"},\n            {\"role\": \"user\", \"content\": question}\n        ]\n\n        try:\n            formatted_prompt: str = tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n        except Exception as e:\n            raise PromptFormattingError(f\"Error applying chat template: {e}\")\n\n        try:\n            yes_tokens = tokenizer.encode(\"Yes\", add_special_tokens=False)\n            no_tokens = tokenizer.encode(\"No\", add_special_tokens=False)\n\n            yes_token = yes_tokens[0]\n            no_token = no_tokens[0]\n\n            # Print the tokens for \"Yes\" and \"No\"\n            logger.log(\"Token for 'Yes':\",\n                       yes_tokens[0], colors=[\"GRAY\", \"ORANGE\"])\n            logger.log(\"Token for 'No':\",\n                       no_tokens[0], colors=[\"GRAY\", \"ORANGE\"])\n        except Exception as e:\n            raise TokenEncodingError(f\"Error encoding tokens: {e}\")\n\n        logit_bias: Dict[int, float] = {\n            yes_token: 0.0,\n            no_token: 0.0,\n            **{i: -1e9 for i in range(tokenizer.vocab_size) if i not in [yes_token, no_token]}\n        }\n        logits_processors = make_logits_processors(logit_bias=logit_bias)\n        sampler = make_sampler(temp=temperature, top_p=top_p)\n        stop_tokens = tokenizer.encode(\"\\n\") + list(tokenizer.eos_token_ids)\n\n        answer = \"\"\n        token_id = -1\n\n        if method == \"stream_generate\":\n            try:\n                for output in stream_generate(\n                    model=model,\n                    tokenizer=tokenizer,\n                    prompt=formatted_prompt,\n                    max_tokens=max_tokens,\n                    logits_processors=logits_processors,\n                    sampler=sampler\n                ):\n                    if output.token in stop_tokens:\n                        raise InvalidOutputError(\n                            \"Generated token is a stop token.\")\n                    answer += output.text\n                    token_id = output.token\n                    break\n            except Exception as e:\n                raise GenerationError(f\"Error during stream_generate: {e}\")\n        else:\n            try:\n                input_ids = mx.array(tokenizer.encode(\n                    formatted_prompt, add_special_tokens=False))\n                prompt_cache = None\n                for token, _ in generate_step(\n                    model=model,\n                    prompt=input_ids,\n                    max_tokens=max_tokens,\n                    logits_processors=logits_processors,\n                    sampler=sampler,\n                    prompt_cache=prompt_cache\n                )\n                if token in stop_tokens:\n                    raise InvalidOutputError(\n                        \"Generated token is a stop token.\")\n                answer = tokenizer.decode([token])\n                token_id = token\n                break\n            except Exception as e:\n                raise GenerationError(f\"Error during generate_step: {e}\")\n\n        if answer.lower() not in [\"yes\", \"no\"]:\n            raise InvalidOutputError(\"Output is not 'Yes' or 'No'.\")\n\n        return AnswerResult(answer=answer, token_id=token_id, is_valid=True, method=method, error=None)\n\n    except Exception as e:\n        return AnswerResult(answer=\"\", token_id=-1, is_valid=False, method=method, error=str(e))",
  "error": null
}