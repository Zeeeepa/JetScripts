{
  "structure": "Multimodal Data",
  "system": "You are an expert Python developer tasked with generating complete, functional, and well-documented Python code. Based on the provided example, create a Python script that implements the specified structure. The code should:\n- Be syntactically correct and follow PEP 8 style guidelines.\n- Include necessary imports and type hints where applicable.\n- Handle errors gracefully with try-except blocks.\n- Include docstrings and comments for clarity.\n- Be compatible with the existing MLX framework (e.g., use jet.llm.mlx modules).\n- Produce output matching the example structure exactly.\n\nGenerate a complete Python script that implements the provided info. Do not include markdown code fences or any non-Python content. Ensure the script can be saved and run directly.",
  "query": "Generate a Python script for the Multimodal Data structure.",
  "code": "from typing import List, Dict, Optional\nfrom jet.llm.mlx.config import DEFAULT_MODEL\nfrom jet.llm.mlx.mlx_types import ModelType\nfrom jet.llm.mlx.models import resolve_model\nfrom jet.llm.mlx.token_utils import tokenize_strings\nfrom jet.logger import logger\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom mlx_lm.generate import stream_generate, generate_step\nfrom mlx_lm.sample_utils import make_sampler, make_logits_processors\nfrom mlx_lm.utils import TokenizerWrapper\n\nclass MultimodalData:\n    def __init__(self, model_path: ModelType = DEFAULT_MODEL):\n        self.model_path = model_path\n\n    def generate(self, method: str = \"stream_generate\", max_tokens: int = 10, temperature: float = 0.0, top_p: float = 0.9):\n        try:\n            # Validate inputs\n            validate_method(method)\n\n            # Load model and tokenizer\n            model_components = load_model_components(self.model_path)\n\n            # Create and log prompt\n            system_prompt = create_system_prompt()\n            log_prompt_details(system_prompt, \"Question\", self.model_path)\n\n            # Format messages and apply chat template\n            messages = format_chat_messages(system_prompt, \"Question\")\n            formatted_prompt = model_components.tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n\n            # Encode choices and setup generation parameters\n            choice_token_map = encode_choices(model_components.tokenizer, [\"Yes\", \"No\"])\n            logits_processors, sampler, stop_tokens = setup_generation_parameters(\n                model_components.tokenizer, choice_token_map, temperature, top_p\n            )\n\n            # Generate answer based on method\n            if method == \"stream_generate\":\n                answer, token_id, _ = generate_answer_stream(\n                    model_components, formatted_prompt, max_tokens, logits_processors, sampler, stop_tokens, [\"Yes\", \"No\"])\n            else:\n                answer, token_id, _ = generate_answer_step(\n                    model_components, formatted_prompt, max_tokens, logits_processors, sampler, stop_tokens, [\"Yes\", \"No\"])\n\n            # Validate the answer\n            validate_answer(answer, [\"Yes\", \"No\"])\n\n            return AnswerResult(\n                answer=answer,\n                token_id=token_id,\n                is_valid=True,\n                method=method,\n                error=None\n            )",
  "error": null
}