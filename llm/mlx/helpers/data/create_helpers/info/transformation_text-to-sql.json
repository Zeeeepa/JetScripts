{
  "structure": "Text-to-SQL",
  "system": "You are an expert Python developer tasked with generating complete, functional, and well-documented Python code. Based on the provided example, create a Python script that implements the specified structure. The code should:\n- Be syntactically correct and follow PEP 8 style guidelines.\n- Include necessary imports and type hints where applicable.\n- Handle errors gracefully with try-except blocks.\n- Include docstrings and comments for clarity.\n- Be compatible with the existing MLX framework (e.g., use jet.llm.mlx modules).\n- Produce output matching the example structure exactly.\n\nGenerate a complete Python script that implements the provided info. Do not include markdown code fences or any non-Python content. Ensure the script can be saved and run directly.",
  "query": "Generate a Python script for the Text-to-SQL structure.",
  "code": "from typing import List, Dict, Optional, TypedDict\nfrom uuid import uuid4\nfrom jet.llm.mlx.config import DEFAULT_MODEL\nfrom jet.llm.mlx.mlx_types import ModelType\nfrom jet.llm.mlx.models import resolve_model\nfrom jet.llm.mlx.token_utils import tokenize_strings\nfrom jet.logger import logger\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom mlx_lm.generate import stream_generate, generate_step\nfrom mlx_lm.sample_utils import make_sampler, make_logits_processors\nfrom mlx_lm.utils import TokenizerWrapper\n\n# Custom exceptions for specific error cases\n\n\nclass ModelLoadError(Exception):\n    pass\n\n\nclass InvalidMethodError(Exception):\n    pass\n\n\nclass InvalidOutputError(Exception):\n    pass\n\n# Type definitions for structured data\n\n\nclass ChatMessage(TypedDict):\n    role: str\n    content: str\n\n\nclass AnswerResult(TypedDict):\n    answer: str\n    token_id: int\n    is_valid: bool\n    method: str\n    error: Optional[str]\n\n\nclass ModelComponents:\n    \"\"\"Encapsulates model and tokenizer for easier management.\"\"\"\n\n    def __init__(self, model, tokenizer: TokenizerWrapper):\n        self.model = model\n        self.tokenizer = tokenizer\n\n\ndef load_model_components(model_path: ModelType) -> ModelComponents:\n    \"\"\"Loads model and tokenizer from the specified path.\"\"\"\n    try:\n        model, tokenizer = load(resolve_model(model_path))\n        return ModelComponents(model, tokenizer)\n    except Exception as e:\n        raise ModelLoadError(f\"Error loading model or tokenizer: {e}\")\n\n\ndef validate_method(method: str) -> None:\n    \"\"\"Validates the generation method.\"\"\"\n    valid_methods = [\"stream_generate\", \"generate_step\"]\n    if method not in valid_methods:\n        raise InvalidMethodError(\n            f\"Invalid method specified: {method}. Valid methods: {valid_methods}\")\n\n\ndef create_system_prompt(choices: List[str]) -> str:\n    \"\"\"Creates a formatted system prompt with the given choices.\"\"\"\n    return f\"Answer the following question by choosing one of the options provided without any additional text.\\nOptions:\\n{'\\n'.join(choices)}\"\n\n\ndef log_prompt_details(system_prompt: str, question: str, model_path: ModelType) -> None:\n    \"\"\"Logs system prompt, tokenized system prompt, and user question for debugging.\"\"\"",
  "error": null
}