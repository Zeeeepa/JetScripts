{
  "structure": "Code and Output",
  "system": "You are an expert Python developer tasked with generating complete, functional, and well-documented Python code. Based on the provided example, create a Python script that implements the specified structure. The code should:\n- Be syntactically correct and follow PEP 8 style guidelines.\n- Include necessary imports and type hints where applicable.\n- Handle errors gracefully with try-except blocks.\n- Include docstrings and comments for clarity.\n- Be compatible with the existing MLX framework (e.g., use jet.llm.mlx modules).\n- Produce output matching the example structure exactly.\n\nGenerate a complete Python script that implements the provided info. Do not include markdown code fences or any non-Python content. Ensure the script can be saved and run directly.",
  "query": "Generate a Python script for the Code and Output structure.",
  "code": "# Code\nfrom typing import List, Dict, Optional, TypedDict\nfrom uuid import uuid4\nfrom jet.llm.mlx.config import DEFAULT_MODEL\nfrom jet.llm.mlx.mlx_types import ModelType\nfrom jet.llm.mlx.models import resolve_model\nfrom jet.llm.mlx.token_utils import tokenize_strings\nfrom jet.logger import logger\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom mlx_lm.generate import stream_generate, generate_step\nfrom mlx_lm.sample_utils import make_sampler, make_logits_processors\nfrom mlx_lm.utils import TokenizerWrapper\n\n# Custom exceptions\nclass ModelLoadError(Exception):\n    pass\n\nclass InvalidMethodError(Exception):\n    pass\n\nclass InvalidOutputError(Exception):\n    pass\n\n# Type definitions\nclass ChatMessage(TypedDict):\n    role: str\n    content: str\n\nclass AnswerResult(TypedDict):\n    answer: str\n    token_id: int\n    is_valid: bool\n    method: str\n    error: Optional[str]\n\n# Function definitions\ndef load_model_components(model_path: ModelType = DEFAULT_MODEL) -> ModelComponents:\n    \"\"\"Loads model and tokenizer from the specified path.\"\"\"\n    try:\n        model, tokenizer = load(resolve_model(model_path))\n        return ModelComponents(model, tokenizer)\n    except Exception as e:\n        raise ModelLoadError(f\"Error loading model or tokenizer: {e}\")\n\ndef validate_method(method: str) -> None:\n    \"\"\"Validates the generation method.\"\"\"\n    valid_methods = [\"stream_generate\", \"generate_step\"]\n    if method not in valid_methods:\n        raise InvalidMethodError(\n            f\"Invalid method specified: {method}. Valid methods: {valid_methods}\"\n\ndef create_system_prompt(choices: List[str]) -> str:\n    \"\"\"Creates a formatted system prompt with the given choices.\"\"\"\n    return f\"Answer the following question by choosing one of the options provided without any additional text.\\nOptions:\\n{'\\n'.join(choices)}\"\n\ndef log_prompt_details(system_prompt: str, question: str, model_path: ModelType) -> None:\n    \"\"\"Logs system prompt, tokenized system prompt, and user question for debugging.\"\"\"\n    logger.gray(\"System:\")\n    logger.debug(system_prompt)\n    logger.gray(\"Tokenized System:\")\n    logger.debug(tokenize_strings(system_prompt, model_path))\n    logger.gray(\"User:\")\n    logger.debug(question)\n    logger.newline()\n\ndef format_chat_messages(system_prompt: str, question: str) -> List[ChatMessage]:\n    \"\"\"Formats the system and user messages for the chat template.\"\"\"\n    return [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": question}\n    ]\n\ndef encode_choices(tokenizer: TokenizerWrapper, choices: List[str]) -> Dict[str, List[int]]:\n    \"\"\"Encodes each choice into tokens and logs the results.\"\"\"\n    choice_token_map = {}\n    for choice in choices:\n        tokens = tokenizer.encode(choice, add_special_tokens=False)\n        choice_token_map[choice] = tokens\n        logger.log(f\"Tokens for '{choice}':\",\n                   tokens, colors=[\"GRAY\", \"ORANGE\"])\n    return choice_token_map\n\ndef setup_generation_parameters(\n    tokenizer: TokenizerWrapper,\n    choice_token_map: Dict[str, List[int]],\n    temperature: float,\n    top_p: float\n) -> tuple:\n    \"\"\"Sets up logit bias, logits processors, sampler, and stop tokens for generation.\"\"\"\n    logit_bias = {tokens[0]: 0.0 for choice,\n                  tokens in choice_token_map.items() if tokens}\n    logits_processors = make_logits_processors(logit_bias=logit_bias)\n    sampler = make_sampler(temp=temperature, top_p=top_p)\n    stop_tokens = tokenizer.encode(\"\\n\") + list(tokenizer.eos_token_ids)\n    return logits_processors, sampler, stop_tokens\n\ndef generate_answer_stream(\n    model_components: ModelComponents,\n    formatted_prompt: str,\n    max_tokens: int,\n    logits_processors: list,\n    sampler,\n    stop_tokens: List[int],\n    choices: List[str]\n) -> tuple[str, int, List[int]]:\n    \"\"\"Generates an answer using stream_generate method.\"\"\"\n    answer = \"\"\n    token_id = -1\n    generated_tokens = []\n\n    for output in stream_generate(\n        model=model_components.model,\n        tokenizer=model_components.tokenizer,\n        prompt=formatted_prompt,\n        max_tokens=max_tokens,\n        logits_processors=logits_processors,\n        sampler=sampler\n    ):\n        if output.token in stop_tokens:\n            break\n        generated_tokens.append(output.token)\n        token_id = output.token\n        answer = model_components.tokenizer.decode(generated_tokens)\n        if answer in choices:\n            break\n\n    return answer, token_id, generated_tokens\n\ndef generate_answer_step(\n    model_components: ModelComponents,\n    formatted_prompt: str,\n    max_tokens: int,\n    logits_processors: list,\n    sampler,\n    stop_tokens: List[int],\n    choices: List[str]\n) -> tuple[str, int, List[int]]:\n    \"\"\"Generates an answer using generate_step method.\"\"\"\n    answer = \"\"\n    token_id = -1\n    generated_tokens = []\n\n    input_ids = mx.array(model_components.tokenizer.encode(\n        formatted_prompt, add_special_tokens=False))\n    prompt_cache = None\n\n    for token, _ in generate_step(\n        model=model_components.model,\n        prompt=input_ids,\n        max_tokens=max_tokens,\n        logits_processors=logits_processors,\n        sampler=sampler,\n        prompt_cache=prompt_cache\n    ):\n        if token in stop_tokens:\n            break\n        generated_tokens.append(token)\n        token_id = token\n        answer = model_components.tokenizer.decode(generated_tokens)\n        if answer in choices:\n            break\n\n    return answer, token_id, generated_tokens\n\ndef validate_answer(answer: str, choices: List[str]) -> None:\n    \"\"\"Validates that the generated answer is one of the provided choices.\"\"\"\n    if answer not in choices:\n        raise InvalidOutputError(\n            f\"Output '{answer}' is not one of the provided choices.\"\n\ndef answer_multiple_choice(\n    question: str,\n    choices: List[str],\n    model_path: ModelType = DEFAULT_MODEL,\n    method: str = \"stream_generate\",\n    max_tokens: int = 10,\n    temperature: float = 0.0,\n    top_p: float = 0.9\n) -> AnswerResult:\n    \"\"\"Answers a multiple-choice question using a language model.\"\"\"\n    try:\n        # Validate inputs\n        validate_method(method)\n\n        # Load model and tokenizer\n        model_components = load_model_components(model_path)\n\n        # Create and log prompt\n        system_prompt = create_system_prompt(choices)\n        log_prompt_details(system_prompt, question, model_path)\n\n        # Format messages and apply chat template\n        messages = format_chat_messages(system_prompt, question)\n        formatted_prompt = model_components.tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        # Encode choices and setup generation parameters\n        choice_token_map = encode_choices(model_components.tokenizer, choices)\n        logits_processors, sampler, stop_tokens = setup_generation_parameters(\n            model_components.tokenizer, choice_token_map, temperature, top_p\n        )\n\n        # Generate answer based on method\n        if method == \"stream_generate\":\n            answer, token_id, _ = generate_answer_stream(\n                model_components, formatted_prompt, max_tokens,\n                logits_processors, sampler, stop_tokens, choices\n            )\n        else:\n            answer, token_id, _ = generate_answer_step(\n                model_components, formatted_prompt, max_tokens,\n                logits_processors, sampler, stop_tokens, choices\n            )\n\n        # Validate the answer\n        validate_answer(answer, choices)\n\n        return AnswerResult(\n            answer=answer,\n            token_id=token_id,\n            is_valid=True,\n            method=method,\n            error=None\n        )\n\n    except Exception as e:\n        return AnswerResult(\n            answer=\"\",\n            token_id=-1,\n            is_valid=False,\n            method=method,\n            error=str(e)\n        )",
  "error": null
}