[
  {
    "id": "944f25eb13eebbf8771cfd0f23c3a04f",
    "rank": 1,
    "doc_index": 5,
    "score": 0.6079847812652588,
    "percent_difference": 0.0,
    "text": "## Chunking the Extracted Text\nOnce we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy.",
    "relevance": null,
    "word_count": 26
  },
  {
    "id": "d20123565d5fa6f9dce11a5e541cf11d",
    "rank": 2,
    "doc_index": 9,
    "score": 0.529926061630249,
    "percent_difference": 12.84,
    "text": "## Extracting and Chunking Text from a PDF File\nNow, we load the PDF, extract text, and split it into chunks.",
    "relevance": null,
    "word_count": 26
  },
  {
    "id": "9b27730e7633bcc4e44da8b067ac906d",
    "rank": 3,
    "doc_index": 10,
    "score": 0.5252268314361572,
    "percent_difference": 13.61,
    "text": "## Extracting and Chunking Text from a PDF File\nCode:\n# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\ntext_chunks = chunk_text(extracted_text, 1000, 200)\n# Print the number of text chunks created\nprint(\"Number of text chunks:\", len(text_chunks))\n# Print the first text chunk\nprint(\"nFirst text chunk:\")\nprint(text_chunks[0])",
    "relevance": null,
    "word_count": 109
  },
  {
    "id": "fc525f4d8d7a82a57f0948613616fe75",
    "rank": 4,
    "doc_index": 6,
    "score": 0.4944360554218292,
    "percent_difference": 18.68,
    "text": "## Chunking the Extracted Text\nCode:\ndef chunk_text(text, n, overlap):\n\"\"\"\nChunks the given text into segments of n characters with overlap.\nArgs:\ntext (str): The text to be chunked.\nn (int): The number of characters in each chunk.\noverlap (int): The number of overlapping characters between chunks.\nReturns:\nList[str]: A list of text chunks.\n\"\"\"\nchunks = []  # Initialize an empty list to store the chunks\n# Loop through the text with a step size of (n - overlap)\nfor i in range(0, len(text), n - overlap):\n# Append a chunk of text from index i to i + n to the chunks list\nchunks.append(text[i:i + n])\nreturn chunks  # Return the list of text chunks",
    "relevance": null,
    "word_count": 169
  },
  {
    "id": "d9cdea86cc3822c5e3ac1b315f8102b2",
    "rank": 5,
    "doc_index": 11,
    "score": 0.4374611973762512,
    "percent_difference": 28.05,
    "text": "## Creating Embeddings for Text Chunks\nEmbeddings transform text into numerical vectors, which allow for efficient similarity search.",
    "relevance": null,
    "word_count": 21
  },
  {
    "id": "0337018cc08ac7ae70524f1aad161325",
    "rank": 6,
    "doc_index": 16,
    "score": 0.40190744400024414,
    "percent_difference": 33.9,
    "text": "## Running a Query with Context Retrieval\nCode:\n# Load the validation dataset from a JSON file\nwith open('data/val.json') as f:\ndata = json.load(f)\n# Extract the first question from the dataset to use as our query\nquery = data[0]['question']\n# Retrieve the most relevant chunk and its neighboring chunks for context\n# Parameters:\n# - query: The question we're searching for\n# - text_chunks: Our text chunks extracted from the PDF\n# - response.data: The embeddings of our text chunks\n# - k=1: Return the top match\n# - context_size=1: Include 1 chunk before and after the top match for context\ntop_chunks = context_enriched_search(query, text_chunks, response.data, k=1, context_size=1)\n# Print the query for reference\nprint(\"Query:\", query)\n# Print each retrieved chunk with a heading and separator\nfor i, chunk in enumerate(top_chunks):\nprint(f\"Context {i + 1}:n{chunk}n=====================================\")",
    "relevance": null,
    "word_count": 193
  },
  {
    "id": "e9d9087f58d21f5dc0124d1fede6425f",
    "rank": 7,
    "doc_index": 12,
    "score": 0.3754081726074219,
    "percent_difference": 38.25,
    "text": "## Creating Embeddings for Text Chunks\nCode:\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreates embeddings for the given text using the specified OpenAI model.\nArgs:\ntext (str): The input text for which embeddings are to be created.\nmodel (str): The model to be used for creating embeddings. Default is \"BAAI/bge-en-icl\".\nReturns:\ndict: The response from the OpenAI API containing the embeddings.\n\"\"\"\n# Create embeddings for the input text using the specified model\nresponse = client.embeddings.create(\nmodel=model,\ninput=text\n)\nreturn response  # Return the response containing the embeddings\n# Create embeddings for the text chunks\nresponse = create_embeddings(text_chunks)",
    "relevance": null,
    "word_count": 130
  },
  {
    "id": "257ce92621f8dc5b7ef94eb037b5156b",
    "rank": 8,
    "doc_index": 0,
    "score": 0.36965131759643555,
    "percent_difference": 39.2,
    "text": "## Context-Enriched Retrieval in RAG\nRetrieval-Augmented Generation (RAG) enhances AI responses by retrieving relevant knowledge from external sources. Traditional retrieval methods return isolated text chunks, which can lead to incomplete answers.\nTo address this, we introduce Context-Enriched Retrieval, which ensures that retrieved information includes neighboring chunks for better coherence.\nSteps in This Notebook:\n- Data Ingestion: Extract text from a PDF.\n- Chunking with Overlapping Context: Split text into overlapping chunks to preserve context.\n- Embedding Creation: Convert text chunks into numerical representations.\n- Context-Aware Retrieval: Retrieve relevant chunks along with their neighbors for better completeness.\n- Response Generation: Use a language model to generate responses based on retrieved context.\n- Evaluation: Assess the model's response accuracy.",
    "relevance": null,
    "word_count": 140
  },
  {
    "id": "1c6de06906d62ae66ebe23f50f89ee97",
    "rank": 9,
    "doc_index": 13,
    "score": 0.293728768825531,
    "percent_difference": 51.69,
    "text": "## Implementing Context-Aware Semantic Search\nWe modify retrieval to include neighboring chunks for better context.",
    "relevance": null,
    "word_count": 17
  },
  {
    "id": "ddc12ca461fde46206b59ce349e18085",
    "rank": 10,
    "doc_index": 3,
    "score": 0.2566831111907959,
    "percent_difference": 57.78,
    "text": "## Extracting Text from a PDF File\nTo implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library.",
    "relevance": null,
    "word_count": 37
  },
  {
    "id": "9eedf73b166ccdc909218eba6e69dd72",
    "rank": 11,
    "doc_index": 4,
    "score": 0.18455010652542114,
    "percent_difference": 69.65,
    "text": "## Extracting Text from a PDF File\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file and prints the first `num_chars` characters.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text",
    "relevance": null,
    "word_count": 143
  },
  {
    "id": "a99acb4bbbcef42045c232de6ade6604",
    "rank": 12,
    "doc_index": 18,
    "score": 0.16188665355245271,
    "percent_difference": 73.37,
    "text": "## Generating a Response Using Retrieved Context\nCode:\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates a response from the AI model based on the system prompt and user message.\nArgs:\nsystem_prompt (str): The system prompt to guide the AI's behavior.\nuser_message (str): The user's message or query.\nmodel (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\nReturns:\ndict: The response from the AI model.\n\"\"\"\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_message}\n]\n)\nreturn response\n# Create the user prompt based on the top chunks\nuser_prompt = \"n\".join([f\"Context {i + 1}:n{chunk}n=====================================n\" for i, chunk in enumerate(top_chunks)])\nuser_prompt = f\"{user_prompt}nQuestion: {query}\"\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)",
    "relevance": null,
    "word_count": 274
  },
  {
    "id": "9534063c1ad930c9d6f075bc3aa651f7",
    "rank": 13,
    "doc_index": 15,
    "score": 0.1349826604127884,
    "percent_difference": 77.8,
    "text": "## Running a Query with Context Retrieval\nWe now test the context-enriched retrieval.",
    "relevance": null,
    "word_count": 15
  },
  {
    "id": "a6030ec345f66aaba95320f7e51e8156",
    "rank": 14,
    "doc_index": 20,
    "score": 0.09838847070932388,
    "percent_difference": 83.82,
    "text": "## Evaluating the AI Response\nCode:\n# Define the system prompt for the evaluation system\nevaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\nevaluation_prompt = f\"User Query: {query}nAI Response:n{ai_response.choices[0].message.content}nTrue Response: {data[0]['ideal_answer']}n{evaluate_system_prompt}\"\n# Generate the evaluation response using the evaluation system prompt and evaluation prompt\nevaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n# Print the evaluation response\nprint(evaluation_response.choices[0].message.content)",
    "relevance": null,
    "word_count": 187
  },
  {
    "id": "0430b10c43379bd27f0a6b6f3d3f3ae2",
    "rank": 15,
    "doc_index": 19,
    "score": 0.09619999676942825,
    "percent_difference": 84.18,
    "text": "## Evaluating the AI Response\nWe compare the AI response with the expected answer and assign a score.",
    "relevance": null,
    "word_count": 20
  },
  {
    "id": "b93ea725f8e7917f6ec32ebd896ac8b3",
    "rank": 16,
    "doc_index": 17,
    "score": 0.08761507272720337,
    "percent_difference": 85.59,
    "text": "## Generating a Response Using Retrieved Context\nWe now generate a response using LLM.",
    "relevance": null,
    "word_count": 16
  },
  {
    "id": "94052a55d4f2f654a85c9cc659c6f18e",
    "rank": 17,
    "doc_index": 2,
    "score": 0.048928402364254,
    "percent_difference": 91.95,
    "text": "## Setting Up the Environment\nCode:\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI",
    "relevance": null,
    "word_count": 22
  },
  {
    "id": "ec06847c02eaf5195ff95315d2a8295a",
    "rank": 18,
    "doc_index": 7,
    "score": 0.03732878342270851,
    "percent_difference": 93.86,
    "text": "## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.",
    "relevance": null,
    "word_count": 19
  },
  {
    "id": "53b6e66239969bc2f4e5a8dd71ab2cf1",
    "rank": 19,
    "doc_index": 1,
    "score": 0.027201073244214058,
    "percent_difference": 95.53,
    "text": "## Setting Up the Environment\nWe begin by importing necessary libraries.",
    "relevance": null,
    "word_count": 13
  },
  {
    "id": "6c3ee1acca5fd41d4f638e78e59023ae",
    "rank": 20,
    "doc_index": 14,
    "score": 0.003732644021511078,
    "percent_difference": 99.39,
    "text": "## Implementing Context-Aware Semantic Search\nCode:\ndef cosine_similarity(vec1, vec2):\n\"\"\"\nCalculates the cosine similarity between two vectors.\nArgs:\nvec1 (np.ndarray): The first vector.\nvec2 (np.ndarray): The second vector.\nReturns:\nfloat: The cosine similarity between the two vectors.\n\"\"\"\n# Compute the dot product of the two vectors and divide by the product of their norms\nreturn np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))",
    "relevance": null,
    "word_count": 97
  }
]