[
  {
    "id": "d20123565d5fa6f9dce11a5e541cf11d",
    "rank": 1,
    "doc_index": 10,
    "score": 0.529926061630249,
    "percent_difference": 0.0,
    "text": "## Extracting and Chunking Text from a PDF File\nNow, we load the PDF, extract text, and split it into chunks.",
    "relevance": null,
    "word_count": 26
  },
  {
    "id": "0e9c4071eb6ad96d7bad26a343070bba",
    "rank": 2,
    "doc_index": 8,
    "score": 0.5213038921356201,
    "percent_difference": 1.63,
    "text": "## Chunking Text with Contextual Headers\nTo improve retrieval, we generate descriptive headers for each chunk using an LLM model.",
    "relevance": null,
    "word_count": 23
  },
  {
    "id": "ace31e4d0e3d6d99cce8deab6d43c6ce",
    "rank": 3,
    "doc_index": 11,
    "score": 0.5077205896377563,
    "percent_difference": 4.19,
    "text": "## Extracting and Chunking Text from a PDF File\nCode:\n# Define the PDF file path\npdf_path = \"data/AI_Information.pdf\"\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n# Chunk the extracted text with headers\n# We use a chunk size of 1000 characters and an overlap of 200 characters\ntext_chunks = chunk_text_with_headers(extracted_text, 1000, 200)\n# Print a sample chunk with its generated header\nprint(\"Sample Chunk:\")\nprint(\"Header:\", text_chunks[0]['header'])\nprint(\"Content:\", text_chunks[0]['text'])",
    "relevance": null,
    "word_count": 117
  },
  {
    "id": "23ad80f0cd047cb1093a6a226586f798",
    "rank": 4,
    "doc_index": 1,
    "score": 0.4987434148788452,
    "percent_difference": 5.88,
    "text": "## Steps in this Notebook:\n1. **Data Ingestion**: Load and preprocess the text data.\n2. **Chunking with Contextual Headers**: Extract section titles and prepend them to chunks.\n3. **Embedding Creation**: Convert context-enhanced chunks into numerical representations.\n4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n5. **Response Generation**: Use a language model to generate a response from retrieved text.\n6. **Evaluation**: Assess response accuracy using a scoring system.",
    "relevance": null,
    "word_count": 114
  },
  {
    "id": "5be0368362b8e18b7a6baa05451e3f22",
    "rank": 5,
    "doc_index": 16,
    "score": 0.47100913524627686,
    "percent_difference": 11.12,
    "text": "## Running a Query on Extracted Chunks\nCode:\n# Load validation data\nwith open('data/val.json') as f:\ndata = json.load(f)\nquery = data[0]['question']\n# Retrieve the top 2 most relevant text chunks\ntop_chunks = semantic_search(query, embeddings, k=2)\n# Print the results\nprint(\"Query:\", query)\nfor i, chunk in enumerate(top_chunks):\nprint(f\"Header {i+1}: {chunk['header']}\")\nprint(f\"Content:n{chunk['text']}n\")",
    "relevance": null,
    "word_count": 116
  },
  {
    "id": "ab783bdc8aa3a5f9b890f5714044f42b",
    "rank": 6,
    "doc_index": 4,
    "score": 0.4404677450656891,
    "percent_difference": 16.88,
    "text": "## Extracting Text and Identifying Section Headers\nWe extract text from a PDF while also identifying section titles (potential headers for chunks).",
    "relevance": null,
    "word_count": 26
  },
  {
    "id": "03af90633763e1840efb78b09cab447b",
    "rank": 7,
    "doc_index": 0,
    "score": 0.3572082221508026,
    "percent_difference": 32.59,
    "text": "# Contextual Chunk Headers (CCH) in Simple RAG\nRetrieval-Augmented Generation (RAG) improves the factual accuracy of language models by retrieving relevant external knowledge before generating a response. However, standard chunking often loses important context, making retrieval less effective.\nContextual Chunk Headers (CCH) enhance RAG by prepending high-level context (like document titles or section headers) to each chunk before embedding them. This improves retrieval quality and prevents out-of-context responses.",
    "relevance": null,
    "word_count": 82
  },
  {
    "id": "b9f091393a59debaeeac27add762e67b",
    "rank": 8,
    "doc_index": 9,
    "score": 0.34966228157281876,
    "percent_difference": 34.02,
    "text": "## Chunking Text with Contextual Headers\nCode:\ndef generate_chunk_header(chunk, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates a title/header for a given text chunk using an LLM.\nArgs:\nchunk (str): The text chunk to summarize as a header.\nmodel (str): The model to be used for generating the header. Default is \"meta-llama/Llama-3.2-3B-Instruct\".\nReturns:\nstr: Generated header/title.\n\"\"\"\n# Define the system prompt to guide the AI's behavior\nsystem_prompt = \"Generate a concise and informative title for the given text.\"\n# Generate a response from the AI model based on the system prompt and text chunk\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": chunk}\n]\n)\n# Return the generated header/title, stripping any leading/trailing whitespace\nreturn response.choices[0].message.content.strip()",
    "relevance": null,
    "word_count": 184
  },
  {
    "id": "8554438c02e2093b2a7b3719348ae5a9",
    "rank": 9,
    "doc_index": 14,
    "score": 0.31285378336906433,
    "percent_difference": 40.96,
    "text": "## Performing Semantic Search\nWe implement cosine similarity to find the most relevant text chunks for a user query.",
    "relevance": null,
    "word_count": 21
  },
  {
    "id": "4cabff9ffb8cb5def2f4d3b15f3051a1",
    "rank": 10,
    "doc_index": 17,
    "score": 0.26084640497962636,
    "percent_difference": 50.78,
    "text": "## Generating a Response Based on Retrieved Chunks\nCode:\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates a response from the AI model based on the system prompt and user message.\nArgs:\nsystem_prompt (str): The system prompt to guide the AI's behavior.\nuser_message (str): The user's message or query.\nmodel (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\nReturns:\ndict: The response from the AI model.\n\"\"\"\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_message}\n]\n)\nreturn response\n# Create the user prompt based on the top chunks\nuser_prompt = \"n\".join([f\"Header: {chunk['header']}nContent:n{chunk['text']}\" for chunk in top_chunks])\nuser_prompt = f\"{user_prompt}nQuestion: {query}\"\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)",
    "relevance": null,
    "word_count": 277
  },
  {
    "id": "35a8d1cb15863d6574f62fcdbe22f2f6",
    "rank": 11,
    "doc_index": 5,
    "score": 0.22110557556152344,
    "percent_difference": 58.28,
    "text": "## Extracting Text and Identifying Section Headers\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file and prints the first `num_chars` characters.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text",
    "relevance": null,
    "word_count": 143
  },
  {
    "id": "6c2241bcf8ef3f3a8b8804aff34e6b2a",
    "rank": 12,
    "doc_index": 12,
    "score": 0.16913604736328125,
    "percent_difference": 68.08,
    "text": "## Creating Embeddings for Headers and Text\nWe create embeddings for both headers and text to improve retrieval accuracy.",
    "relevance": null,
    "word_count": 21
  },
  {
    "id": "70d5d9dd3a06de1c5399b9e2ce8a23ea",
    "rank": 13,
    "doc_index": 13,
    "score": 0.1402696669101715,
    "percent_difference": 73.53,
    "text": "## Creating Embeddings for Headers and Text\nCode:\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreates embeddings for the given text.\nArgs:\ntext (str): The input text to be embedded.\nmodel (str): The embedding model to be used. Default is \"BAAI/bge-en-icl\".\nReturns:\ndict: The response containing the embedding for the input text.\n\"\"\"\n# Create embeddings using the specified model and input text\nresponse = client.embeddings.create(\nmodel=model,\ninput=text\n)\n# Return the embedding from the response\nreturn response.data[0].embedding",
    "relevance": null,
    "word_count": 110
  },
  {
    "id": "2566660a6faafff06cf8189d8e418df7",
    "rank": 14,
    "doc_index": 19,
    "score": 0.11874649301171303,
    "percent_difference": 77.59,
    "text": "## Evaluating the AI Response\nCode:\n# Define evaluation system prompt\nevaluate_system_prompt = \"\"\"You are an intelligent evaluation system.\nAssess the AI assistant's response based on the provided context.\n- Assign a score of 1 if the response is very close to the true answer.\n- Assign a score of 0.5 if the response is partially correct.\n- Assign a score of 0 if the response is incorrect.\nReturn only the score (0, 0.5, or 1).\"\"\"\n# Extract the ground truth answer from validation data\ntrue_answer = data[0]['ideal_answer']\n# Construct evaluation prompt\nevaluation_prompt = f\"\"\"\nUser Query: {query}\nAI Response: {ai_response}\nTrue Answer: {true_answer}\n{evaluate_system_prompt}\n\"\"\"\n# Generate evaluation score\nevaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n# Print the evaluation score\nprint(\"Evaluation Score:\", evaluation_response.choices[0].message.content)",
    "relevance": null,
    "word_count": 179
  },
  {
    "id": "0430b10c43379bd27f0a6b6f3d3f3ae2",
    "rank": 15,
    "doc_index": 18,
    "score": 0.09619999676942825,
    "percent_difference": 81.85,
    "text": "## Evaluating the AI Response\nWe compare the AI response with the expected answer and assign a score.",
    "relevance": null,
    "word_count": 20
  },
  {
    "id": "00e73ca64aaf39e5f9e42e8e46705f1f",
    "rank": 16,
    "doc_index": 3,
    "score": 0.06520255655050278,
    "percent_difference": 87.7,
    "text": "## Setting Up the Environment\nCode:\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\nimport fitz\nfrom tqdm import tqdm",
    "relevance": null,
    "word_count": 26
  },
  {
    "id": "ec06847c02eaf5195ff95315d2a8295a",
    "rank": 17,
    "doc_index": 6,
    "score": 0.03732878342270851,
    "percent_difference": 92.96,
    "text": "## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.",
    "relevance": null,
    "word_count": 19
  },
  {
    "id": "53b6e66239969bc2f4e5a8dd71ab2cf1",
    "rank": 18,
    "doc_index": 2,
    "score": 0.027201073244214058,
    "percent_difference": 94.87,
    "text": "## Setting Up the Environment\nWe begin by importing necessary libraries.",
    "relevance": null,
    "word_count": 13
  },
  {
    "id": "0793e20e45f830b7aec05717b92ef98e",
    "rank": 19,
    "doc_index": 15,
    "score": 0.010226093232631683,
    "percent_difference": 98.07,
    "text": "## Performing Semantic Search\nCode:\ndef cosine_similarity(vec1, vec2):\n\"\"\"\nComputes cosine similarity between two vectors.\nArgs:\nvec1 (np.ndarray): First vector.\nvec2 (np.ndarray): Second vector.\nReturns:\nfloat: Cosine similarity score.\n\"\"\"\nreturn np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))",
    "relevance": null,
    "word_count": 72
  }
]