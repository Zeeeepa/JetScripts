[
  "## Extracting and Chunking Text from a PDF File\nNow, we load the PDF, extract text, and split it into chunks.\n## Chunking Text with Contextual Headers\nTo improve retrieval, we generate descriptive headers for each chunk using an LLM model.\n## Extracting and Chunking Text from a PDF File\nCode:\n# Define the PDF file path\npdf_path = \"data/AI_Information.pdf\"\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n# Chunk the extracted text with headers\n# We use a chunk size of 1000 characters and an overlap of 200 characters\ntext_chunks = chunk_text_with_headers(extracted_text, 1000, 200)\n# Print a sample chunk with its generated header\nprint(\"Sample Chunk:\")\nprint(\"Header:\", text_chunks[0]['header'])\nprint(\"Content:\", text_chunks[0]['text'])\n## Steps in this Notebook:\n1. **Data Ingestion**: Load and preprocess the text data.\n2. **Chunking with Contextual Headers**: Extract section titles and prepend them to chunks.\n3. **Embedding Creation**: Convert context-enhanced chunks into numerical representations.\n4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n5. **Response Generation**: Use a language model to generate a response from retrieved text.\n6. **Evaluation**: Assess response accuracy using a scoring system.",
  "## Running a Query on Extracted Chunks\nCode:\n# Load validation data\nwith open('data/val.json') as f:\ndata = json.load(f)\nquery = data[0]['question']\n# Retrieve the top 2 most relevant text chunks\ntop_chunks = semantic_search(query, embeddings, k=2)\n# Print the results\nprint(\"Query:\", query)\nfor i, chunk in enumerate(top_chunks):\nprint(f\"Header {i+1}: {chunk['header']}\")\nprint(f\"Content:n{chunk['text']}n\")\n## Extracting Text and Identifying Section Headers\nWe extract text from a PDF while also identifying section titles (potential headers for chunks).\n# Contextual Chunk Headers (CCH) in Simple RAG\nRetrieval-Augmented Generation (RAG) improves the factual accuracy of language models by retrieving relevant external knowledge before generating a response. However, standard chunking often loses important context, making retrieval less effective.\nContextual Chunk Headers (CCH) enhance RAG by prepending high-level context (like document titles or section headers) to each chunk before embedding them. This improves retrieval quality and prevents out-of-context responses.",
  "## Chunking Text with Contextual Headers\nCode:\ndef generate_chunk_header(chunk, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates a title/header for a given text chunk using an LLM.\nArgs:\nchunk (str): The text chunk to summarize as a header.\nmodel (str): The model to be used for generating the header. Default is \"meta-llama/Llama-3.2-3B-Instruct\".\nReturns:\nstr: Generated header/title.\n\"\"\"\n# Define the system prompt to guide the AI's behavior\nsystem_prompt = \"Generate a concise and informative title for the given text.\"\n# Generate a response from the AI model based on the system prompt and text chunk\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": chunk}\n]\n)\n# Return the generated header/title, stripping any leading/trailing whitespace\nreturn response.choices[0].message.content.strip()\n## Performing Semantic Search\nWe implement cosine similarity to find the most relevant text chunks for a user query.\n## Generating a Response Based on Retrieved Chunks\nCode:",
  "# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates a response from the AI model based on the system prompt and user message.\nArgs:\nsystem_prompt (str): The system prompt to guide the AI's behavior.\nuser_message (str): The user's message or query.\nmodel (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\nReturns:\ndict: The response from the AI model.\n\"\"\"\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_message}\n]\n)\nreturn response\n# Create the user prompt based on the top chunks\nuser_prompt = \"n\".join([f\"Header: {chunk['header']}nContent:n{chunk['text']}\" for chunk in top_chunks])\nuser_prompt = f\"{user_prompt}nQuestion: {query}\"\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)",
  "## Extracting Text and Identifying Section Headers\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file and prints the first `num_chars` characters.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text\n## Creating Embeddings for Headers and Text\nWe create embeddings for both headers and text to improve retrieval accuracy.\n## Creating Embeddings for Headers and Text\nCode:\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreates embeddings for the given text.\nArgs:\ntext (str): The input text to be embedded.\nmodel (str): The embedding model to be used. Default is \"BAAI/bge-en-icl\".\nReturns:\ndict: The response containing the embedding for the input text.\n\"\"\"",
  "# Create embeddings using the specified model and input text\nresponse = client.embeddings.create(\nmodel=model,\ninput=text\n)\n# Return the embedding from the response\nreturn response.data[0].embedding\n## Evaluating the AI Response\nCode:\n# Define evaluation system prompt\nevaluate_system_prompt = \"\"\"You are an intelligent evaluation system.\nAssess the AI assistant's response based on the provided context.\n- Assign a score of 1 if the response is very close to the true answer.\n- Assign a score of 0.5 if the response is partially correct.\n- Assign a score of 0 if the response is incorrect.\nReturn only the score (0, 0.5, or 1).\"\"\"\n# Extract the ground truth answer from validation data\ntrue_answer = data[0]['ideal_answer']\n# Construct evaluation prompt\nevaluation_prompt = f\"\"\"\nUser Query: {query}\nAI Response: {ai_response}\nTrue Answer: {true_answer}\n{evaluate_system_prompt}\n\"\"\"\n# Generate evaluation score\nevaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n# Print the evaluation score\nprint(\"Evaluation Score:\", evaluation_response.choices[0].message.content)\n## Evaluating the AI Response\nWe compare the AI response with the expected answer and assign a score.\n## Setting Up the Environment\nCode:\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\nimport fitz\nfrom tqdm import tqdm",
  "## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.\n## Setting Up the Environment\nWe begin by importing necessary libraries.\n## Performing Semantic Search\nCode:\ndef cosine_similarity(vec1, vec2):\n\"\"\"\nComputes cosine similarity between two vectors.\nArgs:\nvec1 (np.ndarray): First vector.\nvec2 (np.ndarray): Second vector.\nReturns:\nfloat: Cosine similarity score.\n\"\"\"\nreturn np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
]