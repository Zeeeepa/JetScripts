[
  "## Chunking the Extracted Text\nTo improve retrieval, we split the extracted text into overlapping chunks of different sizes.\n## Generating a Response Based on Retrieved Chunks\nLet's  generate a response based on the retrieved text for chunk size `256`.\n## Chunking the Extracted Text\nCode:\ndef chunk_text(text, n, overlap):\n\"\"\"\nSplits text into overlapping chunks.\nArgs:\ntext (str): The text to be chunked.\nn (int): Number of characters per chunk.\noverlap (int): Overlapping characters between chunks.\nReturns:\nList[str]: A list of text chunks.\n\"\"\"\nchunks = []  # Initialize an empty list to store the chunks\nfor i in range(0, len(text), n - overlap):\n# Append a chunk of text from the current index to the index + chunk size\nchunks.append(text[i:i + n])\nreturn chunks  # Return the list of text chunks\n# Define different chunk sizes to evaluate\nchunk_sizes = [128, 256, 512]\n# Create a dictionary to store text chunks for each chunk size\ntext_chunks_dict = {size: chunk_text(extracted_text, size, size // 5) for size in chunk_sizes}\n# Print the number of chunks created for each chunk size\nfor size, chunks in text_chunks_dict.items():\nprint(f\"Chunk Size: {size}, Number of Chunks: {len(chunks)}\")",
  "## Evaluating Chunk Sizes in Simple RAG\nChoosing the right chunk size is crucial for improving retrieval accuracy in a Retrieval-Augmented Generation (RAG) pipeline. The goal is to balance retrieval performance with response quality.\nThis section evaluates different chunk sizes by:\n1. Extracting text from a PDF.\n2. Splitting text into chunks of varying sizes.\n3. Creating embeddings for each chunk.\n4. Retrieving relevant chunks for a query.\n5. Generating a response using retrieved chunks.\n6. Evaluating faithfulness and relevancy.\n7. Comparing results for different chunk sizes.\n## Creating Embeddings for Text Chunks\nEmbeddings convert text into numerical representations for similarity search.\n## Creating Embeddings for Text Chunks\nCode:\nfrom tqdm import tqdm\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nGenerates embeddings for a list of texts.\nArgs:\ntexts (List[str]): List of input texts.\nmodel (str): Embedding model.\nReturns:\nList[np.ndarray]: List of numerical embeddings.\n\"\"\"\n# Create embeddings using the specified model\nresponse = client.embeddings.create(model=model, input=texts)\n# Convert the response to a list of numpy arrays and return\nreturn [np.array(embedding.embedding) for embedding in response.data]\n# Generate embeddings for each chunk size",
  "# Iterate over each chunk size and its corresponding chunks in the text_chunks_dict\nchunk_embeddings_dict = {size: create_embeddings(chunks) for size, chunks in tqdm(text_chunks_dict.items(), desc=\"Generating Embeddings\")}\n## Generating a Response Based on Retrieved Chunks\nCode:\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\ndef generate_response(query, system_prompt, retrieved_chunks, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates an AI response based on retrieved chunks.\nArgs:\nquery (str): User query.\nretrieved_chunks (List[str]): List of retrieved text chunks.\nmodel (str): AI model.\nReturns:\nstr: AI-generated response.\n\"\"\"\n# Combine retrieved chunks into a single context string\ncontext = \"n\".join([f\"Context {i+1}:n{chunk}\" for i, chunk in enumerate(retrieved_chunks)])\n# Create the user prompt by combining the context and the query\nuser_prompt = f\"{context}nnQuestion: {query}\"",
  "# Generate the AI response using the specified model\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n]\n)\n# Return the content of the AI response\nreturn response.choices[0].message.content\n# Generate AI responses for each chunk size\nai_responses_dict = {size: generate_response(query, system_prompt, retrieved_chunks_dict[size]) for size in chunk_sizes}\n# Print the response for chunk size 256\nprint(ai_responses_dict[256])\n## Performing Semantic Search\nWe use cosine similarity to find the most relevant text chunks for a user query.\n## Extracting Text from the PDF\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page in mypdf:\n# Extract text from the current page and add spacing\nall_text += page.get_text(\"text\") + \" \"\n# Return the extracted text, stripped of leading/trailing whitespace\nreturn all_text.strip()",
  "# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n# Print the first 500 characters of the extracted text\nprint(extracted_text[:500])\n## Extracting Text from the PDF\nFirst, we will extract text from the `AI_Information.pdf` file.\n## Evaluating the AI Response\nCode:\n# Define evaluation scoring system constants\nSCORE_FULL = 1.0     # Complete match or fully satisfactory\nSCORE_PARTIAL = 0.5  # Partial match or somewhat satisfactory\nSCORE_NONE = 0.0     # No match or unsatisfactory\n## Setting Up the Environment\nCode:\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\n## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.\n## Setting Up the Environment\nWe begin by importing necessary libraries."
]