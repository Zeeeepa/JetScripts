[
  {
    "id": "02357c830750aee4c67d524ff11a036d",
    "rank": 1,
    "doc_index": 7,
    "score": 0.6084121465682983,
    "percent_difference": 0.0,
    "text": "## Chunking the Extracted Text\nTo improve retrieval, we split the extracted text into overlapping chunks of different sizes.",
    "relevance": null,
    "word_count": 22
  },
  {
    "id": "03dcbe09477ea327128194acfb671625",
    "rank": 2,
    "doc_index": 13,
    "score": 0.5911455750465393,
    "percent_difference": 2.84,
    "text": "## Generating a Response Based on Retrieved Chunks\nLet's  generate a response based on the retrieved text for chunk size `256`.",
    "relevance": null,
    "word_count": 26
  },
  {
    "id": "78ebe7d0f2d5eae6e7d246323490455a",
    "rank": 3,
    "doc_index": 8,
    "score": 0.5226126611232758,
    "percent_difference": 14.1,
    "text": "## Chunking the Extracted Text\nCode:\ndef chunk_text(text, n, overlap):\n\"\"\"\nSplits text into overlapping chunks.\nArgs:\ntext (str): The text to be chunked.\nn (int): Number of characters per chunk.\noverlap (int): Overlapping characters between chunks.\nReturns:\nList[str]: A list of text chunks.\n\"\"\"\nchunks = []  # Initialize an empty list to store the chunks\nfor i in range(0, len(text), n - overlap):\n# Append a chunk of text from the current index to the index + chunk size\nchunks.append(text[i:i + n])\nreturn chunks  # Return the list of text chunks\n# Define different chunk sizes to evaluate\nchunk_sizes = [128, 256, 512]\n# Create a dictionary to store text chunks for each chunk size\ntext_chunks_dict = {size: chunk_text(extracted_text, size, size // 5) for size in chunk_sizes}\n# Print the number of chunks created for each chunk size\nfor size, chunks in text_chunks_dict.items():\nprint(f\"Chunk Size: {size}, Number of Chunks: {len(chunks)}\")",
    "relevance": null,
    "word_count": 233
  },
  {
    "id": "df4a8d1eb2bb20628be1ba172a9142f5",
    "rank": 4,
    "doc_index": 0,
    "score": 0.5218717455863953,
    "percent_difference": 14.22,
    "text": "## Evaluating Chunk Sizes in Simple RAG\nChoosing the right chunk size is crucial for improving retrieval accuracy in a Retrieval-Augmented Generation (RAG) pipeline. The goal is to balance retrieval performance with response quality.\nThis section evaluates different chunk sizes by:\n1. Extracting text from a PDF.\n2. Splitting text into chunks of varying sizes.\n3. Creating embeddings for each chunk.\n4. Retrieving relevant chunks for a query.\n5. Generating a response using retrieved chunks.\n6. Evaluating faithfulness and relevancy.\n7. Comparing results for different chunk sizes.",
    "relevance": null,
    "word_count": 107
  },
  {
    "id": "69b05b480314059ad87cb3bab94c0e36",
    "rank": 5,
    "doc_index": 9,
    "score": 0.4469223916530609,
    "percent_difference": 26.54,
    "text": "## Creating Embeddings for Text Chunks\nEmbeddings convert text into numerical representations for similarity search.",
    "relevance": null,
    "word_count": 17
  },
  {
    "id": "cd40e33023de696360a8cabced85f91d",
    "rank": 6,
    "doc_index": 10,
    "score": 0.43811316788196564,
    "percent_difference": 27.99,
    "text": "## Creating Embeddings for Text Chunks\nCode:\nfrom tqdm import tqdm\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nGenerates embeddings for a list of texts.\nArgs:\ntexts (List[str]): List of input texts.\nmodel (str): Embedding model.\nReturns:\nList[np.ndarray]: List of numerical embeddings.\n\"\"\"\n# Create embeddings using the specified model\nresponse = client.embeddings.create(model=model, input=texts)\n# Convert the response to a list of numpy arrays and return\nreturn [np.array(embedding.embedding) for embedding in response.data]\n# Generate embeddings for each chunk size\n# Iterate over each chunk size and its corresponding chunks in the text_chunks_dict\nchunk_embeddings_dict = {size: create_embeddings(chunks) for size, chunks in tqdm(text_chunks_dict.items(), desc=\"Generating Embeddings\")}",
    "relevance": null,
    "word_count": 157
  },
  {
    "id": "1e2777e3cb3e4b905d135178243945e7",
    "rank": 7,
    "doc_index": 14,
    "score": 0.3548450966676076,
    "percent_difference": 41.68,
    "text": "## Generating a Response Based on Retrieved Chunks\nCode:\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\ndef generate_response(query, system_prompt, retrieved_chunks, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates an AI response based on retrieved chunks.\nArgs:\nquery (str): User query.\nretrieved_chunks (List[str]): List of retrieved text chunks.\nmodel (str): AI model.\nReturns:\nstr: AI-generated response.\n\"\"\"\n# Combine retrieved chunks into a single context string\ncontext = \"n\".join([f\"Context {i+1}:n{chunk}\" for i, chunk in enumerate(retrieved_chunks)])\n# Create the user prompt by combining the context and the query\nuser_prompt = f\"{context}nnQuestion: {query}\"\n# Generate the AI response using the specified model\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n]\n)\n# Return the content of the AI response\nreturn response.choices[0].message.content\n# Generate AI responses for each chunk size\nai_responses_dict = {size: generate_response(query, system_prompt, retrieved_chunks_dict[size]) for size in chunk_sizes}\n# Print the response for chunk size 256\nprint(ai_responses_dict[256])",
    "relevance": null,
    "word_count": 309
  },
  {
    "id": "41008b6e610fa469673fa14befbc5663",
    "rank": 8,
    "doc_index": 11,
    "score": 0.2993922829627991,
    "percent_difference": 50.79,
    "text": "## Performing Semantic Search\nWe use cosine similarity to find the most relevant text chunks for a user query.",
    "relevance": null,
    "word_count": 21
  },
  {
    "id": "77d1342ae78ecd0f37a443c78db28909",
    "rank": 9,
    "doc_index": 6,
    "score": 0.21672247350215912,
    "percent_difference": 64.38,
    "text": "## Extracting Text from the PDF\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page in mypdf:\n# Extract text from the current page and add spacing\nall_text += page.get_text(\"text\") + \" \"\n# Return the extracted text, stripped of leading/trailing whitespace\nreturn all_text.strip()\n# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n# Print the first 500 characters of the extracted text\nprint(extracted_text[:500])",
    "relevance": null,
    "word_count": 166
  },
  {
    "id": "3f63f37c17a7d2d0eefbd9b998610a02",
    "rank": 10,
    "doc_index": 5,
    "score": 0.19937890768051147,
    "percent_difference": 67.23,
    "text": "## Extracting Text from the PDF\nFirst, we will extract text from the `AI_Information.pdf` file.",
    "relevance": null,
    "word_count": 20
  },
  {
    "id": "b3d728d802ae69423dc23ff0a9b33228",
    "rank": 11,
    "doc_index": 16,
    "score": 0.05273405835032463,
    "percent_difference": 91.33,
    "text": "## Evaluating the AI Response\nCode:\n# Define evaluation scoring system constants\nSCORE_FULL = 1.0     # Complete match or fully satisfactory\nSCORE_PARTIAL = 0.5  # Partial match or somewhat satisfactory\nSCORE_NONE = 0.0     # No match or unsatisfactory",
    "relevance": null,
    "word_count": 40
  },
  {
    "id": "94052a55d4f2f654a85c9cc659c6f18e",
    "rank": 12,
    "doc_index": 2,
    "score": 0.048928402364254,
    "percent_difference": 91.96,
    "text": "## Setting Up the Environment\nCode:\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI",
    "relevance": null,
    "word_count": 22
  },
  {
    "id": "ec06847c02eaf5195ff95315d2a8295a",
    "rank": 13,
    "doc_index": 3,
    "score": 0.03732878342270851,
    "percent_difference": 93.86,
    "text": "## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.",
    "relevance": null,
    "word_count": 19
  },
  {
    "id": "53b6e66239969bc2f4e5a8dd71ab2cf1",
    "rank": 14,
    "doc_index": 1,
    "score": 0.027201073244214058,
    "percent_difference": 95.53,
    "text": "## Setting Up the Environment\nWe begin by importing necessary libraries.",
    "relevance": null,
    "word_count": 13
  }
]