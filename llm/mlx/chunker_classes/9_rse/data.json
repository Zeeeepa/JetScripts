[
  "## Chunking the Extracted Text\nOnce we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy.\n## RSE Core Algorithm: Computing Chunk Values and Finding Best Segments\nNow that we have the necessary functions to process a document and generate embeddings for its chunks, we can implement the core algorithm for RSE.\n## Reconstructing and Using Segments for RAG\nCode:\ndef reconstruct_segments(chunks, best_segments):\n\"\"\"\nReconstruct text segments based on chunk indices.\nArgs:\nchunks (List[str]): List of all document chunks\nbest_segments (List[Tuple[int, int]]): List of (start, end) indices for segments\nReturns:\nList[str]: List of reconstructed text segments\n\"\"\"\nreconstructed_segments = []  # Initialize an empty list to store the reconstructed segments\nfor start, end in best_segments:\n# Join the chunks in this segment to form the complete segment text\nsegment_text = \" \".join(chunks[start:end])\n# Append the segment text and its range to the reconstructed_segments list\nreconstructed_segments.append({\n\"text\": segment_text,\n\"segment_range\": (start, end),\n})\nreturn reconstructed_segments  # Return the list of reconstructed text segments",
  "## Chunking the Extracted Text\nCode:\ndef chunk_text(text, chunk_size=800, overlap=0):\n\"\"\"\nSplit text into non-overlapping chunks.\nFor RSE, we typically want non-overlapping chunks so we can reconstruct segments properly.\nArgs:\ntext (str): Input text to chunk\nchunk_size (int): Size of each chunk in characters\noverlap (int): Overlap between chunks in characters\nReturns:\nList[str]: List of text chunks\n\"\"\"\nchunks = []\n# Simple character-based chunking\nfor i in range(0, len(text), chunk_size - overlap):\nchunk = text[i:i + chunk_size]\nif chunk:  # Ensure we don't add empty chunks\nchunks.append(chunk)\nreturn chunks\n## Creating Embeddings for Text Chunks\nEmbeddings transform text into numerical vectors, which allow for efficient similarity search.\n## Complete RSE Pipeline Function\nCode:\ndef rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n\"\"\"\nComplete RAG pipeline with Relevant Segment Extraction.\nArgs:\npdf_path (str): Path to the document\nquery (str): User query\nchunk_size (int): Size of chunks\nirrelevant_chunk_penalty (float): Penalty for irrelevant chunks\nReturns:\nDict: Result with query, segments, and response\n\"\"\"\nprint(\"n=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\")\nprint(f\"Query: {query}\")",
  "# Process the document to extract text, chunk it, and create embeddings\nchunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n# Calculate relevance scores and chunk values based on the query\nprint(\"nCalculating relevance scores and chunk values...\")\nchunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n# Find the best segments of text based on chunk values\nbest_segments, scores = find_best_segments(\nchunk_values,\nmax_segment_length=20,\ntotal_max_length=30,\nmin_segment_value=0.2\n)\n# Reconstruct text segments from the best chunks\nprint(\"nReconstructing text segments from chunks...\")\nsegments = reconstruct_segments(chunks, best_segments)\n# Format the segments into a context string for the language model\ncontext = format_segments_for_context(segments)\n# Generate a response from the language model using the context\nresponse = generate_response(query, context)\n# Compile the result into a dictionary\nresult = {\n\"query\": query,\n\"segments\": segments,\n\"response\": response\n}\nprint(\"n=== FINAL RESPONSE ===\")\nprint(response)\nreturn result",
  "## Processing Documents with RSE\nCode:\ndef process_document(pdf_path, chunk_size=800):\n\"\"\"\nProcess a document for use with RSE.\nArgs:\npdf_path (str): Path to the PDF document\nchunk_size (int): Size of each chunk in characters\nReturns:\nTuple[List[str], SimpleVectorStore, Dict]: Chunks, vector store, and document info\n\"\"\"\nprint(\"Extracting text from document...\")\n# Extract text from the PDF file\ntext = extract_text_from_pdf(pdf_path)\nprint(\"Chunking text into non-overlapping segments...\")\n# Chunk the extracted text into non-overlapping segments\nchunks = chunk_text(text, chunk_size=chunk_size, overlap=0)\nprint(f\"Created {len(chunks)} chunks\")\nprint(\"Generating embeddings for chunks...\")\n# Generate embeddings for the text chunks\nchunk_embeddings = create_embeddings(chunks)\n# Create an instance of the SimpleVectorStore\nvector_store = SimpleVectorStore()\n# Add documents with metadata (including chunk index for later reconstruction)\nmetadata = [{\"chunk_index\": i, \"source\": pdf_path} for i in range(len(chunks))]\nvector_store.add_documents(chunks, chunk_embeddings, metadata)\n# Track original document structure for segment reconstruction\ndoc_info = {\n\"chunks\": chunks,\n\"source\": pdf_path,\n}\nreturn chunks, vector_store, doc_info",
  "## RSE Core Algorithm: Computing Chunk Values and Finding Best Segments\nCode:\ndef calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty=0.2):\n\"\"\"\nCalculate chunk values by combining relevance and position.\nArgs:\nquery (str): Query text\nchunks (List[str]): List of document chunks\nvector_store (SimpleVectorStore): Vector store containing the chunks\nirrelevant_chunk_penalty (float): Penalty for irrelevant chunks\nReturns:\nList[float]: List of chunk values\n\"\"\"\n# Create query embedding\nquery_embedding = create_embeddings([query])[0]\n# Get all chunks with similarity scores\nnum_chunks = len(chunks)\nresults = vector_store.search(query_embedding, top_k=num_chunks)\n# Create a mapping of chunk_index to relevance score\nrelevance_scores = {result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in results}\n# Calculate chunk values (relevance score minus penalty)\nchunk_values = []\nfor i in range(num_chunks):\n# Get relevance score or default to 0 if not in results\nscore = relevance_scores.get(i, 0.0)\n# Apply penalty to convert to a value where irrelevant chunks have negative value\nvalue = score - irrelevant_chunk_penalty\nchunk_values.append(value)\nreturn chunk_values",
  "## Comparing with Standard Retrieval\nCode:\ndef standard_top_k_retrieval(pdf_path, query, k=10, chunk_size=800):\n\"\"\"\nStandard RAG with top-k retrieval.\nArgs:\npdf_path (str): Path to the document\nquery (str): User query\nk (int): Number of chunks to retrieve\nchunk_size (int): Size of chunks\nReturns:\nDict: Result with query, chunks, and response\n\"\"\"\nprint(\"n=== STARTING STANDARD TOP-K RETRIEVAL ===\")\nprint(f\"Query: {query}\")\n# Process the document to extract text, chunk it, and create embeddings\nchunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n# Create an embedding for the query\nprint(\"Creating query embedding and retrieving chunks...\")\nquery_embedding = create_embeddings([query])[0]\n# Retrieve the top-k most relevant chunks based on the query embedding\nresults = vector_store.search(query_embedding, top_k=k)\nretrieved_chunks = [result[\"document\"] for result in results]\n# Format the retrieved chunks into a context string\ncontext = \"nn\".join([\nf\"CHUNK {i+1}:n{chunk}\"\nfor i, chunk in enumerate(retrieved_chunks)\n])\n# Generate a response from the language model using the context\nresponse = generate_response(query, context)",
  "# Compile the result into a dictionary\nresult = {\n\"query\": query,\n\"chunks\": retrieved_chunks,\n\"response\": response\n}\nprint(\"n=== FINAL RESPONSE ===\")\nprint(response)\nreturn result\n# Relevant Segment Extraction (RSE) for Enhanced RAG\nIn this notebook, we implement a Relevant Segment Extraction (RSE) technique to improve the context quality in our RAG system. Rather than simply retrieving a collection of isolated chunks, we identify and reconstruct continuous segments of text that provide better context to our language model.\n## Key Concept\nRelevant chunks tend to be clustered together within documents. By identifying these clusters and preserving their continuity, we provide more coherent context for the LLM to work with.\n## Creating Embeddings for Text Chunks\nCode:\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nGenerate embeddings for texts.\nArgs:\ntexts (List[str]): List of texts to embed\nmodel (str): Embedding model to use\nReturns:\nList[List[float]]: List of embedding vectors\n\"\"\"\nif not texts:\nreturn []  # Return an empty list if no texts are provided",
  "# Process in batches if the list is long\nbatch_size = 100  # Adjust based on your API limits\nall_embeddings = []  # Initialize a list to store all embeddings\nfor i in range(0, len(texts), batch_size):\nbatch = texts[i:i + batch_size]  # Get the current batch of texts\n# Create embeddings for the current batch using the specified model\nresponse = client.embeddings.create(\ninput=batch,\nmodel=model\n)\n# Extract embeddings from the response\nbatch_embeddings = [item.embedding for item in response.data]\nall_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\nreturn all_embeddings  # Return the list of all embeddings\n## Extracting Text from a PDF File\nTo implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library.\n## Building a Simple Vector Store\nlet's implement a simple vector store.\n## Extracting Text from a PDF File\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file and prints the first `num_chars` characters.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text",
  "# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text\n## Processing Documents with RSE\nNow let's implement the core RSE functionality.",
  "## Building a Simple Vector Store\nCode:\nclass SimpleVectorStore:\n\"\"\"\nA lightweight vector store implementation using NumPy.\n\"\"\"\ndef __init__(self, dimension=1536):\n\"\"\"\nInitialize the vector store.\nArgs:\ndimension (int): Dimension of embeddings\n\"\"\"\nself.dimension = dimension\nself.vectors = []\nself.documents = []\nself.metadata = []\ndef add_documents(self, documents, vectors=None, metadata=None):\n\"\"\"\nAdd documents to the vector store.\nArgs:\ndocuments (List[str]): List of document chunks\nvectors (List[List[float]], optional): List of embedding vectors\nmetadata (List[Dict], optional): List of metadata dictionaries\n\"\"\"\nif vectors is None:\nvectors = [None] * len(documents)\nif metadata is None:\nmetadata = [{} for _ in range(len(documents))]\nfor doc, vec, meta in zip(documents, vectors, metadata):\nself.documents.append(doc)\nself.vectors.append(vec)\nself.metadata.append(meta)\ndef search(self, query_vector, top_k=5):\n\"\"\"\nSearch for most similar documents.\nArgs:\nquery_vector (List[float]): Query embedding vector\ntop_k (int): Number of results to return\nReturns:\nList[Dict]: List of results with documents, scores, and metadata\n\"\"\"\nif not self.vectors or not self.documents:\nreturn []\n# Convert query vector to numpy array\nquery_array = np.array(query_vector)",
  "# Calculate similarities\nsimilarities = []\nfor i, vector in enumerate(self.vectors):\nif vector is not None:\n# Compute cosine similarity\nsimilarity = np.dot(query_array, vector) / (\nnp.linalg.norm(query_array) * np.linalg.norm(vector)\n)\nsimilarities.append((i, similarity))\n# Sort by similarity (descending)\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n# Get top-k results\nresults = []\nfor i, score in similarities[:top_k]:\nresults.append({\n\"document\": self.documents[i],\n\"score\": float(score),\n\"metadata\": self.metadata[i]\n})\nreturn results\n## Comparing with Standard Retrieval\nLet's implement a standard retrieval approach to compare with RSE:\n## Evaluation of RSE\nCode:\ndef evaluate_methods(pdf_path, query, reference_answer=None):\n\"\"\"\nCompare RSE with standard top-k retrieval.\nArgs:\npdf_path (str): Path to the document\nquery (str): User query\nreference_answer (str, optional): Reference answer for evaluation\n\"\"\"\nprint(\"n========= EVALUATION =========n\")\n# Run the RAG with Relevant Segment Extraction (RSE) method\nrse_result = rag_with_rse(pdf_path, query)\n# Run the standard top-k retrieval method\nstandard_result = standard_top_k_retrieval(pdf_path, query)\n# If a reference answer is provided, evaluate the responses\nif reference_answer:\nprint(\"n=== COMPARING RESULTS ===\")",
  "# Create an evaluation prompt to compare the responses against the reference answer\nevaluation_prompt = f\"\"\"\nQuery: {query}\nReference Answer:\n{reference_answer}\nResponse from Standard Retrieval:\n{standard_result[\"response\"]}\nResponse from Relevant Segment Extraction:\n{rse_result[\"response\"]}\nCompare these two responses against the reference answer. Which one is:\n1. More accurate and comprehensive\n2. Better at addressing the user's query\n3. Less likely to include irrelevant information\nExplain your reasoning for each point.\n\"\"\"\nprint(\"Evaluating responses against reference answer...\")\n# Generate the evaluation using the specified model\nevaluation = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": \"You are an objective evaluator of RAG system responses.\"},\n{\"role\": \"user\", \"content\": evaluation_prompt}\n]\n)\n# Print the evaluation results\nprint(\"n=== EVALUATION RESULTS ===\")\nprint(evaluation.choices[0].message.content)\n# Return the results of both methods\nreturn {\n\"rse_result\": rse_result,\n\"standard_result\": standard_result\n}",
  "## Generating Responses with RSE Context\nCode:\ndef generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerate a response based on the query and context.\nArgs:\nquery (str): User query\ncontext (str): Context text from relevant segments\nmodel (str): LLM model to use\nReturns:\nstr: Generated response\n\"\"\"\nprint(\"Generating response using relevant segments as context...\")\n# Define the system prompt to guide the AI's behavior\nsystem_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\nThe context consists of document segments that have been retrieved as relevant to the user's query.\nUse the information from these segments to provide a comprehensive and accurate answer.\nIf the context doesn't contain relevant information to answer the question, say so clearly.\"\"\"\n# Create the user prompt by combining the context and the query\nuser_prompt = f\"\"\"\nContext:\n{context}\nQuestion: {query}\nPlease provide a helpful answer based on the context provided.\n\"\"\"\n# Generate the response using the specified model\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0\n)\n# Return the generated response content\nreturn response.choices[0].message.content",
  "## Setting Up the Environment\nCode:\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\nimport re\n## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.\n## Setting Up the Environment\nWe begin by importing necessary libraries."
]