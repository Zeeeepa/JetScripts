[
  {
    "id": "1172f72372966bbed20cd23b353c6099",
    "rank": 1,
    "doc_index": 11,
    "score": 0.33349241813023883,
    "percent_difference": 0.0,
    "text": "## Complete Graph RAG Pipeline\nCode:\ndef graph_rag_pipeline(pdf_path, query, chunk_size=1000, chunk_overlap=200, top_k=3):\n\"\"\"\nComplete Graph RAG pipeline from document to answer.\nArgs:\npdf_path (str): Path to the PDF document\nquery (str): The user's question\nchunk_size (int): Size of text chunks\nchunk_overlap (int): Overlap between chunks\ntop_k (int): Number of top nodes to consider for traversal\nReturns:\nDict: Results including answer and graph visualization data\n\"\"\"\n# Extract text from the PDF document\ntext = extract_text_from_pdf(pdf_path)\n# Split the extracted text into overlapping chunks\nchunks = chunk_text(text, chunk_size, chunk_overlap)\n# Build a knowledge graph from the text chunks\ngraph, embeddings = build_knowledge_graph(chunks)\n# Traverse the knowledge graph to find relevant information for the query\nrelevant_chunks, traversal_path = traverse_graph(query, graph, embeddings, top_k)\n# Generate a response based on the query and the relevant chunks\nresponse = generate_response(query, relevant_chunks)\n# Visualize the graph traversal path\nvisualize_graph_traversal(graph, traversal_path)\n# Return the query, response, relevant chunks, traversal path, and the graph\nreturn {\n\"query\": query,\n\"response\": response,\n\"relevant_chunks\": relevant_chunks,\n\"traversal_path\": traversal_path,\n\"graph\": graph\n}",
    "relevance": null,
    "word_count": 253
  },
  {
    "id": "2d3c5be01221d594b8d106660fd99bdb",
    "rank": 2,
    "doc_index": 6,
    "score": 0.2339474782347679,
    "percent_difference": 29.85,
    "text": "## Creating Embeddings\nCode:\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreate embeddings for the given texts.\nArgs:\ntexts (List[str]): Input texts\nmodel (str): Embedding model name\nReturns:\nList[List[float]]: Embedding vectors\n\"\"\"\n# Handle empty input\nif not texts:\nreturn []\n# Process in batches if needed (OpenAI API limits)\nbatch_size = 100\nall_embeddings = []\n# Iterate over the input texts in batches\nfor i in range(0, len(texts), batch_size):\nbatch = texts[i:i + batch_size]  # Get the current batch of texts\n# Create embeddings for the current batch\nresponse = client.embeddings.create(\nmodel=model,\ninput=batch\n)\n# Extract embeddings from the response\nbatch_embeddings = [item.embedding for item in response.data]\nall_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\nreturn all_embeddings  # Return all embeddings",
    "relevance": null,
    "word_count": 179
  },
  {
    "id": "e86f7da7f6a87f185247625c67a5f83c",
    "rank": 3,
    "doc_index": 9,
    "score": 0.22047474732001623,
    "percent_difference": 33.89,
    "text": "## Response Generation\nCode:\ndef generate_response(query, context_chunks):\n\"\"\"\nGenerate a response using the retrieved context.\nArgs:\nquery (str): The user's question\ncontext_chunks (List[Dict]): Relevant chunks from graph traversal\nReturns:\nstr: Generated response\n\"\"\"\n# Extract text from each chunk in the context\ncontext_texts = [chunk[\"text\"] for chunk in context_chunks]\n# Combine the extracted texts into a single context string, separated by \"---\"\ncombined_context = \"nn---nn\".join(context_texts)\n# Define the maximum allowed length for the context (OpenAI limit)\nmax_context = 14000\n# Truncate the combined context if it exceeds the maximum length\nif len(combined_context) > max_context:\ncombined_context = combined_context[:max_context] + \"... [truncated]\"\n# Define the system message to guide the AI assistant\nsystem_message = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context.\nIf the information is not in the context, say so. Refer to specific parts of the context in your answer when possible.\"\"\"\n# Generate the response using the OpenAI API\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\nmessages=[\n{\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n{\"role\": \"user\", \"content\": f\"Context:n{combined_context}nnQuestion: {query}\"}  # User message with context and query\n],\ntemperature=0.2  # Set the temperature for response generation\n)\n# Return the generated response content\nreturn response.choices[0].message.content",
    "relevance": null,
    "word_count": 325
  },
  {
    "id": "951a157fda9e28c55a6c0f85c52fd423",
    "rank": 4,
    "doc_index": 0,
    "score": 0.2065810114145279,
    "percent_difference": 38.06,
    "text": "# Graph RAG: Graph-Enhanced Retrieval-Augmented Generation\nIn this notebook, I implement Graph RAG - a technique that enhances traditional RAG systems by organizing knowledge as a connected graph rather than a flat collection of documents. This allows the system to navigate related concepts and retrieve more contextually relevant information than standard vector similarity approaches.\nKey Benefits of Graph RAG\n- Preserves relationships between pieces of information\n- Enables traversal through connected concepts to find relevant context\n- Improves handling of complex, multi-part queries\n- Provides better explainability through visualized knowledge paths",
    "relevance": null,
    "word_count": 96
  },
  {
    "id": "420c7dc1aaa2b52f19754074e4d93a21",
    "rank": 5,
    "doc_index": 5,
    "score": 0.17251205444335938,
    "percent_difference": 48.27,
    "text": "## Document Processing Functions\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtract text content from a PDF file.\nArgs:\npdf_path (str): Path to the PDF file\nReturns:\nstr: Extracted text content\n\"\"\"\nprint(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\npdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\ntext = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(pdf_document.page_count):\npage = pdf_document[page_num]  # Get the page object\ntext += page.get_text()  # Extract text from the page and append to the text string\nreturn text  # Return the extracted text content",
    "relevance": null,
    "word_count": 147
  },
  {
    "id": "afc696bb0c316741033260f0295e8109",
    "rank": 6,
    "doc_index": 12,
    "score": 0.16047330759465694,
    "percent_difference": 51.88,
    "text": "## Evaluation Function\nCode:\ndef evaluate_graph_rag(pdf_path, test_queries, reference_answers=None):\n\"\"\"\nEvaluate Graph RAG on multiple test queries.\nArgs:\npdf_path (str): Path to the PDF document\ntest_queries (List[str]): List of test queries\nreference_answers (List[str], optional): Reference answers for comparison\nReturns:\nDict: Evaluation results\n\"\"\"\n# Extract text from PDF\ntext = extract_text_from_pdf(pdf_path)\n# Split text into chunks\nchunks = chunk_text(text)\n# Build knowledge graph (do this once for all queries)\ngraph, embeddings = build_knowledge_graph(chunks)\nresults = []\nfor i, query in enumerate(test_queries):\nprint(f\"nn=== Evaluating Query {i+1}/{len(test_queries)} ===\")\nprint(f\"Query: {query}\")\n# Traverse graph to find relevant information\nrelevant_chunks, traversal_path = traverse_graph(query, graph, embeddings)\n# Generate response\nresponse = generate_response(query, relevant_chunks)\n# Compare with reference answer if available\nreference = None\ncomparison = None\nif reference_answers and i < len(reference_answers):\nreference = reference_answers[i]\ncomparison = compare_with_reference(response, reference, query)\n# Append results for the current query\nresults.append({\n\"query\": query,\n\"response\": response,\n\"reference_answer\": reference,\n\"comparison\": comparison,\n\"traversal_path_length\": len(traversal_path),\n\"relevant_chunks_count\": len(relevant_chunks)\n})\n# Display results\nprint(f\"nResponse: {response}n\")\nif comparison:\nprint(f\"Comparison: {comparison}n\")\n# Return evaluation results and graph statistics\nreturn {\n\"results\": results,\n\"graph_stats\": {\n\"nodes\": graph.number_of_nodes(),\n\"edges\": graph.number_of_edges(),\n\"avg_degree\": sum(dict(graph.degree()).values()) / graph.number_of_nodes()\n}\n}",
    "relevance": null,
    "word_count": 371
  },
  {
    "id": "e068f456e6e60ac7f91ac167fcf31efe",
    "rank": 7,
    "doc_index": 13,
    "score": 0.1391415943702062,
    "percent_difference": 58.28,
    "text": "## Evaluation of Graph RAG on a Sample PDF Document\nCode:\n# Path to the PDF document containing AI information\npdf_path = \"data/AI_Information.pdf\"\n# Define an AI-related query for testing Graph RAG\nquery = \"What are the key applications of transformers in natural language processing?\"\n# Execute the Graph RAG pipeline to process the document and answer the query\nresults = graph_rag_pipeline(pdf_path, query)\n# Print the response generated from the Graph RAG system\nprint(\"n=== ANSWER ===\")\nprint(results[\"response\"])\n# Define a test query and reference answer for formal evaluation\ntest_queries = [\n\"How do transformers handle sequential data compared to RNNs?\"\n]\n# Reference answer for evaluation purposes\nreference_answers = [\n\"Transformers handle sequential data differently from RNNs by using self-attention mechanisms instead of recurrent connections. This allows transformers to process all tokens in parallel rather than sequentially, capturing long-range dependencies more efficiently and enabling better parallelization during training. Unlike RNNs, transformers don't suffer from vanishing gradient problems with long sequences.\"\n]\n# Run formal evaluation of the Graph RAG system with the test query\nevaluation = evaluate_graph_rag(pdf_path, test_queries, reference_answers)\n# Print evaluation summary statistics\nprint(\"n=== EVALUATION SUMMARY ===\")\nprint(f\"Graph nodes: {evaluation['graph_stats']['nodes']}\")\nprint(f\"Graph edges: {evaluation['graph_stats']['edges']}\")\nfor i, result in enumerate(evaluation['results']):\nprint(f\"nQuery {i+1}: {result['query']}\")\nprint(f\"Path length: {result['traversal_path_length']}\")\nprint(f\"Chunks used: {result['relevant_chunks_count']}\")",
    "relevance": null,
    "word_count": 337
  },
  {
    "id": "c96aa24630d11429ef9d23a2651bf342",
    "rank": 8,
    "doc_index": 7,
    "score": 0.11387455711762111,
    "percent_difference": 65.85,
    "text": "## Knowledge Graph Construction\nCode:\ndef extract_concepts(text):\n\"\"\"\nExtract key concepts from text using OpenAI's API.\nArgs:\ntext (str): Text to extract concepts from\nReturns:\nList[str]: List of concepts\n\"\"\"\n# System message to instruct the model on what to do\nsystem_message = \"\"\"Extract key concepts and entities from the provided text.\nReturn ONLY a list of 5-10 key terms, entities, or concepts that are most important in this text.\nFormat your response as a JSON array of strings.\"\"\"\n# Make a request to the OpenAI API\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_message},\n{\"role\": \"user\", \"content\": f\"Extract key concepts from:nn{text[:3000]}\"}  # Limit for API\n],\ntemperature=0.0,\nresponse_format={\"type\": \"json_object\"}\n)\ntry:\n# Parse concepts from the response\nconcepts_json = json.loads(response.choices[0].message.content)\nconcepts = concepts_json.get(\"concepts\", [])\nif not concepts and \"concepts\" not in concepts_json:\n# Try to get any array in the response\nfor key, value in concepts_json.items():\nif isinstance(value, list):\nconcepts = value\nbreak\nreturn concepts\nexcept (json.JSONDecodeError, AttributeError):\n# Fallback if JSON parsing fails\ncontent = response.choices[0].message.content\n# Try to extract anything that looks like a list\nmatches = re.findall(r'[(.*?)]', content, re.DOTALL)\nif matches:\nitems = re.findall(r'\"([^\"]*)\"', matches[0])\nreturn items\nreturn []",
    "relevance": null,
    "word_count": 345
  },
  {
    "id": "b172b38362d5e978b837f0f56458b54f",
    "rank": 9,
    "doc_index": 8,
    "score": 0.0984119102358818,
    "percent_difference": 70.49,
    "text": "## Graph Traversal and Query Processing\nCode:\ndef traverse_graph(query, graph, embeddings, top_k=5, max_depth=3):\n\"\"\"\nTraverse the knowledge graph to find relevant information for the query.\nArgs:\nquery (str): The user's question\ngraph (nx.Graph): The knowledge graph\nembeddings (List): List of node embeddings\ntop_k (int): Number of initial nodes to consider\nmax_depth (int): Maximum traversal depth\nReturns:\nList[Dict]: Relevant information from graph traversal\n\"\"\"\nprint(f\"Traversing graph for query: {query}\")\n# Get query embedding\nquery_embedding = create_embeddings(query)\n# Calculate similarity between query and all nodes\nsimilarities = []\nfor i, node_embedding in enumerate(embeddings):\nsimilarity = np.dot(query_embedding, node_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding))\nsimilarities.append((i, similarity))\n# Sort by similarity (descending)\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n# Get top-k most similar nodes as starting points\nstarting_nodes = [node for node, _ in similarities[:top_k]]\nprint(f\"Starting traversal from {len(starting_nodes)} nodes\")\n# Initialize traversal\nvisited = set()  # Set to keep track of visited nodes\ntraversal_path = []  # List to store the traversal path\nresults = []  # List to store the results\n# Use a priority queue for traversal\nqueue = []\nfor node in starting_nodes:\nheapq.heappush(queue, (-similarities[node][1], node))  # Negative for max-heap\n# Traverse the graph using a modified breadth-first search with priority\nwhile queue and len(results) < (top_k * 3):  # Limit results to top_k * 3\n_, node = heapq.heappop(queue)\nif node in visited:\ncontinue\n# Mark as visited\nvisited.add(node)\ntraversal_path.append(node)\n# Add current node's text to results\nresults.append({\n\"text\": graph.nodes[node][\"text\"],\n\"concepts\": graph.nodes[node][\"concepts\"],\n\"node_id\": node\n})\n# Explore neighbors if we haven't reached max depth\nif len(traversal_path) < max_depth:\nneighbors = [(neighbor, graph[node][neighbor][\"weight\"])\nfor neighbor in graph.neighbors(node)\nif neighbor not in visited]\n# Add neighbors to queue based on edge weight\nfor neighbor, weight in sorted(neighbors, key=lambda x: x[1], reverse=True):\nheapq.heappush(queue, (-weight, neighbor))\nprint(f\"Graph traversal found {len(results)} relevant chunks\")\nreturn results, traversal_path",
    "relevance": null,
    "word_count": 520
  },
  {
    "id": "8ba03f601fc437da1b65e96403315aac",
    "rank": 10,
    "doc_index": 2,
    "score": 0.09458457678556442,
    "percent_difference": 71.64,
    "text": "## Setting Up the Environment\nCode:\nimport os\nimport numpy as np\nimport json\nimport fitz  # PyMuPDF\nfrom openai import OpenAI\nfrom typing import List, Dict, Tuple, Any\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport heapq\nfrom collections import defaultdict\nimport re\nfrom PIL import Image\nimport io",
    "relevance": null,
    "word_count": 56
  },
  {
    "id": "ec06847c02eaf5195ff95315d2a8295a",
    "rank": 11,
    "doc_index": 3,
    "score": 0.03732878342270851,
    "percent_difference": 88.81,
    "text": "## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.",
    "relevance": null,
    "word_count": 19
  },
  {
    "id": "27625eb0219097c8d26974892745c566",
    "rank": 12,
    "doc_index": 10,
    "score": 0.029583869501948357,
    "percent_difference": 91.13,
    "text": "## Visualization\nCode:\ndef visualize_graph_traversal(graph, traversal_path):\n\"\"\"\nVisualize the knowledge graph and the traversal path.\nArgs:\ngraph (nx.Graph): The knowledge graph\ntraversal_path (List): List of nodes in traversal order\n\"\"\"\nplt.figure(figsize=(12, 10))  # Set the figure size\n# Define node colors, default to light blue\nnode_color = ['lightblue'] * graph.number_of_nodes()\n# Highlight traversal path nodes in light green\nfor node in traversal_path:\nnode_color[node] = 'lightgreen'\n# Highlight start node in green and end node in red\nif traversal_path:\nnode_color[traversal_path[0]] = 'green'\nnode_color[traversal_path[-1]] = 'red'\n# Create positions for all nodes using spring layout\npos = nx.spring_layout(graph, k=0.5, iterations=50, seed=42)\n# Draw the graph nodes\nnx.draw_networkx_nodes(graph, pos, node_color=node_color, node_size=500, alpha=0.8)\n# Draw edges with width proportional to weight\nfor u, v, data in graph.edges(data=True):\nweight = data.get('weight', 1.0)\nnx.draw_networkx_edges(graph, pos, edgelist=[(u, v)], width=weight*2, alpha=0.6)\n# Draw traversal path with red dashed lines\ntraversal_edges = [(traversal_path[i], traversal_path[i+1])\nfor i in range(len(traversal_path)-1)]\nnx.draw_networkx_edges(graph, pos, edgelist=traversal_edges,\nwidth=3, alpha=0.8, edge_color='red',\nstyle='dashed', arrows=True)\n# Add labels with the first concept for each node\nlabels = {}\nfor node in graph.nodes():\nconcepts = graph.nodes[node]['concepts']\nlabel = concepts[0] if concepts else f\"Node {node}\"\nlabels[node] = f\"{node}: {label}\"\nnx.draw_networkx_labels(graph, pos, labels=labels, font_size=8)\nplt.title(\"Knowledge Graph with Traversal Path\")  # Set the plot title\nplt.axis('off')  # Turn off the axis\nplt.tight_layout()  # Adjust layout\nplt.show()  # Display the plot",
    "relevance": null,
    "word_count": 388
  },
  {
    "id": "53b6e66239969bc2f4e5a8dd71ab2cf1",
    "rank": 13,
    "doc_index": 1,
    "score": 0.02720121666789055,
    "percent_difference": 91.84,
    "text": "## Setting Up the Environment\nWe begin by importing necessary libraries.",
    "relevance": null,
    "word_count": 13
  }
]