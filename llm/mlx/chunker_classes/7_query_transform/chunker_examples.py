from chunker
import fitz
import numpy as np
import client

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    
    Args:
    pdf_path (str): Path to the PDF file.
    
    Returns:
    str: Extracted text from the PDF.
    """
    # Open the PDF file
    mypdf = fitz.open(pdf_path)
    all_text = ""  # Initialize an empty string to store the extracted text
    # Iterate through each page in the PDF
    for page_num in range(mypdf.page_count):
        page = mypdf[page_num]  # Get the page
        text = page.get_text("text")  # Extract text from the page
        all_text += text  # Append the extracted text to the all_text string
    return all_text  # Return the extracted text

def create_embeddings(text, model="BAAI/bge-en-icl"):
    """
    Creates embeddings for the given text using the specified OpenAI model.
    
    Args:
    text (str): The input text for which embeddings are to be created.
    model (str): The model to be used for creating embeddings.
    
    Returns:
    List[float]: The embedding vector.
    """
    # Handle both string and list inputs by converting string input to a list
    input_text = text if isinstance(text, list) else [text]
    # Create embeddings for the input text using the specified model
    response = client.embeddings.create(
        model=model,
        input=input_text
    )
    # If input was a string, return just the first embedding
    if isinstance(text, str):
        return response.data[0].embedding
    # Otherwise, return all embeddings as a list of vectors
    return [item.embedding for item in response.data]

def rewrite_query(original_query, model="meta-llama/Llama-3.2-3B-Instruct"):
    """
    Rewrites a query to make it more specific and detailed for better retrieval.
    
    Args:
    original_query (str): The original user query
    model (str): The model to use for query rewriting
    
    Returns:
    str: The rewritten query
    """
    # Define the system prompt to guide the AI assistant's behavior
    system_prompt = "You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information."  # Define the user prompt with the original query to be rewritten
    user_prompt = f"""
    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information. Original query: {original_query}
    Rewritten query:
    """
    # Generate the rewritten query using the specified model
    response = client.chat.completions.create(
        model=model,
        temperature=0.0,  # Low temperature for deterministic output
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )
    # Return the rewritten query, stripping any leading/trailing whitespace
    return response.choices[0].message.content.strip()

def generate_step_back_query(original_query):
    """
    Generates a broader query to retrieve contextual background information.
    
    Args:
    original_query (str): The original user query
    
    Returns:
    str: The step-back query
    """
    # Define the system prompt to guide the AI assistant's behavior
    system_prompt = "You are an AI assistant specialized in generating broader queries for contextual background information. Your task is to generate a query that retrieves useful contextual information."  # Define the user prompt with the original query to be transformed
    user_prompt = f"""
    Generate a broader query to retrieve contextual background information. Include relevant terms and concepts that might help in retrieving accurate information. Original query: {original_query}
    Generate a query:
    """
    # Generate the step-back query using the specified model
    response = client.chat.completions.create(
        model="meta-llama/Llama-3.2-3B-Instruct",
        temperature=0.0,  # Low temperature for deterministic output
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )
    # Return the step-back query
    return response.choices[0].message.content.strip()

def decompose_query(original_query, num_subqueries=4, model="meta-llama/Llama-3.2-3B-Instruct"):
    """
    Decomposes a query into multiple subqueries for better retrieval.
    
    Args:
    original_query (str): The original user query
    num_subqueries (int): The number of subqueries to be generated
    model (str): The model to use for generating subqueries
    
    Returns:
    List[str]: The subqueries generated by the AI assistant
    """
    # Define the system prompt to guide the AI assistant's behavior
    system_prompt = "You are an AI assistant specialized in generating subqueries for contextual background information. Your task is to generate a query that retrieves useful contextual information."  # Define the user prompt with the original query to be transformed
    user_prompt = f"""
    Generate a subquery to retrieve contextual background information. Include relevant terms and concepts that might help in retrieving accurate information. Original query: {original_query}
    Generate a subquery:
    """
    # Generate the subqueries using the specified model
    response = client.chat.completions.create(
        model=model,
        temperature=0.0,  # Low temperature for deterministic output
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )
    # Return the subqueries
    return response.choices[0].message.content.strip()

def main():
    # Example usage 1: Extract text from a PDF file
    pdf_path = "example.pdf"
    extracted_text = extract_text_from_pdf(pdf_path)
    print(f"Extracted text from PDF file: {extracted_text}")

    # Example usage 2: Create embeddings for a given text
    text = "This is an example PDF file."
    embeddings = create_embeddings(text)
    print(f"Embedded vectors: {embeddings}")

    # Example usage 3: Rewrite a query
    original_query = "This is a sample query."
    rewritten_query = rewrite_query(original_query)
    print(f"Rewritten query: {rewritten_query}")

    # Example usage 4: Decompose a query
    original_query = "This is a sample query."
    num_subqueries = 4
    model = "meta-llama/Llama-3.2-3B-Instruct"
    subqueries = decompose_query(original_query, num_subqueries, model)
    print(f"Subqueries: {subqueries}")

if __name__ == "__main__":
    main()