[
  "### 3. Sub-query Decomposition\nThis technique breaks down complex queries into simpler components for comprehensive retrieval.\n## Implementing RAG with Query Transformations\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text\n# Query Transformations for Enhanced RAG Systems\nThis notebook implements three query transformation techniques to enhance retrieval performance in RAG systems without relying on specialized libraries like LangChain. By modifying user queries, we can significantly improve the relevance and comprehensiveness of retrieved information.",
  "### 3. Sub-query Decomposition\nCode:\ndef decompose_query(original_query, num_subqueries=4, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nDecomposes a complex query into simpler sub-queries.\nArgs:\noriginal_query (str): The original complex query\nnum_subqueries (int): Number of sub-queries to generate\nmodel (str): The model to use for query decomposition\nReturns:\nList[str]: A list of simpler sub-queries\n\"\"\"\n# Define the system prompt to guide the AI assistant's behavior\nsystem_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.\"\n# Define the user prompt with the original query to be decomposed\nuser_prompt = f\"\"\"\nBreak down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\nOriginal query: {original_query}\nGenerate {num_subqueries} sub-queries, one per line, in this format:\n1. [First sub-query]\n2. [Second sub-query]\nAnd so on...\n\"\"\"",
  "# Generate the sub-queries using the specified model\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0.2,  # Slightly higher temperature for some variation\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n]\n)\n# Process the response to extract sub-queries\ncontent = response.choices[0].message.content.strip()\n# Extract numbered queries using simple parsing\nlines = content.split(\"n\")\nsub_queries = []\nfor line in lines:\nif line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n# Remove the number and leading space\nquery = line.strip()\nquery = query[query.find(\".\")+1:].strip()\nsub_queries.append(query)\nreturn sub_queries\n## Demonstrating Query Transformation Techniques\nLet's apply these techniques to an example query.\n## Building a Simple Vector Store\nTo demonstrate how query transformations integrate with retrieval, let's implement a simple vector store.\n## Creating Embeddings\nCode:\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreates embeddings for the given text using the specified OpenAI model.\nArgs:\ntext (str): The input text for which embeddings are to be created.\nmodel (str): The model to be used for creating embeddings.\nReturns:\nList[float]: The embedding vector.\n\"\"\"",
  "# Handle both string and list inputs by converting string input to a list\ninput_text = text if isinstance(text, list) else [text]\n# Create embeddings for the input text using the specified model\nresponse = client.embeddings.create(\nmodel=model,\ninput=input_text\n)\n# If input was a string, return just the first embedding\nif isinstance(text, str):\nreturn response.data[0].embedding\n# Otherwise, return all embeddings as a list of vectors\nreturn [item.embedding for item in response.data]\n## Implementing Query Transformation Techniques\n### 1. Query Rewriting\nThis technique makes queries more specific and detailed to improve precision in retrieval.\n## Key Transformation Techniques\n1. **Query Rewriting**: Makes queries more specific and detailed for better search precision.\n2. **Step-back Prompting**: Generates broader queries to retrieve useful contextual information.\n3. **Sub-query Decomposition**: Breaks complex queries into simpler components for comprehensive retrieval.",
  "## Building a Simple Vector Store\nCode:\nclass SimpleVectorStore:\n\"\"\"\nA simple vector store implementation using NumPy.\n\"\"\"\ndef __init__(self):\n\"\"\"\nInitialize the vector store.\n\"\"\"\nself.vectors = []  # List to store embedding vectors\nself.texts = []  # List to store original texts\nself.metadata = []  # List to store metadata for each text\ndef add_item(self, text, embedding, metadata=None):\n\"\"\"\nAdd an item to the vector store.\nArgs:\ntext (str): The original text.\nembedding (List[float]): The embedding vector.\nmetadata (dict, optional): Additional metadata.\n\"\"\"\nself.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\nself.texts.append(text)  # Add the original text to texts list\nself.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\ndef similarity_search(self, query_embedding, k=5):\n\"\"\"\nFind the most similar items to a query embedding.\nArgs:\nquery_embedding (List[float]): Query embedding vector.\nk (int): Number of results to return.\nReturns:\nList[Dict]: Top k most similar items with their texts and metadata.\n\"\"\"\nif not self.vectors:\nreturn []  # Return empty list if no vectors are stored\n# Convert query embedding to numpy array\nquery_vector = np.array(query_embedding)",
  "# Calculate similarities using cosine similarity\nsimilarities = []\nfor i, vector in enumerate(self.vectors):\n# Compute cosine similarity between query vector and stored vector\nsimilarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\nsimilarities.append((i, similarity))  # Append index and similarity score\n# Sort by similarity (descending)\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n# Return top k results\nresults = []\nfor i in range(min(k, len(similarities))):\nidx, score = similarities[i]\nresults.append({\n\"text\": self.texts[idx],  # Add the corresponding text\n\"metadata\": self.metadata[idx],  # Add the corresponding metadata\n\"similarity\": score  # Add the similarity score\n})\nreturn results  # Return the list of top k similar items",
  "## RAG with Query Transformations\nCode:\ndef transformed_search(query, vector_store, transformation_type, top_k=3):\n\"\"\"\nSearch using a transformed query.\nArgs:\nquery (str): Original query\nvector_store (SimpleVectorStore): Vector store to search\ntransformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')\ntop_k (int): Number of results to return\nReturns:\nList[Dict]: Search results\n\"\"\"\nprint(f\"Transformation type: {transformation_type}\")\nprint(f\"Original query: {query}\")\nresults = []\nif transformation_type == \"rewrite\":\n# Query rewriting\ntransformed_query = rewrite_query(query)\nprint(f\"Rewritten query: {transformed_query}\")\n# Create embedding for transformed query\nquery_embedding = create_embeddings(transformed_query)\n# Search with rewritten query\nresults = vector_store.similarity_search(query_embedding, k=top_k)\nelif transformation_type == \"step_back\":\n# Step-back prompting\ntransformed_query = generate_step_back_query(query)\nprint(f\"Step-back query: {transformed_query}\")\n# Create embedding for transformed query\nquery_embedding = create_embeddings(transformed_query)\n# Search with step-back query\nresults = vector_store.similarity_search(query_embedding, k=top_k)\nelif transformation_type == \"decompose\":",
  "# Sub-query decomposition\nsub_queries = decompose_query(query)\nprint(\"Decomposed into sub-queries:\")\nfor i, sub_q in enumerate(sub_queries, 1):\nprint(f\"{i}. {sub_q}\")\n# Create embeddings for all sub-queries\nsub_query_embeddings = create_embeddings(sub_queries)\n# Search with each sub-query and combine results\nall_results = []\nfor i, embedding in enumerate(sub_query_embeddings):\nsub_results = vector_store.similarity_search(embedding, k=2)  # Get fewer results per sub-query\nall_results.extend(sub_results)\n# Remove duplicates (keep highest similarity score)\nseen_texts = {}\nfor result in all_results:\ntext = result[\"text\"]\nif text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\nseen_texts[text] = result\n# Sort by similarity and take top_k\nresults = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\nelse:\n# Regular search without transformation\nquery_embedding = create_embeddings(query)\nresults = vector_store.similarity_search(query_embedding, k=top_k)\nreturn results\n### 2. Step-back Prompting\nThis technique generates broader queries to retrieve contextual background information.",
  "## Implementing Query Transformation Techniques\nCode:\ndef rewrite_query(original_query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nRewrites a query to make it more specific and detailed for better retrieval.\nArgs:\noriginal_query (str): The original user query\nmodel (str): The model to use for query rewriting\nReturns:\nstr: The rewritten query\n\"\"\"\n# Define the system prompt to guide the AI assistant's behavior\nsystem_prompt = \"You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.\"\n# Define the user prompt with the original query to be rewritten\nuser_prompt = f\"\"\"\nRewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.\nOriginal query: {original_query}\nRewritten query:\n\"\"\"\n# Generate the rewritten query using the specified model\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0.0,  # Low temperature for deterministic output\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n]\n)\n# Return the rewritten query, stripping any leading/trailing whitespace\nreturn response.choices[0].message.content.strip()",
  "## Evaluating Transformation Techniques\nCode:\ndef compare_responses(results, reference_answer, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nCompare responses from different query transformation techniques.\nArgs:\nresults (Dict): Results from different transformation techniques\nreference_answer (str): Reference answer for comparison\nmodel (str): Model for evaluation\n\"\"\"\n# Define the system prompt to guide the AI assistant's behavior\nsystem_prompt = \"\"\"You are an expert evaluator of RAG systems.\nYour task is to compare different responses generated using various query transformation techniques\nand determine which technique produced the best response compared to the reference answer.\"\"\"\n# Prepare the comparison text with the reference answer and responses from each technique\ncomparison_text = f\"\"\"Reference Answer: {reference_answer}nn\"\"\"\nfor technique, result in results.items():\ncomparison_text += f\"{technique.capitalize()} Query Response:n{result['response']}nn\"\n# Define the user prompt with the comparison text\nuser_prompt = f\"\"\"\n{comparison_text}\nCompare the responses generated by different query transformation techniques to the reference answer.\nFor each technique (original, rewrite, step_back, decompose):\n1. Score the response from 1-10 based on accuracy, completeness, and relevance\n2. Identify strengths and weaknesses\nThen rank the techniques from best to worst and explain which technique performed best overall and why.\n\"\"\"",
  "# Generate the evaluation response using the specified model\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n]\n)\n# Print the evaluation results\nprint(\"n===== EVALUATION RESULTS =====\")\nprint(response.choices[0].message.content)\nprint(\"=============================\")\n## Demonstrating Query Transformation Techniques\nCode:\n# Example query\noriginal_query = \"What are the impacts of AI on job automation and employment?\"\n# Apply query transformations\nprint(\"Original Query:\", original_query)\n# Query Rewriting\nrewritten_query = rewrite_query(original_query)\nprint(\"n1. Rewritten Query:\")\nprint(rewritten_query)\n# Step-back Prompting\nstep_back_query = generate_step_back_query(original_query)\nprint(\"n2. Step-back Query:\")\nprint(step_back_query)\n# Sub-query Decomposition\nsub_queries = decompose_query(original_query, num_subqueries=4)\nprint(\"n3. Sub-queries:\")\nfor i, query in enumerate(sub_queries, 1):\nprint(f\"   {i}. {query}\")",
  "## Running the Complete RAG Pipeline with Query Transformations\nCode:\ndef rag_with_query_transformation(pdf_path, query, transformation_type=None):\n\"\"\"\nRun complete RAG pipeline with optional query transformation.\nArgs:\npdf_path (str): Path to PDF document\nquery (str): User query\ntransformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\nReturns:\nDict: Results including query, transformed query, context, and response\n\"\"\"\n# Process the document to create a vector store\nvector_store = process_document(pdf_path)\n# Apply query transformation and search\nif transformation_type:\n# Perform search with transformed query\nresults = transformed_search(query, vector_store, transformation_type)\nelse:\n# Perform regular search without transformation\nquery_embedding = create_embeddings(query)\nresults = vector_store.similarity_search(query_embedding, k=3)\n# Combine context from search results\ncontext = \"nn\".join([f\"PASSAGE {i+1}:n{result['text']}\" for i, result in enumerate(results)])\n# Generate response based on the query and combined context\nresponse = generate_response(query, context)\n# Return the results including original query, transformation type, context, and response\nreturn {\n\"original_query\": query,\n\"transformation_type\": transformation_type,\n\"context\": context,\n\"response\": response\n}",
  "### 2. Step-back Prompting\nCode:\ndef generate_step_back_query(original_query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates a more general 'step-back' query to retrieve broader context.\nArgs:\noriginal_query (str): The original user query\nmodel (str): The model to use for step-back query generation\nReturns:\nstr: The step-back query\n\"\"\"\n# Define the system prompt to guide the AI assistant's behavior\nsystem_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.\"\n# Define the user prompt with the original query to be generalized\nuser_prompt = f\"\"\"\nGenerate a broader, more general version of the following query that could help retrieve useful background information.\nOriginal query: {original_query}\nStep-back query:\n\"\"\"\n# Generate the step-back query using the specified model\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0.1,  # Slightly higher temperature for some variation\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n]\n)\n# Return the step-back query, stripping any leading/trailing whitespace\nreturn response.choices[0].message.content.strip()",
  "## Generating a Response with Transformed Queries\nCode:\ndef generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates a response based on the query and retrieved context.\nArgs:\nquery (str): User query\ncontext (str): Retrieved context\nmodel (str): The model to use for response generation\nReturns:\nstr: Generated response\n\"\"\"\n# Define the system prompt to guide the AI assistant's behavior\nsystem_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n# Define the user prompt with the context and query\nuser_prompt = f\"\"\"\nContext:\n{context}\nQuestion: {query}\nPlease provide a comprehensive answer based only on the context above.\n\"\"\"\n# Generate the response using the specified model\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,  # Low temperature for deterministic output\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n]\n)\n# Return the generated response, stripping any leading/trailing whitespace\nreturn response.choices[0].message.content.strip()\n## Setting Up the Environment\nCode:\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI",
  "## Evaluation of Query Transformations\nCode:\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\ndata = json.load(f)\n# Extract the first query from the validation data\nquery = data[0]['question']\n# Extract the reference answer from the validation data\nreference_answer = data[0]['ideal_answer']\n# pdf_path\npdf_path = \"data/AI_Information.pdf\"\n# Run evaluation\nevaluation_results = evaluate_transformations(pdf_path, query, reference_answer)\n## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.\n## Setting Up the Environment\nWe begin by importing necessary libraries."
]