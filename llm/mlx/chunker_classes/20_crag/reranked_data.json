[
  {
    "id": "a8dc3341b1ab929ed1ae77df9eb32e3a",
    "rank": 1,
    "doc_index": 8,
    "score": 0.46714603900909424,
    "percent_difference": 0.0,
    "text": "## Document Processing Pipeline\nCode:\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n\"\"\"\nProcess a document into a vector store.\nArgs:\npdf_path (str): Path to the PDF file\nchunk_size (int): Size of each chunk in characters\nchunk_overlap (int): Overlap between chunks in characters\nReturns:\nSimpleVectorStore: Vector store containing document chunks\n\"\"\"\n# Extract text from the PDF file\ntext = extract_text_from_pdf(pdf_path)\n# Split the extracted text into chunks with specified size and overlap\nchunks = chunk_text(text, chunk_size, chunk_overlap)\n# Create embeddings for each chunk of text\nprint(\"Creating embeddings for chunks...\")\nchunk_texts = [chunk[\"text\"] for chunk in chunks]\nchunk_embeddings = create_embeddings(chunk_texts)\n# Initialize a new vector store\nvector_store = SimpleVectorStore()\n# Add the chunks and their embeddings to the vector store\nvector_store.add_items(chunks, chunk_embeddings)\nprint(f\"Vector store created with {len(chunks)} chunks\")\nreturn vector_store",
    "relevance": null,
    "word_count": 193
  },
  {
    "id": "84efe51a23035945f6aec0f214f03395",
    "rank": 2,
    "doc_index": 0,
    "score": 0.2405678778886795,
    "percent_difference": 48.5,
    "text": "# Corrective RAG (CRAG) Implementation\nIn this notebook, I implement Corrective RAG - an advanced approach that dynamically evaluates retrieved information and corrects the retrieval process when necessary, using web search as a fallback.\nCRAG improves on traditional RAG by:\n- Evaluating retrieved content before using it\n- Dynamically switching between knowledge sources based on relevance\n- Correcting the retrieval with web search when local knowledge is insufficient\n- Combining information from multiple sources when appropriate",
    "relevance": null,
    "word_count": 82
  },
  {
    "id": "a677036c51a2fa5ef234267ab2a00bab",
    "rank": 3,
    "doc_index": 7,
    "score": 0.2096597601970037,
    "percent_difference": 55.12,
    "text": "## Creating Embeddings\nCode:\ndef create_embeddings(texts, model=\"text-embedding-3-small\"):\n\"\"\"\nCreate vector embeddings for text inputs using OpenAI's embedding models.\nEmbeddings are dense vector representations of text that capture semantic meaning,\nallowing for similarity comparisons. In RAG systems, embeddings are essential\nfor matching queries with relevant document chunks.\nArgs:\ntexts (str or List[str]): Input text(s) to be embedded. Can be a single string\nor a list of strings.\nmodel (str): The embedding model name to use. Defaults to \"text-embedding-3-small\".\nReturns:\nList[List[float]]: If input is a list, returns a list of embedding vectors.\nIf input is a single string, returns a single embedding vector.\n\"\"\"\n# Handle both single string and list inputs by converting single strings to a list\ninput_texts = texts if isinstance(texts, list) else [texts]\n# Process in batches to avoid API rate limits and payload size restrictions\n# OpenAI API typically has limits on request size and rate\nbatch_size = 100\nall_embeddings = []\n# Process each batch of texts\nfor i in range(0, len(input_texts), batch_size):\n# Extract the current batch of texts\nbatch = input_texts[i:i + batch_size]\n# Make API call to generate embeddings for the current batch\nresponse = client.embeddings.create(\nmodel=model,\ninput=batch\n)\n# Extract the embedding vectors from the response\nbatch_embeddings = [item.embedding for item in response.data]\nall_embeddings.extend(batch_embeddings)\n# If the original input was a single string, return just the first embedding\nif isinstance(texts, str):\nreturn all_embeddings[0]\n# Otherwise return the full list of embeddings\nreturn all_embeddings",
    "relevance": null,
    "word_count": 328
  },
  {
    "id": "50d186ea3702b4aefa0f1dcabeb708d6",
    "rank": 4,
    "doc_index": 11,
    "score": 0.1635027527809143,
    "percent_difference": 65.0,
    "text": "## Knowledge Refinement Function\nCode:\ndef refine_knowledge(text):\n\"\"\"\nExtract and refine key information from text.\nArgs:\ntext (str): Input text to refine\nReturns:\nstr: Refined key points from the text\n\"\"\"\n# Define the system prompt to instruct the model on how to extract key information\nsystem_prompt = \"\"\"\nExtract the key information from the following text as a set of clear, concise bullet points.\nFocus on the most relevant facts and important details.\nFormat your response as a bulleted list with each point on a new line starting with \"* \".\n\"\"\"\ntry:\n# Make a request to the OpenAI API to refine the text\nresponse = client.chat.completions.create(\nmodel=\"gpt-3.5-turbo\",  # Specify the model to use\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n{\"role\": \"user\", \"content\": f\"Text to refine:nn{text}\"}  # User message with the text to refine\n],\ntemperature=0.3  # Set the temperature for response generation\n)\n# Return the refined key points from the response\nreturn response.choices[0].message.content.strip()\nexcept Exception as e:\n# Print the error message and return the original text on error\nprint(f\"Error refining knowledge: {e}\")\nreturn text  # Return original text on error",
    "relevance": null,
    "word_count": 272
  },
  {
    "id": "493f5b7e16f35d389f5cb3e2b8910c10",
    "rank": 5,
    "doc_index": 5,
    "score": 0.16288280487060547,
    "percent_difference": 65.13,
    "text": "## Document Processing Functions\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtract text content from a PDF file.\nArgs:\npdf_path (str): Path to the PDF file\nReturns:\nstr: Extracted text content\n\"\"\"\nprint(f\"Extracting text from {pdf_path}...\")\n# Open the PDF file\npdf = fitz.open(pdf_path)\ntext = \"\"\n# Iterate through each page in the PDF\nfor page_num in range(len(pdf)):\npage = pdf[page_num]\n# Extract text from the current page and append it to the text variable\ntext += page.get_text()\nreturn text",
    "relevance": null,
    "word_count": 120
  },
  {
    "id": "6fe3e081b7cce0a09f361473c3baa906",
    "rank": 6,
    "doc_index": 15,
    "score": 0.16162804514169693,
    "percent_difference": 65.4,
    "text": "## Complete Evaluation Pipeline\nCode:\ndef run_crag_evaluation(pdf_path, test_queries, reference_answers=None):\n\"\"\"\nRun a complete evaluation of CRAG with multiple test queries.\nArgs:\npdf_path (str): Path to the PDF document\ntest_queries (List[str]): List of test queries\nreference_answers (List[str], optional): Reference answers for queries\nReturns:\nDict: Complete evaluation results\n\"\"\"\n# Process document and create vector store\nvector_store = process_document(pdf_path)\nresults = []\nfor i, query in enumerate(test_queries):\nprint(f\"nn===== Evaluating Query {i+1}/{len(test_queries)} =====\")\nprint(f\"Query: {query}\")\n# Get reference answer if available\nreference = None\nif reference_answers and i < len(reference_answers):\nreference = reference_answers[i]\n# Run comparison between CRAG and standard RAG\nresult = compare_crag_vs_standard_rag(query, vector_store, reference)\nresults.append(result)\n# Display comparison results\nprint(\"n=== Comparison ===\")\nprint(result[\"comparison\"])\n# Generate overall analysis from individual results\noverall_analysis = generate_overall_analysis(results)\nreturn {\n\"results\": results,\n\"overall_analysis\": overall_analysis\n}",
    "relevance": null,
    "word_count": 232
  },
  {
    "id": "84f93103aaabc867b4aab5ee4fecace9",
    "rank": 7,
    "doc_index": 6,
    "score": 0.15643550604581832,
    "percent_difference": 66.51,
    "text": "## Simple Vector Store Implementation\nCode:\nclass SimpleVectorStore:\n\"\"\"\nA simple vector store implementation using NumPy.\n\"\"\"\ndef __init__(self):\n# Initialize lists to store vectors, texts, and metadata\nself.vectors = []\nself.texts = []\nself.metadata = []\ndef add_item(self, text, embedding, metadata=None):\n\"\"\"\nAdd an item to the vector store.\nArgs:\ntext (str): The text content\nembedding (List[float]): The embedding vector\nmetadata (Dict, optional): Additional metadata\n\"\"\"\n# Append the embedding, text, and metadata to their respective lists\nself.vectors.append(np.array(embedding))\nself.texts.append(text)\nself.metadata.append(metadata or {})\ndef add_items(self, items, embeddings):\n\"\"\"\nAdd multiple items to the vector store.\nArgs:\nitems (List[Dict]): List of items with text and metadata\nembeddings (List[List[float]]): List of embedding vectors\n\"\"\"\n# Iterate over items and embeddings and add them to the store\nfor i, (item, embedding) in enumerate(zip(items, embeddings)):\nself.add_item(\ntext=item[\"text\"],\nembedding=embedding,\nmetadata=item.get(\"metadata\", {})\n)\ndef similarity_search(self, query_embedding, k=5):\n\"\"\"\nFind the most similar items to a query embedding.\nArgs:\nquery_embedding (List[float]): Query embedding vector\nk (int): Number of results to return\nReturns:\nList[Dict]: Top k most similar items\n\"\"\"\n# Return an empty list if there are no vectors in the store\nif not self.vectors:\nreturn []\n# Convert query embedding to numpy array\nquery_vector = np.array(query_embedding)\n# Calculate similarities using cosine similarity\nsimilarities = []\nfor i, vector in enumerate(self.vectors):\nsimilarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\nsimilarities.append((i, similarity))\n# Sort by similarity (descending)\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n# Return top k results\nresults = []\nfor i in range(min(k, len(similarities))):\nidx, score = similarities[i]\nresults.append({\n\"text\": self.texts[idx],\n\"metadata\": self.metadata[idx],\n\"similarity\": float(score)\n})\nreturn results",
    "relevance": null,
    "word_count": 475
  },
  {
    "id": "7a909e430784fd20a6871b5de1b2d09c",
    "rank": 8,
    "doc_index": 16,
    "score": 0.12932872772216797,
    "percent_difference": 72.32,
    "text": "## Evaluation of CRAG with Test Queries\nCode:\n# Path to the AI information PDF document\npdf_path = \"data/AI_Information.pdf\"\n# Run comprehensive evaluation with multiple AI-related queries\ntest_queries = [\n\"How does machine learning differ from traditional programming?\",\n]\n# Optional reference answers for better quality evaluation\nreference_answers = [\n\"Machine learning differs from traditional programming by having computers learn patterns from data rather than following explicit instructions. In traditional programming, developers write specific rules for the computer to follow, while in machine learning\",\n]\n# Run the full evaluation comparing CRAG vs standard RAG\nevaluation_results = run_crag_evaluation(pdf_path, test_queries, reference_answers)\nprint(\"n=== Overall Analysis of CRAG vs Standard RAG ===\")\nprint(evaluation_results[\"overall_analysis\"])",
    "relevance": null,
    "word_count": 142
  },
  {
    "id": "48813dc6f1cf42da59f066f1a20b55a5",
    "rank": 9,
    "doc_index": 2,
    "score": 0.12182316184043884,
    "percent_difference": 73.92,
    "text": "## Setting Up the Environment\nCode:\nimport os\nimport numpy as np\nimport json\nimport fitz  # PyMuPDF\nfrom openai import OpenAI\nimport requests\nfrom typing import List, Dict, Tuple, Any\nimport re\nfrom urllib.parse import quote_plus\nimport time",
    "relevance": null,
    "word_count": 44
  },
  {
    "id": "531d2f5f37148c247f1cb83d4828e604",
    "rank": 10,
    "doc_index": 12,
    "score": 0.11926406845450402,
    "percent_difference": 74.47,
    "text": "## Core CRAG Process\nCode:\ndef crag_process(query, vector_store, k=3):\n\"\"\"\nRun the Corrective RAG process.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store with document chunks\nk (int): Number of initial documents to retrieve\nReturns:\nDict: Process results including response and debug info\n\"\"\"\nprint(f\"n=== Processing query with CRAG: {query} ===n\")\n# Step 1: Create query embedding and retrieve documents\nprint(\"Retrieving initial documents...\")\nquery_embedding = create_embeddings(query)\nretrieved_docs = vector_store.similarity_search(query_embedding, k=k)\n# Step 2: Evaluate document relevance\nprint(\"Evaluating document relevance...\")\nrelevance_scores = []\nfor doc in retrieved_docs:\nscore = evaluate_document_relevance(query, doc[\"text\"])\nrelevance_scores.append(score)\ndoc[\"relevance\"] = score\nprint(f\"Document scored {score:.2f} relevance\")\n# Step 3: Determine action based on best relevance score\nmax_score = max(relevance_scores) if relevance_scores else 0\nbest_doc_idx = relevance_scores.index(max_score) if relevance_scores else -1\n# Track sources for attribution\nsources = []\nfinal_knowledge = \"\"\n# Step 4: Execute the appropriate knowledge acquisition strategy\nif max_score > 0.7:\n# Case 1: High relevance - Use document directly\nprint(f\"High relevance ({max_score:.2f}) - Using document directly\")\nbest_doc = retrieved_docs[best_doc_idx][\"text\"]\nfinal_knowledge = best_doc\nsources.append({\n\"title\": \"Document\",\n\"url\": \"\"\n})\nelif max_score < 0.3:\n# Case 2: Low relevance - Use web search\nprint(f\"Low relevance ({max_score:.2f}) - Performing web search\")\nweb_results, web_sources = perform_web_search(query)\nfinal_knowledge = refine_knowledge(web_results)\nsources.extend(web_sources)\nelse:\n# Case 3: Medium relevance - Combine document with web search\nprint(f\"Medium relevance ({max_score:.2f}) - Combining document with web search\")\nbest_doc = retrieved_docs[best_doc_idx][\"text\"]\nrefined_doc = refine_knowledge(best_doc)\n# Get web results\nweb_results, web_sources = perform_web_search(query)\nrefined_web = refine_knowledge(web_results)\n# Combine knowledge\nfinal_knowledge = f\"From document:n{refined_doc}nnFrom web search:n{refined_web}\"\n# Add sources\nsources.append({\n\"title\": \"Document\",\n\"url\": \"\"\n})\nsources.extend(web_sources)\n# Step 5: Generate final response\nprint(\"Generating final response...\")\nresponse = generate_response(query, final_knowledge, sources)\n# Return comprehensive results\nreturn {\n\"query\": query,\n\"response\": response,\n\"retrieved_docs\": retrieved_docs,\n\"relevance_scores\": relevance_scores,\n\"max_relevance\": max_score,\n\"final_knowledge\": final_knowledge,\n\"sources\": sources\n}",
    "relevance": null,
    "word_count": 549
  },
  {
    "id": "1e465b3a787d96d4cdcc7fef4a744ae7",
    "rank": 11,
    "doc_index": 10,
    "score": 0.10052795186638833,
    "percent_difference": 78.48,
    "text": "## Web Search Function\nCode:\ndef duck_duck_go_search(query, num_results=3):\n\"\"\"\nPerform a web search using DuckDuckGo.\nArgs:\nquery (str): Search query\nnum_results (int): Number of results to return\nReturns:\nTuple[str, List[Dict]]: Combined search results text and source metadata\n\"\"\"\n# Encode the query for URL\nencoded_query = quote_plus(query)\n# DuckDuckGo search API endpoint (unofficial)\nurl = f\"https://api.duckduckgo.com/?q={encoded_query}&format=json\"\ntry:\n# Perform the web search request\nresponse = requests.get(url, headers={\n\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n})\ndata = response.json()\n# Initialize variables to store results text and sources\nresults_text = \"\"\nsources = []\n# Add abstract if available\nif data.get(\"AbstractText\"):\nresults_text += f\"{data['AbstractText']}nn\"\nsources.append({\n\"title\": data.get(\"AbstractSource\", \"Wikipedia\"),\n\"url\": data.get(\"AbstractURL\", \"\")\n})\n# Add related topics\nfor topic in data.get(\"RelatedTopics\", [])[:num_results]:\nif \"Text\" in topic and \"FirstURL\" in topic:\nresults_text += f\"{topic['Text']}nn\"\nsources.append({\n\"title\": topic.get(\"Text\", \"\").split(\" - \")[0],\n\"url\": topic.get(\"FirstURL\", \"\")\n})\nreturn results_text, sources\nexcept Exception as e:\n# Print error message if the main search fails\nprint(f\"Error performing web search: {e}\")\n# Fallback to a backup search API\ntry:\nbackup_url = f\"https://serpapi.com/search.json?q={encoded_query}&engine=duckduckgo\"\nresponse = requests.get(backup_url)\ndata = response.json()\n# Initialize variables to store results text and sources\nresults_text = \"\"\nsources = []\n# Extract results from the backup API\nfor result in data.get(\"organic_results\", [])[:num_results]:\nresults_text += f\"{result.get('title', '')}: {result.get('snippet', '')}nn\"\nsources.append({\n\"title\": result.get(\"title\", \"\"),\n\"url\": result.get(\"link\", \"\")\n})\nreturn results_text, sources\nexcept Exception as backup_error:\n# Print error message if the backup search also fails\nprint(f\"Backup search also failed: {backup_error}\")\nreturn \"Failed to retrieve search results.\", []",
    "relevance": null,
    "word_count": 523
  },
  {
    "id": "cc54073ef80644d1c600277e785a5d65",
    "rank": 12,
    "doc_index": 9,
    "score": 0.10024041185776393,
    "percent_difference": 78.54,
    "text": "## Relevance Evaluation Function\nCode:\ndef evaluate_document_relevance(query, document):\n\"\"\"\nEvaluate the relevance of a document to a query.\nArgs:\nquery (str): User query\ndocument (str): Document text\nReturns:\nfloat: Relevance score (0-1)\n\"\"\"\n# Define the system prompt to instruct the model on how to evaluate relevance\nsystem_prompt = \"\"\"\nYou are an expert at evaluating document relevance.\nRate how relevant the given document is to the query on a scale from 0 to 1.\n0 means completely irrelevant, 1 means perfectly relevant.\nProvide ONLY the score as a float between 0 and 1.\n\"\"\"\n# Define the user prompt with the query and document\nuser_prompt = f\"Query: {query}nnDocument: {document}\"\ntry:\n# Make a request to the OpenAI API to evaluate the relevance\nresponse = client.chat.completions.create(\nmodel=\"gpt-3.5-turbo\",  # Specify the model to use\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n{\"role\": \"user\", \"content\": user_prompt}  # User message with the query and document\n],\ntemperature=0,  # Set the temperature for response generation\nmax_tokens=5  # Very short response needed\n)\n# Extract the score from the response\nscore_text = response.choices[0].message.content.strip()\n# Use regex to find the float value in the response\nscore_match = re.search(r'(d+(.d+)?)', score_text)\nif score_match:\nreturn float(score_match.group(1))  # Return the extracted score as a float\nreturn 0.5  # Default to middle value if parsing fails\nexcept Exception as e:\n# Print the error message and return a default value on error\nprint(f\"Error evaluating document relevance: {e}\")\nreturn 0.5  # Default to middle value on error",
    "relevance": null,
    "word_count": 359
  },
  {
    "id": "6207c70d86f6e805067a86837c69a82f",
    "rank": 13,
    "doc_index": 14,
    "score": 0.0825478034093976,
    "percent_difference": 82.33,
    "text": "## Evaluation Functions\nCode:\ndef evaluate_crag_response(query, response, reference_answer=None):\n\"\"\"\nEvaluate the quality of a CRAG response.\nArgs:\nquery (str): User query\nresponse (str): Generated response\nreference_answer (str, optional): Reference answer for comparison\nReturns:\nDict: Evaluation metrics\n\"\"\"\n# System prompt for the evaluation criteria\nsystem_prompt = \"\"\"\nYou are an expert at evaluating the quality of responses to questions.\nPlease evaluate the provided response based on the following criteria:\n1. Relevance (0-10): How directly does the response address the query?\n2. Accuracy (0-10): How factually correct is the information?\n3. Completeness (0-10): How thoroughly does the response answer all aspects of the query?\n4. Clarity (0-10): How clear and easy to understand is the response?\n5. Source Quality (0-10): How well does the response cite relevant sources?\nReturn your evaluation as a JSON object with scores for each criterion and a brief explanation for each score.\nAlso include an \"overall_score\" (0-10) and a brief \"summary\" of your evaluation.\n\"\"\"\n# User prompt with the query and response to be evaluated\nuser_prompt = f\"\"\"\nQuery: {query}\nResponse to evaluate:\n{response}\n\"\"\"\n# Include reference answer in the prompt if provided\nif reference_answer:\nuser_prompt += f\"\"\"\nReference answer (for comparison):\n{reference_answer}\n\"\"\"\ntry:\n# Request evaluation from the GPT-4 model\nevaluation_response = client.chat.completions.create(\nmodel=\"gpt-4\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\nresponse_format={\"type\": \"json_object\"},\ntemperature=0\n)\n# Parse the evaluation response\nevaluation = json.loads(evaluation_response.choices[0].message.content)\nreturn evaluation\nexcept Exception as e:\n# Handle any errors during the evaluation process\nprint(f\"Error evaluating response: {e}\")\nreturn {\n\"error\": str(e),\n\"overall_score\": 0,\n\"summary\": \"Evaluation failed due to an error.\"\n}",
    "relevance": null,
    "word_count": 426
  },
  {
    "id": "12b2be4e83f22afd7750335835dab11b",
    "rank": 14,
    "doc_index": 13,
    "score": 0.08154383487999439,
    "percent_difference": 82.54,
    "text": "## Response Generation\nCode:\ndef generate_response(query, knowledge, sources):\n\"\"\"\nGenerate a response based on the query and knowledge.\nArgs:\nquery (str): User query\nknowledge (str): Knowledge to base the response on\nsources (List[Dict]): List of sources with title and URL\nReturns:\nstr: Generated response\n\"\"\"\n# Format sources for inclusion in prompt\nsources_text = \"\"\nfor source in sources:\ntitle = source.get(\"title\", \"Unknown Source\")\nurl = source.get(\"url\", \"\")\nif url:\nsources_text += f\"- {title}: {url}n\"\nelse:\nsources_text += f\"- {title}n\"\n# Define the system prompt to instruct the model on how to generate the response\nsystem_prompt = \"\"\"\nYou are a helpful AI assistant. Generate a comprehensive, informative response to the query based on the provided knowledge.\nInclude all relevant information while keeping your answer clear and concise.\nIf the knowledge doesn't fully answer the query, acknowledge this limitation.\nInclude source attribution at the end of your response.\n\"\"\"\n# Define the user prompt with the query, knowledge, and sources\nuser_prompt = f\"\"\"\nQuery: {query}\nKnowledge:\n{knowledge}\nSources:\n{sources_text}\nPlease provide an informative response to the query based on this information.\nInclude the sources at the end of your response.\n\"\"\"\ntry:\n# Make a request to the OpenAI API to generate the response\nresponse = client.chat.completions.create(\nmodel=\"gpt-4\",  # Using GPT-4 for high-quality responses\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0.2\n)\n# Return the generated response\nreturn response.choices[0].message.content.strip()\nexcept Exception as e:\n# Print the error message and return an error response\nprint(f\"Error generating response: {e}\")\nreturn f\"I apologize, but I encountered an error while generating a response to your query: '{query}'. The error was: {str(e)}\"",
    "relevance": null,
    "word_count": 424
  },
  {
    "id": "ec06847c02eaf5195ff95315d2a8295a",
    "rank": 15,
    "doc_index": 3,
    "score": 0.03732878342270851,
    "percent_difference": 92.01,
    "text": "## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.",
    "relevance": null,
    "word_count": 19
  },
  {
    "id": "53b6e66239969bc2f4e5a8dd71ab2cf1",
    "rank": 16,
    "doc_index": 1,
    "score": 0.027201073244214058,
    "percent_difference": 94.18,
    "text": "## Setting Up the Environment\nWe begin by importing necessary libraries.",
    "relevance": null,
    "word_count": 13
  }
]