[
  "## Chunking the Extracted Text\nOnce we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy.\n## Chunking the Extracted Text\nCode:\ndef chunk_text(text, n, overlap):\n\"\"\"\nChunks the given text into segments of n characters with overlap.\nArgs:\ntext (str): The text to be chunked.\nn (int): The number of characters in each chunk.\noverlap (int): The number of overlapping characters between chunks.\nReturns:\nList[str]: A list of text chunks.\n\"\"\"\nchunks = []  # Initialize an empty list to store the chunks\n# Loop through the text with a step size of (n - overlap)\nfor i in range(0, len(text), n - overlap):\n# Append a chunk of text from index i to i + n to the chunks list\nchunks.append(text[i:i + n])\nreturn chunks  # Return the list of text chunks\n## Document Processing Pipeline\nCode:\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n\"\"\"\nProcess a document for RAG.\nArgs:\npdf_path (str): Path to the PDF file.\nchunk_size (int): Size of each chunk in characters.\nchunk_overlap (int): Overlap between chunks in characters.\nReturns:\nSimpleVectorStore: A vector store containing document chunks and their embeddings.\n\"\"\"",
  "# Extract text from the PDF file\nprint(\"Extracting text from PDF...\")\nextracted_text = extract_text_from_pdf(pdf_path)\n# Chunk the extracted text\nprint(\"Chunking text...\")\nchunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\nprint(f\"Created {len(chunks)} text chunks\")\n# Create embeddings for the text chunks\nprint(\"Creating embeddings for chunks...\")\nchunk_embeddings = create_embeddings(chunks)\n# Initialize a simple vector store\nstore = SimpleVectorStore()\n# Add each chunk and its embedding to the vector store\nfor i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\nstore.add_item(\ntext=chunk,\nembedding=embedding,\nmetadata={\"index\": i, \"source\": pdf_path}\n)\nprint(f\"Added {len(chunks)} chunks to the vector store\")\nreturn store\n## Full RAG Pipeline with Reranking\nSo far, we have implemented the core components of the RAG pipeline, including document processing, question answering, and reranking. Now, we will combine these components to create a full RAG pipeline.\n## Extracting Text from a PDF File\nTo implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library.",
  "# Reranking for Enhanced RAG Systems\nThis notebook implements reranking techniques to improve retrieval quality in RAG systems. Reranking acts as a second filtering step after initial retrieval to ensure the most relevant content is used for response generation.\n## Building a Simple Vector Store\nTo demonstrate how reranking integrate with retrieval, let's implement a simple vector store.\n## Extracting Text from a PDF File\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file and prints the first `num_chars` characters.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text",
  "## Creating Embeddings\nCode:\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreates embeddings for the given text using the specified OpenAI model.\nArgs:\ntext (str): The input text for which embeddings are to be created.\nmodel (str): The model to be used for creating embeddings.\nReturns:\nList[float]: The embedding vector.\n\"\"\"\n# Handle both string and list inputs by converting string input to a list\ninput_text = text if isinstance(text, list) else [text]\n# Create embeddings for the input text using the specified model\nresponse = client.embeddings.create(\nmodel=model,\ninput=input_text\n)\n# If input was a string, return just the first embedding\nif isinstance(text, str):\nreturn response.data[0].embedding\n# Otherwise, return all embeddings as a list of vectors\nreturn [item.embedding for item in response.data]\n## Implementing LLM-based Reranking\nLet's implement the LLM-based reranking function using the OpenAI API.\n## Document Processing Pipeline\nNow that we have defined the necessary functions and classes, we can proceed to define the document processing pipeline.",
  "## Full RAG Pipeline with Reranking\nCode:\ndef rag_with_reranking(query, vector_store, reranking_method=\"llm\", top_n=3, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nComplete RAG pipeline incorporating reranking.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store\nreranking_method (str): Method for reranking ('llm' or 'keywords')\ntop_n (int): Number of results to return after reranking\nmodel (str): Model for response generation\nReturns:\nDict: Results including query, context, and response\n\"\"\"\n# Create query embedding\nquery_embedding = create_embeddings(query)\n# Initial retrieval (get more than we need for reranking)\ninitial_results = vector_store.similarity_search(query_embedding, k=10)\n# Apply reranking\nif reranking_method == \"llm\":\nreranked_results = rerank_with_llm(query, initial_results, top_n=top_n)\nelif reranking_method == \"keywords\":\nreranked_results = rerank_with_keywords(query, initial_results, top_n=top_n)\nelse:\n# No reranking, just use top results from initial retrieval\nreranked_results = initial_results[:top_n]\n# Combine context from reranked results\ncontext = \"nn===nn\".join([result[\"text\"] for result in reranked_results])",
  "# Generate response based on context\nresponse = generate_response(query, context, model)\nreturn {\n\"query\": query,\n\"reranking_method\": reranking_method,\n\"initial_results\": initial_results[:top_n],\n\"reranked_results\": reranked_results,\n\"context\": context,\n\"response\": response\n}",
  "## Building a Simple Vector Store\nCode:\nclass SimpleVectorStore:\n\"\"\"\nA simple vector store implementation using NumPy.\n\"\"\"\ndef __init__(self):\n\"\"\"\nInitialize the vector store.\n\"\"\"\nself.vectors = []  # List to store embedding vectors\nself.texts = []  # List to store original texts\nself.metadata = []  # List to store metadata for each text\ndef add_item(self, text, embedding, metadata=None):\n\"\"\"\nAdd an item to the vector store.\nArgs:\ntext (str): The original text.\nembedding (List[float]): The embedding vector.\nmetadata (dict, optional): Additional metadata.\n\"\"\"\nself.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\nself.texts.append(text)  # Add the original text to texts list\nself.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\ndef similarity_search(self, query_embedding, k=5):\n\"\"\"\nFind the most similar items to a query embedding.\nArgs:\nquery_embedding (List[float]): Query embedding vector.\nk (int): Number of results to return.\nReturns:\nList[Dict]: Top k most similar items with their texts and metadata.\n\"\"\"\nif not self.vectors:\nreturn []  # Return empty list if no vectors are stored\n# Convert query embedding to numpy array\nquery_vector = np.array(query_embedding)",
  "# Calculate similarities using cosine similarity\nsimilarities = []\nfor i, vector in enumerate(self.vectors):\n# Compute cosine similarity between query vector and stored vector\nsimilarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\nsimilarities.append((i, similarity))  # Append index and similarity score\n# Sort by similarity (descending)\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n# Return top k results\nresults = []\nfor i in range(min(k, len(similarities))):\nidx, score = similarities[i]\nresults.append({\n\"text\": self.texts[idx],  # Add the corresponding text\n\"metadata\": self.metadata[idx],  # Add the corresponding metadata\n\"similarity\": score  # Add the similarity score\n})\nreturn results  # Return the list of top k similar items",
  "## Implementing LLM-based Reranking\nCode:\ndef rerank_with_llm(query, results, top_n=3, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nReranks search results using LLM relevance scoring.\nArgs:\nquery (str): User query\nresults (List[Dict]): Initial search results\ntop_n (int): Number of results to return after reranking\nmodel (str): Model to use for scoring\nReturns:\nList[Dict]: Reranked results\n\"\"\"\nprint(f\"Reranking {len(results)} documents...\")  # Print the number of documents to be reranked\nscored_results = []  # Initialize an empty list to store scored results\n# Define the system prompt for the LLM\nsystem_prompt = \"\"\"You are an expert at evaluating document relevance for search queries.\nYour task is to rate documents on a scale from 0 to 10 based on how well they answer the given query.\nGuidelines:\n- Score 0-2: Document is completely irrelevant\n- Score 3-5: Document has some relevant information but doesn't directly answer the query\n- Score 6-8: Document is relevant and partially answers the query\n- Score 9-10: Document is highly relevant and directly answers the query\nYou MUST respond with ONLY a single integer score between 0 and 10. Do not include ANY other text.\"\"\"",
  "# Iterate through each result\nfor i, result in enumerate(results):\n# Show progress every 5 documents\nif i % 5 == 0:\nprint(f\"Scoring document {i+1}/{len(results)}...\")\n# Define the user prompt for the LLM\nuser_prompt = f\"\"\"Query: {query}\nDocument:\n{result['text']}\nRate this document's relevance to the query on a scale from 0 to 10:\"\"\"\n# Get the LLM response\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n]\n)\n# Extract the score from the LLM response\nscore_text = response.choices[0].message.content.strip()\n# Use regex to extract the numerical score\nscore_match = re.search(r'b(10|[0-9])b', score_text)\nif score_match:\nscore = float(score_match.group(1))\nelse:\n# If score extraction fails, use similarity score as fallback\nprint(f\"Warning: Could not extract score from response: '{score_text}', using similarity score instead\")\nscore = result[\"similarity\"] * 10\n# Append the scored result to the list\nscored_results.append({\n\"text\": result[\"text\"],\n\"metadata\": result[\"metadata\"],\n\"similarity\": result[\"similarity\"],\n\"relevance_score\": score\n})",
  "# Sort results by relevance score in descending order\nreranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n# Return the top_n results\nreturn reranked_results[:top_n]\n## Evaluating Reranking Quality\nCode:\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\ndata = json.load(f)\n# Extract the first query from the validation data\nquery = data[0]['question']\n# Extract the reference answer from the validation data\nreference_answer = data[0]['ideal_answer']\n# pdf_path\npdf_path = \"data/AI_Information.pdf\"\n## Simple Keyword-based Reranking\nCode:\ndef rerank_with_keywords(query, results, top_n=3):\n\"\"\"\nA simple alternative reranking method based on keyword matching and position.\nArgs:\nquery (str): User query\nresults (List[Dict]): Initial search results\ntop_n (int): Number of results to return after reranking\nReturns:\nList[Dict]: Reranked results\n\"\"\"\n# Extract important keywords from the query\nkeywords = [word.lower() for word in query.split() if len(word) > 3]\nscored_results = []  # Initialize a list to store scored results\nfor result in results:\ndocument_text = result[\"text\"].lower()  # Convert document text to lowercase",
  "# Base score starts with vector similarity\nbase_score = result[\"similarity\"] * 0.5\n# Initialize keyword score\nkeyword_score = 0\nfor keyword in keywords:\nif keyword in document_text:\n# Add points for each keyword found\nkeyword_score += 0.1\n# Add more points if keyword appears near the beginning\nfirst_position = document_text.find(keyword)\nif first_position < len(document_text) / 4:  # In the first quarter of the text\nkeyword_score += 0.1\n# Add points for keyword frequency\nfrequency = document_text.count(keyword)\nkeyword_score += min(0.05 * frequency, 0.2)  # Cap at 0.2\n# Calculate the final score by combining base score and keyword score\nfinal_score = base_score + keyword_score\n# Append the scored result to the list\nscored_results.append({\n\"text\": result[\"text\"],\n\"metadata\": result[\"metadata\"],\n\"similarity\": result[\"similarity\"],\n\"relevance_score\": final_score\n})\n# Sort results by final relevance score in descending order\nreranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n# Return the top_n results\nreturn reranked_results[:top_n]",
  "## Key Concepts of Reranking\n1. **Initial Retrieval**: First pass using basic similarity search (less accurate but faster)\n2. **Document Scoring**: Evaluating each retrieved document's relevance to the query\n3. **Reordering**: Sorting documents by their relevance scores\n4. **Selection**: Using only the most relevant documents for response generation\n## Setting Up the Environment\nCode:\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\nimport re\n## Response Generation\nCode:\ndef generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates a response based on the query and context.\nArgs:\nquery (str): User query\ncontext (str): Retrieved context\nmodel (str): Model to use for response generation\nReturns:\nstr: Generated response\n\"\"\"\n# Define the system prompt to guide the AI's behavior\nsystem_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n# Create the user prompt by combining the context and query\nuser_prompt = f\"\"\"\nContext:\n{context}\nQuestion: {query}\nPlease provide a comprehensive answer based only on the context above.\n\"\"\"",
  "# Generate the response using the specified model\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n]\n)\n# Return the generated response content\nreturn response.choices[0].message.content\n## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.\n## Setting Up the Environment\nWe begin by importing necessary libraries."
]