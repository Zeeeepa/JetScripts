[
  {
    "id": "6325cc74bc273f585567c4f80d579b53",
    "rank": 1,
    "doc_index": 6,
    "score": 0.5761188666025797,
    "percent_difference": 0.0,
    "text": "## Chunking Text Content\nCode:\ndef chunk_text(text_data, chunk_size=1000, overlap=200):\n\"\"\"\nSplit text data into overlapping chunks.\nArgs:\ntext_data (List[Dict]): Text data extracted from PDF\nchunk_size (int): Size of each chunk in characters\noverlap (int): Overlap between chunks in characters\nReturns:\nList[Dict]: Chunked text data\n\"\"\"\nchunked_data = []  # Initialize an empty list to store chunked data\nfor item in text_data:\ntext = item[\"content\"]  # Extract the text content\nmetadata = item[\"metadata\"]  # Extract the metadata\n# Skip if text is too short\nif len(text) < chunk_size / 2:\nchunked_data.append({\n\"content\": text,\n\"metadata\": metadata\n})\ncontinue\n# Create chunks with overlap\nchunks = []\nfor i in range(0, len(text), chunk_size - overlap):\nchunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\nif chunk:  # Ensure we don't add empty chunks\nchunks.append(chunk)\n# Add each chunk with updated metadata\nfor i, chunk in enumerate(chunks):\nchunk_metadata = metadata.copy()  # Copy the original metadata\nchunk_metadata[\"chunk_index\"] = i  # Add chunk index to metadata\nchunk_metadata[\"chunk_count\"] = len(chunks)  # Add total chunk count to metadata\nchunked_data.append({\n\"content\": chunk,  # The chunk text\n\"metadata\": chunk_metadata  # The updated metadata\n})\nprint(f\"Created {len(chunked_data)} text chunks\")  # Print the number of created chunks\nreturn chunked_data  # Return the list of chunked data",
    "relevance": null,
    "word_count": 326
  },
  {
    "id": "83a9fe575626975af88fb89d298ee460",
    "rank": 2,
    "doc_index": 12,
    "score": 0.3924771398305893,
    "percent_difference": 31.88,
    "text": "## Evaluation Against Text-Only RAG\nCode:\ndef build_text_only_store(pdf_path, chunk_size=1000, chunk_overlap=200):\n\"\"\"\nBuild a text-only vector store for comparison.\nArgs:\npdf_path (str): Path to the PDF file\nchunk_size (int): Size of each chunk in characters\nchunk_overlap (int): Overlap between chunks in characters\nReturns:\nMultiModalVectorStore: Text-only vector store\n\"\"\"\n# Extract text from PDF (reuse function but ignore images)\ntext_data, _ = extract_content_from_pdf(pdf_path, None)\n# Chunk text\nchunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n# Extract content for embedding\ncontents = [item[\"content\"] for item in chunked_text]\n# Create embeddings\nprint(\"Creating embeddings for text-only content...\")\nembeddings = create_embeddings(contents)\n# Build vector store\nvector_store = MultiModalVectorStore()\nvector_store.add_items(chunked_text, embeddings)\nprint(f\"Added {len(chunked_text)} text items to text-only vector store\")\nreturn vector_store",
    "relevance": null,
    "word_count": 182
  },
  {
    "id": "4e1836ab758b90fdd474ab73b7f90bb4",
    "rank": 3,
    "doc_index": 10,
    "score": 0.2773761550585429,
    "percent_difference": 51.85,
    "text": "## Complete Processing Pipeline\nCode:\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n\"\"\"\nProcess a document for multi-modal RAG.\nArgs:\npdf_path (str): Path to the PDF file\nchunk_size (int): Size of each chunk in characters\nchunk_overlap (int): Overlap between chunks in characters\nReturns:\nTuple[MultiModalVectorStore, Dict]: Vector store and document info\n\"\"\"\n# Create a directory for extracted images\nimage_dir = \"extracted_images\"\nos.makedirs(image_dir, exist_ok=True)\n# Extract text and images from the PDF\ntext_data, image_paths = extract_content_from_pdf(pdf_path, image_dir)\n# Chunk the extracted text\nchunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n# Process the extracted images to generate captions\nimage_data = process_images(image_paths)\n# Combine all content items (text chunks and image captions)\nall_items = chunked_text + image_data\n# Extract content for embedding\ncontents = [item[\"content\"] for item in all_items]\n# Create embeddings for all content\nprint(\"Creating embeddings for all content...\")\nembeddings = create_embeddings(contents)\n# Build the vector store and add items with their embeddings\nvector_store = MultiModalVectorStore()\nvector_store.add_items(all_items, embeddings)\n# Prepare document info with counts of text chunks and image captions\ndoc_info = {\n\"text_count\": len(chunked_text),\n\"image_count\": len(image_data),\n\"total_items\": len(all_items),\n}\n# Print summary of added items\nprint(f\"Added {len(all_items)} items to vector store ({len(chunked_text)} text chunks, {len(image_data)} image captions)\")\n# Return the vector store and document info\nreturn vector_store, doc_info",
    "relevance": null,
    "word_count": 320
  },
  {
    "id": "2d3c5be01221d594b8d106660fd99bdb",
    "rank": 4,
    "doc_index": 9,
    "score": 0.2339474782347679,
    "percent_difference": 59.39,
    "text": "## Creating Embeddings\nCode:\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreate embeddings for the given texts.\nArgs:\ntexts (List[str]): Input texts\nmodel (str): Embedding model name\nReturns:\nList[List[float]]: Embedding vectors\n\"\"\"\n# Handle empty input\nif not texts:\nreturn []\n# Process in batches if needed (OpenAI API limits)\nbatch_size = 100\nall_embeddings = []\n# Iterate over the input texts in batches\nfor i in range(0, len(texts), batch_size):\nbatch = texts[i:i + batch_size]  # Get the current batch of texts\n# Create embeddings for the current batch\nresponse = client.embeddings.create(\nmodel=model,\ninput=batch\n)\n# Extract embeddings from the response\nbatch_embeddings = [item.embedding for item in response.data]\nall_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\nreturn all_embeddings  # Return all embeddings",
    "relevance": null,
    "word_count": 179
  },
  {
    "id": "818f6fcf541441de582e72bf275aa086",
    "rank": 5,
    "doc_index": 8,
    "score": 0.13985838294029235,
    "percent_difference": 75.72,
    "text": "## Simple Vector Store Implementation\nCode:\nclass MultiModalVectorStore:\n\"\"\"\nA simple vector store implementation for multi-modal content.\n\"\"\"\ndef __init__(self):\n# Initialize lists to store vectors, contents, and metadata\nself.vectors = []\nself.contents = []\nself.metadata = []\ndef add_item(self, content, embedding, metadata=None):\n\"\"\"\nAdd an item to the vector store.\nArgs:\ncontent (str): The content (text or image caption)\nembedding (List[float]): The embedding vector\nmetadata (Dict, optional): Additional metadata\n\"\"\"\n# Append the embedding vector, content, and metadata to their respective lists\nself.vectors.append(np.array(embedding))\nself.contents.append(content)\nself.metadata.append(metadata or {})\ndef add_items(self, items, embeddings):\n\"\"\"\nAdd multiple items to the vector store.\nArgs:\nitems (List[Dict]): List of content items\nembeddings (List[List[float]]): List of embedding vectors\n\"\"\"\n# Loop through items and embeddings and add each to the vector store\nfor item, embedding in zip(items, embeddings):\nself.add_item(\ncontent=item[\"content\"],\nembedding=embedding,\nmetadata=item.get(\"metadata\", {})\n)\ndef similarity_search(self, query_embedding, k=5):\n\"\"\"\nFind the most similar items to a query embedding.\nArgs:\nquery_embedding (List[float]): Query embedding vector\nk (int): Number of results to return\nReturns:\nList[Dict]: Top k most similar items\n\"\"\"\n# Return an empty list if there are no vectors in the store\nif not self.vectors:\nreturn []\n# Convert query embedding to numpy array\nquery_vector = np.array(query_embedding)\n# Calculate similarities using cosine similarity\nsimilarities = []\nfor i, vector in enumerate(self.vectors):\nsimilarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\nsimilarities.append((i, similarity))\n# Sort by similarity (descending)\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n# Return top k results\nresults = []\nfor i in range(min(k, len(similarities))):\nidx, score = similarities[i]\nresults.append({\n\"content\": self.contents[idx],\n\"metadata\": self.metadata[idx],\n\"similarity\": float(score)  # Convert to float for JSON serialization\n})\nreturn results",
    "relevance": null,
    "word_count": 480
  },
  {
    "id": "ff8ce6b8c06a2344d567c64d111a2ac7",
    "rank": 6,
    "doc_index": 11,
    "score": 0.13118663430213928,
    "percent_difference": 77.23,
    "text": "## Query Processing and Response Generation\nCode:\ndef query_multimodal_rag(query, vector_store, k=5):\n\"\"\"\nQuery the multi-modal RAG system.\nArgs:\nquery (str): User query\nvector_store (MultiModalVectorStore): Vector store with document content\nk (int): Number of results to retrieve\nReturns:\nDict: Query results and generated response\n\"\"\"\nprint(f\"n=== Processing query: {query} ===n\")\n# Generate embedding for the query\nquery_embedding = create_embeddings(query)\n# Retrieve relevant content from the vector store\nresults = vector_store.similarity_search(query_embedding, k=k)\n# Separate text and image results\ntext_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"text\"]\nimage_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"image\"]\nprint(f\"Retrieved {len(results)} relevant items ({len(text_results)} text, {len(image_results)} image captions)\")\n# Generate a response using the retrieved content\nresponse = generate_response(query, results)\nreturn {\n\"query\": query,\n\"results\": results,\n\"response\": response,\n\"text_results_count\": len(text_results),\n\"image_results_count\": len(image_results)\n}",
    "relevance": null,
    "word_count": 256
  },
  {
    "id": "f24278856eca9bbb1fc7cccd38a5d2a2",
    "rank": 7,
    "doc_index": 0,
    "score": 0.1198270171880722,
    "percent_difference": 79.2,
    "text": "# Multi-Modal RAG with Image Captioning\nIn this notebook, I implement a Multi-Modal RAG system that extracts both text and images from documents, generates captions for images, and uses both content types to respond to queries. This approach enhances traditional RAG by incorporating visual information into the knowledge base.\nTraditional RAG systems only work with text, but many documents contain crucial information in images, charts, and tables. By captioning these visual elements and incorporating them into our retrieval system, we can:\n- Access information locked in figures and diagrams\n- Understand tables and charts that complement the text\n- Create a more comprehensive knowledge base\n- Answer questions that rely on visual data",
    "relevance": null,
    "word_count": 124
  },
  {
    "id": "e1a4cf7c3602d706917b8b37e6c76cef",
    "rank": 8,
    "doc_index": 13,
    "score": 0.08450000733137131,
    "percent_difference": 85.33,
    "text": "## Evaluation on Multi-Modal RAG vs Text-Only RAG\nCode:\n# Path to your PDF document\npdf_path = \"data/attention_is_all_you_need.pdf\"\n# Define test queries targeting both text and visual content\ntest_queries = [\n\"What is the BLEU score of the Transformer (base model)?\",\n]\n# Optional reference answers for evaluation\nreference_answers = [\n\"The Transformer (base model) achieves a BLEU score of 27.3 on the WMT 2014 English-to-German translation task and 38.1 on the WMT 2014 English-to-French translation task.\",\n]\n# Run evaluation\nevaluation_results = evaluate_multimodal_vs_textonly(\npdf_path=pdf_path,\ntest_queries=test_queries,\nreference_answers=reference_answers\n)\n# Print overall analysis\nprint(\"n=== OVERALL ANALYSIS ===n\")\nprint(evaluation_results[\"overall_analysis\"])",
    "relevance": null,
    "word_count": 129
  },
  {
    "id": "77cc93fd192cf36f332dcdcc137c8d56",
    "rank": 9,
    "doc_index": 5,
    "score": 0.0784108592197299,
    "percent_difference": 86.39,
    "text": "## Document Processing Functions\nCode:\ndef extract_content_from_pdf(pdf_path, output_dir=None):\n\"\"\"\nExtract both text and images from a PDF file.\nArgs:\npdf_path (str): Path to the PDF file\noutput_dir (str, optional): Directory to save extracted images\nReturns:\nTuple[List[Dict], List[Dict]]: Text data and image data\n\"\"\"\n# Create a temporary directory for images if not provided\ntemp_dir = None\nif output_dir is None:\ntemp_dir = tempfile.mkdtemp()\noutput_dir = temp_dir\nelse:\nos.makedirs(output_dir, exist_ok=True)\ntext_data = []  # List to store extracted text data\nimage_paths = []  # List to store paths of extracted images\nprint(f\"Extracting content from {pdf_path}...\")\ntry:\nwith fitz.open(pdf_path) as pdf_file:\n# Loop through every page in the PDF\nfor page_number in range(len(pdf_file)):\npage = pdf_file[page_number]\n# Extract text from the page\ntext = page.get_text().strip()\nif text:\ntext_data.append({\n\"content\": text,\n\"metadata\": {\n\"source\": pdf_path,\n\"page\": page_number + 1,\n\"type\": \"text\"\n}\n})\n# Extract images from the page\nimage_list = page.get_images(full=True)\nfor img_index, img in enumerate(image_list):\nxref = img[0]  # XREF of the image\nbase_image = pdf_file.extract_image(xref)\nif base_image:\nimage_bytes = base_image[\"image\"]\nimage_ext = base_image[\"ext\"]\n# Save the image to the output directory\nimg_filename = f\"page_{page_number+1}_img_{img_index+1}.{image_ext}\"\nimg_path = os.path.join(output_dir, img_filename)\nwith open(img_path, \"wb\") as img_file:\nimg_file.write(image_bytes)\nimage_paths.append({\n\"path\": img_path,\n\"metadata\": {\n\"source\": pdf_path,\n\"page\": page_number + 1,\n\"image_index\": img_index + 1,\n\"type\": \"image\"\n}\n})\nprint(f\"Extracted {len(text_data)} text segments and {len(image_paths)} images\")\nreturn text_data, image_paths\nexcept Exception as e:\nprint(f\"Error extracting content: {e}\")\nif temp_dir and os.path.exists(temp_dir):\nshutil.rmtree(temp_dir)\nraise",
    "relevance": null,
    "word_count": 446
  },
  {
    "id": "7976e1f48034426b848e3d866d445694",
    "rank": 10,
    "doc_index": 2,
    "score": 0.0552634559571743,
    "percent_difference": 90.41,
    "text": "## Setting Up the Environment\nCode:\nimport os\nimport io\nimport numpy as np\nimport json\nimport fitz\nfrom PIL import Image\nfrom openai import OpenAI\nimport base64\nimport re\nimport tempfile\nimport shutil",
    "relevance": null,
    "word_count": 36
  },
  {
    "id": "ec06847c02eaf5195ff95315d2a8295a",
    "rank": 11,
    "doc_index": 3,
    "score": 0.03732878342270851,
    "percent_difference": 93.52,
    "text": "## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.",
    "relevance": null,
    "word_count": 19
  },
  {
    "id": "a71e7b16a82be834a10e42f92fa0c145",
    "rank": 12,
    "doc_index": 7,
    "score": 0.03194403648376465,
    "percent_difference": 94.46,
    "text": "## Image Captioning with OpenAI Vision\nCode:\ndef encode_image(image_path):\n\"\"\"\nEncode an image file as base64.\nArgs:\nimage_path (str): Path to the image file\nReturns:\nstr: Base64 encoded image\n\"\"\"\n# Open the image file in binary read mode\nwith open(image_path, \"rb\") as image_file:\n# Read the image file and encode it to base64\nencoded_image = base64.b64encode(image_file.read())\n# Decode the base64 bytes to a string and return\nreturn encoded_image.decode('utf-8')",
    "relevance": null,
    "word_count": 102
  },
  {
    "id": "53b6e66239969bc2f4e5a8dd71ab2cf1",
    "rank": 13,
    "doc_index": 1,
    "score": 0.027201073244214058,
    "percent_difference": 95.28,
    "text": "## Setting Up the Environment\nWe begin by importing necessary libraries.",
    "relevance": null,
    "word_count": 13
  }
]