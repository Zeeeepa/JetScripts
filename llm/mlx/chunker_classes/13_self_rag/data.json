[
  "## Chunking the Extracted Text\nOnce we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy.\n## Chunking the Extracted Text\nCode:\ndef chunk_text(text, n, overlap):\n\"\"\"\nChunks the given text into segments of n characters with overlap.\nArgs:\ntext (str): The text to be chunked.\nn (int): The number of characters in each chunk.\noverlap (int): The number of overlapping characters between chunks.\nReturns:\nList[str]: A list of text chunks.\n\"\"\"\nchunks = []  # Initialize an empty list to store the chunks\n# Loop through the text with a step size of (n - overlap)\nfor i in range(0, len(text), n - overlap):\n# Append a chunk of text from index i to i + n to the chunks list\nchunks.append(text[i:i + n])\nreturn chunks  # Return the list of text chunks\n## Document Processing Pipeline\nCode:\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n\"\"\"\nProcess a document for Self-RAG.\nArgs:\npdf_path (str): Path to the PDF file.\nchunk_size (int): Size of each chunk in characters.\nchunk_overlap (int): Overlap between chunks in characters.\nReturns:\nSimpleVectorStore: A vector store containing document chunks and their embeddings.\n\"\"\"",
  "# Extract text from the PDF file\nprint(\"Extracting text from PDF...\")\nextracted_text = extract_text_from_pdf(pdf_path)\n# Chunk the extracted text\nprint(\"Chunking text...\")\nchunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\nprint(f\"Created {len(chunks)} text chunks\")\n# Create embeddings for each chunk\nprint(\"Creating embeddings for chunks...\")\nchunk_embeddings = create_embeddings(chunks)\n# Initialize the vector store\nstore = SimpleVectorStore()\n# Add each chunk and its embedding to the vector store\nfor i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\nstore.add_item(\ntext=chunk,\nembedding=embedding,\nmetadata={\"index\": i, \"source\": pdf_path}\n)\nprint(f\"Added {len(chunks)} chunks to the vector store\")\nreturn store\n## Simple Vector Store Implementation\nWe'll create a basic vector store to manage document chunks and their embeddings.\n# Self-RAG: A Dynamic Approach to RAG\nIn this notebook, I implement Self-RAG, an advanced RAG system that dynamically decides when and how to use retrieved information. Unlike traditional RAG approaches, Self-RAG introduces reflection points throughout the retrieval and generation process, resulting in higher quality and more reliable responses.\n## Extracting Text from a PDF File\nTo implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library.",
  "## Self-RAG Components\n### 1. Retrieval Decision\n## Evaluating Self-RAG Against Traditional RAG\nCode:\ndef traditional_rag(query, vector_store, top_k=3):\n\"\"\"\nImplements a traditional RAG approach for comparison.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store containing document chunks\ntop_k (int): Number of documents to retrieve\nReturns:\nstr: Generated response\n\"\"\"\nprint(f\"n=== Running traditional RAG for query: {query} ===n\")\n# Retrieve documents\nprint(\"Retrieving documents...\")\nquery_embedding = create_embeddings(query)  # Create embeddings for the query\nresults = vector_store.similarity_search(query_embedding, k=top_k)  # Search for similar documents\nprint(f\"Retrieved {len(results)} documents\")\n# Combine contexts from retrieved documents\ncontexts = [result[\"text\"] for result in results]  # Extract text from results\ncombined_context = \"nn\".join(contexts)  # Combine texts into a single context\n# Generate response using the combined context\nprint(\"Generating response...\")\nresponse = generate_response(query, combined_context)  # Generate response based on the combined context\nreturn response",
  "## Extracting Text from a PDF File\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file and prints the first `num_chars` characters.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text\n## Running the Complete Self-RAG System\nCode:\ndef run_self_rag_example():\n\"\"\"\nDemonstrates the complete Self-RAG system with examples.\n\"\"\"\n# Process document\npdf_path = \"data/AI_Information.pdf\"  # Path to the PDF document\nprint(f\"Processing document: {pdf_path}\")\nvector_store = process_document(pdf_path)  # Process the document and create a vector store",
  "# Example 1: Query likely needing retrieval\nquery1 = \"What are the main ethical concerns in AI development?\"\nprint(\"n\" + \"=\"*80)\nprint(f\"EXAMPLE 1: {query1}\")\nresult1 = self_rag(query1, vector_store)  # Run Self-RAG for the first query\nprint(\"nFinal response:\")\nprint(result1[\"response\"])  # Print the final response for the first query\nprint(\"nMetrics:\")\nprint(json.dumps(result1[\"metrics\"], indent=2))  # Print the metrics for the first query\n# Example 2: Query likely not needing retrieval\nquery2 = \"Can you write a short poem about artificial intelligence?\"\nprint(\"n\" + \"=\"*80)\nprint(f\"EXAMPLE 2: {query2}\")\nresult2 = self_rag(query2, vector_store)  # Run Self-RAG for the second query\nprint(\"nFinal response:\")\nprint(result2[\"response\"])  # Print the final response for the second query\nprint(\"nMetrics:\")\nprint(json.dumps(result2[\"metrics\"], indent=2))  # Print the metrics for the second query",
  "# Example 3: Query with some relevance to document but requiring additional knowledge\nquery3 = \"How might AI impact economic growth in developing countries?\"\nprint(\"n\" + \"=\"*80)\nprint(f\"EXAMPLE 3: {query3}\")\nresult3 = self_rag(query3, vector_store)  # Run Self-RAG for the third query\nprint(\"nFinal response:\")\nprint(result3[\"response\"])  # Print the final response for the third query\nprint(\"nMetrics:\")\nprint(json.dumps(result3[\"metrics\"], indent=2))  # Print the metrics for the third query\nreturn {\n\"example1\": result1,\n\"example2\": result2,\n\"example3\": result3\n}\n## Creating Embeddings\nCode:\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreates embeddings for the given text.\nArgs:\ntext (str or List[str]): The input text(s) for which embeddings are to be created.\nmodel (str): The model to be used for creating embeddings.\nReturns:\nList[float] or List[List[float]]: The embedding vector(s).\n\"\"\"\n# Handle both string and list inputs by converting string input to a list\ninput_text = text if isinstance(text, list) else [text]\n# Create embeddings for the input text using the specified model\nresponse = client.embeddings.create(\nmodel=model,\ninput=input_text\n)",
  "# If the input was a single string, return just the first embedding\nif isinstance(text, str):\nreturn response.data[0].embedding\n# Otherwise, return all embeddings for the list of texts\nreturn [item.embedding for item in response.data]\n## Key Components of Self-RAG\n1. **Retrieval Decision**: Determines if retrieval is even necessary for a given query\n2. **Document Retrieval**: Fetches potentially relevant documents when needed\n3. **Relevance Evaluation**: Assesses how relevant each retrieved document is\n4. **Response Generation**: Creates responses based on relevant contexts\n5. **Support Assessment**: Evaluates if responses are properly grounded in the context\n6. **Utility Evaluation**: Rates the overall usefulness of generated responses\n## Complete Self-RAG Implementation\nCode:\ndef self_rag(query, vector_store, top_k=3):\n\"\"\"\nImplements the complete Self-RAG pipeline.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store containing document chunks\ntop_k (int): Number of documents to retrieve initially\nReturns:\ndict: Results including query, response, and metrics from the Self-RAG process\n\"\"\"\nprint(f\"n=== Starting Self-RAG for query: {query} ===n\")",
  "# Step 1: Determine if retrieval is necessary\nprint(\"Step 1: Determining if retrieval is necessary...\")\nretrieval_needed = determine_if_retrieval_needed(query)\nprint(f\"Retrieval needed: {retrieval_needed}\")\n# Initialize metrics to track the Self-RAG process\nmetrics = {\n\"retrieval_needed\": retrieval_needed,\n\"documents_retrieved\": 0,\n\"relevant_documents\": 0,\n\"response_support_ratings\": [],\n\"utility_ratings\": []\n}\nbest_response = None\nbest_score = -1\nif retrieval_needed:\n# Step 2: Retrieve documents\nprint(\"nStep 2: Retrieving relevant documents...\")\nquery_embedding = create_embeddings(query)\nresults = vector_store.similarity_search(query_embedding, k=top_k)\nmetrics[\"documents_retrieved\"] = len(results)\nprint(f\"Retrieved {len(results)} documents\")\n# Step 3: Evaluate relevance of each document\nprint(\"nStep 3: Evaluating document relevance...\")\nrelevant_contexts = []\nfor i, result in enumerate(results):\ncontext = result[\"text\"]\nrelevance = evaluate_relevance(query, context)\nprint(f\"Document {i+1} relevance: {relevance}\")\nif relevance == \"relevant\":\nrelevant_contexts.append(context)\nmetrics[\"relevant_documents\"] = len(relevant_contexts)\nprint(f\"Found {len(relevant_contexts)} relevant documents\")\nif relevant_contexts:",
  "# Step 4: Process each relevant context\nprint(\"nStep 4: Processing relevant contexts...\")\nfor i, context in enumerate(relevant_contexts):\nprint(f\"nProcessing context {i+1}/{len(relevant_contexts)}...\")\n# Generate response based on the context\nprint(\"Generating response...\")\nresponse = generate_response(query, context)\n# Assess how well the response is supported by the context\nprint(\"Assessing support...\")\nsupport_rating = assess_support(response, context)\nprint(f\"Support rating: {support_rating}\")\nmetrics[\"response_support_ratings\"].append(support_rating)\n# Rate the utility of the response\nprint(\"Rating utility...\")\nutility_rating = rate_utility(query, response)\nprint(f\"Utility rating: {utility_rating}/5\")\nmetrics[\"utility_ratings\"].append(utility_rating)\n# Calculate overall score (higher for better support and utility)\nsupport_score = {\n\"fully supported\": 3,\n\"partially supported\": 1,\n\"no support\": 0\n}.get(support_rating, 0)\noverall_score = support_score * 5 + utility_rating\nprint(f\"Overall score: {overall_score}\")\n# Keep track of the best response\nif overall_score > best_score:\nbest_response = response\nbest_score = overall_score\nprint(\"New best response found!\")",
  "# If no relevant contexts were found or all responses scored poorly\nif not relevant_contexts or best_score <= 0:\nprint(\"nNo suitable context found or poor responses, generating without retrieval...\")\nbest_response = generate_response(query)\nelse:\n# No retrieval needed, generate directly\nprint(\"nNo retrieval needed, generating response directly...\")\nbest_response = generate_response(query)\n# Final metrics\nmetrics[\"best_score\"] = best_score\nmetrics[\"used_retrieval\"] = retrieval_needed and best_score > 0\nprint(\"n=== Self-RAG Completed ===\")\nreturn {\n\"query\": query,\n\"response\": best_response,\n\"metrics\": metrics\n}\n## Evaluating the Self-RAG System\nThe final step is to evaluate the Self-RAG system against traditional RAG approaches. We'll compare the quality of responses generated by both systems and analyze the performance of Self-RAG in different scenarios.",
  "## Simple Vector Store Implementation\nCode:\nclass SimpleVectorStore:\n\"\"\"\nA simple vector store implementation using NumPy.\n\"\"\"\ndef __init__(self):\n\"\"\"\nInitialize the vector store.\n\"\"\"\nself.vectors = []  # List to store embedding vectors\nself.texts = []  # List to store original texts\nself.metadata = []  # List to store metadata for each text\ndef add_item(self, text, embedding, metadata=None):\n\"\"\"\nAdd an item to the vector store.\nArgs:\ntext (str): The original text.\nembedding (List[float]): The embedding vector.\nmetadata (dict, optional): Additional metadata.\n\"\"\"\nself.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\nself.texts.append(text)  # Add the original text to texts list\nself.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\ndef similarity_search(self, query_embedding, k=5, filter_func=None):\n\"\"\"\nFind the most similar items to a query embedding.\nArgs:\nquery_embedding (List[float]): Query embedding vector.\nk (int): Number of results to return.\nfilter_func (callable, optional): Function to filter results.\nReturns:\nList[Dict]: Top k most similar items with their texts and metadata.\n\"\"\"\nif not self.vectors:\nreturn []  # Return empty list if no vectors are stored\n# Convert query embedding to numpy array\nquery_vector = np.array(query_embedding)",
  "# Calculate similarities using cosine similarity\nsimilarities = []\nfor i, vector in enumerate(self.vectors):\n# Apply filter if provided\nif filter_func and not filter_func(self.metadata[i]):\ncontinue\n# Calculate cosine similarity\nsimilarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\nsimilarities.append((i, similarity))  # Append index and similarity score\n# Sort by similarity (descending)\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n# Return top k results\nresults = []\nfor i in range(min(k, len(similarities))):\nidx, score = similarities[i]\nresults.append({\n\"text\": self.texts[idx],  # Add the text\n\"metadata\": self.metadata[idx],  # Add the metadata\n\"similarity\": score  # Add the similarity score\n})\nreturn results  # Return the list of top k results\n### 3. Support Assessment\nCode:\ndef assess_support(response, context):\n\"\"\"\nAssesses how well a response is supported by the context.\nArgs:\nresponse (str): Generated response\ncontext (str): Context text\nReturns:\nstr: 'fully supported', 'partially supported', or 'no support'\n\"\"\"",
  "# System prompt to instruct the AI on how to evaluate support\nsystem_prompt = \"\"\"You are an AI assistant that determines if a response is supported by the given context.\nEvaluate if the facts, claims, and information in the response are backed by the context.\nAnswer with ONLY one of these three options:\n- \"Fully supported\": All information in the response is directly supported by the context.\n- \"Partially supported\": Some information in the response is supported by the context, but some is not.\n- \"No support\": The response contains significant information not found in or contradicting the context.\n\"\"\"\n# Truncate context if it is too long to avoid exceeding token limits\nmax_context_length = 2000\nif len(context) > max_context_length:\ncontext = context[:max_context_length] + \"... [truncated]\"\n# User prompt containing the context and the response to be evaluated\nuser_prompt = f\"\"\"Context:\n{context}\nResponse:\n{response}\nHow well is this response supported by the context? Answer with ONLY \"Fully supported\", \"Partially supported\", or \"No support\".\n\"\"\"\n# Generate response from the model\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0\n)",
  "# Extract the answer from the model's response and convert to lowercase\nanswer = response.choices[0].message.content.strip().lower()\nreturn answer  # Return the support assessment\n### 2. Relevance Evaluation\nCode:\ndef evaluate_relevance(query, context):\n\"\"\"\nEvaluates the relevance of a context to the query.\nArgs:\nquery (str): User query\ncontext (str): Context text\nReturns:\nstr: 'relevant' or 'irrelevant'\n\"\"\"\n# System prompt to instruct the AI on how to determine document relevance\nsystem_prompt = \"\"\"You are an AI assistant that determines if a document is relevant to a query.\nConsider whether the document contains information that would be helpful in answering the query.\nAnswer with ONLY \"Relevant\" or \"Irrelevant\".\"\"\"\n# Truncate context if it is too long to avoid exceeding token limits\nmax_context_length = 2000\nif len(context) > max_context_length:\ncontext = context[:max_context_length] + \"... [truncated]\"\n# User prompt containing the query and the document content\nuser_prompt = f\"\"\"Query: {query}\nDocument content:\n{context}\nIs this document relevant to the query? Answer with ONLY \"Relevant\" or \"Irrelevant\".\n\"\"\"",
  "# Generate response from the model\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0\n)\n# Extract the answer from the model's response and convert to lowercase\nanswer = response.choices[0].message.content.strip().lower()\nreturn answer  # Return the relevance evaluation\n## Evaluating the Self-RAG System\nCode:\n# Path to the AI information document\npdf_path = \"data/AI_Information.pdf\"\n# Define test queries covering different query types to test Self-RAG's adaptive retrieval\ntest_queries = [\n\"What are the main ethical concerns in AI development?\",        # Document-focused query\n# \"How does explainable AI improve trust in AI systems?\",         # Document-focused query\n# \"Write a poem about artificial intelligence\",                   # Creative query, doesn't need retrieval\n# \"Will superintelligent AI lead to human obsolescence?\"          # Speculative query, partial retrieval needed\n]\n# Reference answers for more objective evaluation\nreference_answers = [\n\"The main ethical concerns in AI development include bias and fairness, privacy, transparency, accountability, safety, and the potential for misuse or harmful applications.\",",
  "# \"Explainable AI improves trust by making AI decision-making processes transparent and understandable to users, helping them verify fairness, identify potential biases, and better understand AI limitations.\",\n# \"A quality poem about artificial intelligence should creatively explore themes of AI's capabilities, limitations, relationship with humanity, potential futures, or philosophical questions about consciousness and intelligence.\",\n# \"Views on superintelligent AI's impact on human relevance vary widely. Some experts warn of potential risks if AI surpasses human capabilities across domains, possibly leading to economic displacement or loss of human agency. Others argue humans will remain relevant through complementary skills, emotional intelligence, and by defining AI's purpose. Most experts agree that thoughtful governance and human-centered design are essential regardless of the outcome.\"\n]\n# Run the evaluation comparing Self-RAG with traditional RAG approaches\nevaluation_results = evaluate_rag_approaches(\npdf_path=pdf_path,                  # Source document containing AI information\ntest_queries=test_queries,          # List of AI-related test queries\nreference_answers=reference_answers  # Ground truth answers for evaluation\n)\n# Print the overall comparative analysis\nprint(\"n=== OVERALL ANALYSIS ===n\")\nprint(evaluation_results[\"overall_analysis\"])\n## Self-RAG Components\nCode:\ndef determine_if_retrieval_needed(query):\n\"\"\"\nDetermines if retrieval is necessary for the given query.\nArgs:\nquery (str): User query\nReturns:\nbool: True if retrieval is needed, False otherwise\n\"\"\"",
  "# System prompt to instruct the AI on how to determine if retrieval is necessary\nsystem_prompt = \"\"\"You are an AI assistant that determines if retrieval is necessary to answer a query.\nFor factual questions, specific information requests, or questions about events, people, or concepts, answer \"Yes\".\nFor opinions, hypothetical scenarios, or simple queries with common knowledge, answer \"No\".\nAnswer with ONLY \"Yes\" or \"No\".\"\"\"\n# User prompt containing the query\nuser_prompt = f\"Query: {query}nnIs retrieval necessary to answer this query accurately?\"\n# Generate response from the model\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0\n)\n# Extract the answer from the model's response and convert to lowercase\nanswer = response.choices[0].message.content.strip().lower()\n# Return True if the answer contains \"yes\", otherwise return False\nreturn \"yes\" in answer\n## Setting Up the Environment\nCode:\nimport os\nimport numpy as np\nimport json\nimport fitz\nfrom openai import OpenAI\nimport re",
  "## Response Generation\nCode:\ndef generate_response(query, context=None):\n\"\"\"\nGenerates a response based on the query and optional context.\nArgs:\nquery (str): User query\ncontext (str, optional): Context text\nReturns:\nstr: Generated response\n\"\"\"\n# System prompt to instruct the AI on how to generate a helpful response\nsystem_prompt = \"\"\"You are a helpful AI assistant. Provide a clear, accurate, and informative response to the query.\"\"\"\n# Create the user prompt based on whether context is provided\nif context:\nuser_prompt = f\"\"\"Context:\n{context}\nQuery: {query}\nPlease answer the query based on the provided context.\n\"\"\"\nelse:\nuser_prompt = f\"\"\"Query: {query}\nPlease answer the query to the best of your ability.\"\"\"\n# Generate the response using the OpenAI client\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0.2\n)\n# Return the generated response text\nreturn response.choices[0].message.content.strip()",
  "### 4. Utility Evaluation\nCode:\ndef rate_utility(query, response):\n\"\"\"\nRates the utility of a response for the query.\nArgs:\nquery (str): User query\nresponse (str): Generated response\nReturns:\nint: Utility rating from 1 to 5\n\"\"\"\n# System prompt to instruct the AI on how to rate the utility of the response\nsystem_prompt = \"\"\"You are an AI assistant that rates the utility of a response to a query.\nConsider how well the response answers the query, its completeness, correctness, and helpfulness.\nRate the utility on a scale from 1 to 5, where:\n- 1: Not useful at all\n- 2: Slightly useful\n- 3: Moderately useful\n- 4: Very useful\n- 5: Exceptionally useful\nAnswer with ONLY a single number from 1 to 5.\"\"\"\n# User prompt containing the query and the response to be rated\nuser_prompt = f\"\"\"Query: {query}\nResponse:\n{response}\nRate the utility of this response on a scale from 1 to 5:\"\"\"\n# Generate the utility rating using the OpenAI client\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0\n)",
  "# Extract the rating from the model's response\nrating = response.choices[0].message.content.strip()\n# Extract just the number from the rating\nrating_match = re.search(r'[1-5]', rating)\nif rating_match:\nreturn int(rating_match.group())  # Return the extracted rating as an integer\nreturn 3  # Default to middle rating if parsing fails\n## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.\n## Setting Up the Environment\nWe begin by importing necessary libraries."
]