[
  {
    "id": "5d3eb9bb8b519ac43e200b24d034cf83",
    "rank": 1,
    "doc_index": 4,
    "score": 0.5472935140132904,
    "percent_difference": 0.0,
    "text": "## Data Preprocessing\nNow that we have moved onto the data preprocessing stage, we need to load the data and preprocess it. Let's create a function that will load all the `.txt` files from a directory and return a list of documents.\nWe need to create a function that performs chunking of the documents once they are loaded. We are using a `chunk_size` of `100` characters, but you can adjust it as per your requirements.\nThis step is **optional**, where we preprocess each chunk by removing special characters, converting to lowercase, etc.\nHowever, if you are using the previous preprocessing step, you can simply create a function to preprocess the entire document.\nNow that we have implemented all the functions for data preprocessing, we can load the documents from the directory, split them into chunks, and preprocess the chunks.\nPrint the first 200 characters of the first two chunks",
    "relevance": null,
    "word_count": 178
  },
  {
    "id": "5a339e5678975fdbd53d274690f2a975",
    "rank": 2,
    "doc_index": 6,
    "score": 0.41550859808921814,
    "percent_difference": 24.08,
    "text": "## Document Embedding Generation\nIn the previous step, we chunked our document. Now it's time to generate embeddings for the chunk dataset. When working with RAG, our knowledge base is typically quite large. Therefore, we need to perform embedding generation in batches. Let's create a core function to generate embeddings for the chunks in batches.\nThe embedding model we are using is `BAAI/bge-en-icl`.\nNext, we will define a function to generate embeddings for all text chunks in batches. This function will take a list of text chunks as input and generate embeddings for each batch of chunks using the OpenAI client. The function will return a list of embeddings corresponding to all the text chunks.\nLet's create another function to save the embeddings to a file in JSON format.\nNow that we have implemented all the functions for embedding generation, we can proceed to generate embeddings for the preprocessed text chunks and save them to a JSON file.",
    "relevance": null,
    "word_count": 180
  },
  {
    "id": "df4be32a7831f345bff43528f112c6f2",
    "rank": 3,
    "doc_index": 15,
    "score": 0.4127329885959625,
    "percent_difference": 24.59,
    "text": "## Basic RAG Pipeline\nCode:\n# Function to implement the basic Retrieval-Augmented Generation (RAG) pipeline\ndef basic_rag_pipeline(query: str) -> str:\n\"\"\"\nImplement the basic Retrieval-Augmented Generation (RAG) pipeline:\nretrieve relevant chunks, construct a prompt, and generate a response.\nArgs:\nquery (str): The input query for which a response is to be generated.\nReturns:\nstr: The generated response from the LLM based on the query and retrieved context.\n\"\"\"\n# Step 1: Retrieve the most relevant chunks for the given query\nrelevant_chunks: List[str] = retrieve_relevant_chunks(query)\n# Step 2: Construct a prompt using the query and the retrieved chunks\nprompt: str = construct_prompt(query, relevant_chunks)\n# Step 3: Generate a response from the LLM using the constructed prompt\nresponse: str = generate_response(prompt)\n# Return the generated response\nreturn response",
    "relevance": null,
    "word_count": 172
  },
  {
    "id": "d39f20866d8cb2e5684f892d83a746fe",
    "rank": 4,
    "doc_index": 10,
    "score": 0.3856881558895111,
    "percent_difference": 29.53,
    "text": "## Simple Retrieval Implementation\nWe do know for retrieving the most similar text chunks to a given query, we can use the cosine similarity between the query embedding and the embeddings of all text chunks. The higher the cosine similarity, the more similar the text chunks are. We can then sort the chunks based on their similarity scores and return the top-k most similar chunks.\nSo, let's implement a simple cosine similarity-based retrieval function.\nThe cosine similarity between two vectors $A$ and $B$ is calculated as:\n$$text{cosine similarity} = frac{A cdot B}{||A|| times ||B||} = frac{sum_{i=1}^{n} A_i B_i}{sqrt{sum_{i=1}^{n} A_i^2} times sqrt{sum_{i=1}^{n} B_i^2}}$$\nWhere:\n- $A cdot B$ is the dot product of vectors $A$ and $B$\n- $||A||$ and $||B||$ are the Euclidean norms (magnitudes) of the vectors\n- $n$ is the dimension of the vectors\nWhen we calculate the cosine similarity between a query and all the chunks, we can perform a similarity search. Based on the `top_k` parameter, we retrieve the top k most similar chunks.\nOnce we have the similarity search function ready, we can simply code a retrieval function on top of it that will provide the relevant chunks based on the query.\nNow that we have implemented all the functions for retrieval, we can proceed to test the retrieval system with a sample query.",
    "relevance": null,
    "word_count": 304
  },
  {
    "id": "036598d1927982ab97d573d2951781fe",
    "rank": 5,
    "doc_index": 34,
    "score": 0.3425707519054413,
    "percent_difference": 37.41,
    "text": "## Evaluation Framework (**Optional**)\nCode:\n# Function to evaluate relevance of retrieved chunks\ndef evaluate_relevance(retrieved_chunks: List[str], ground_truth_chunks: List[str]) -> float:\n\"\"\"\nEvaluate the relevance of retrieved chunks by comparing them to ground truth chunks.\nArgs:\nretrieved_chunks (List[str]): A list of text chunks retrieved by the system.\nground_truth_chunks (List[str]): A list of ground truth text chunks for comparison.\nReturns:\nfloat: The average relevance score between the retrieved chunks and the ground truth chunks.\n\"\"\"\nrelevance_scores: List[float] = []  # Initialize a list to store relevance scores\n# Iterate through pairs of retrieved and ground truth chunks\nfor retrieved, ground_truth in zip(retrieved_chunks, ground_truth_chunks):\n# Calculate the cosine similarity between the embeddings of the retrieved and ground truth chunks\nrelevance: float = cosine_similarity(\ngenerate_embeddings([retrieved])[0],\ngenerate_embeddings([ground_truth])[0]\n)\n# Append the relevance score to the list\nrelevance_scores.append(relevance)\n# Return the average relevance score\nreturn np.mean(relevance_scores)",
    "relevance": null,
    "word_count": 220
  },
  {
    "id": "5a8bba7dc56dfebaffcdf9c04c84eab8",
    "rank": 6,
    "doc_index": 9,
    "score": 0.3416368067264557,
    "percent_difference": 37.58,
    "text": "## Vector Store Implementation\nCode:\n# Initialize an in-memory vector store as a dictionary\n# The keys will be unique identifiers (integers), and the values will be dictionaries containing embeddings and corresponding text chunks\nvector_store: dict[int, dict[str, object]] = {}\n# Function to add embeddings and corresponding text chunks to the vector store\ndef add_to_vector_store(embeddings: np.ndarray, chunks: List[str]) -> None:\n\"\"\"\nAdd embeddings and their corresponding text chunks to the vector store.\nArgs:\nembeddings (np.ndarray): A NumPy array containing the embeddings to add.\nchunks (List[str]): A list of text chunks corresponding to the embeddings.\nReturns:\nNone\n\"\"\"\n# Iterate over embeddings and chunks simultaneously\nfor embedding, chunk in zip(embeddings, chunks):\n# Add each embedding and its corresponding chunk to the vector store\n# Use the current length of the vector store as the unique key\nvector_store[len(vector_store)] = {\"embedding\": embedding, \"chunk\": chunk}",
    "relevance": null,
    "word_count": 206
  },
  {
    "id": "552beab60c5d7b7380676142acba6194",
    "rank": 7,
    "doc_index": 7,
    "score": 0.3350164592266083,
    "percent_difference": 38.79,
    "text": "## Document Embedding Generation\nCode:\n# Function to generate embeddings for a single batch of text chunks\ndef generate_embeddings_batch(chunks_batch: List[str], model: str = \"BAAI/bge-en-icl\") -> List[List[float]]:\n\"\"\"\nGenerate embeddings for a batch of text chunks using the OpenAI client.\nArgs:\nchunks_batch (List[str]): A batch of text chunks to generate embeddings for.\nmodel (str): The model to use for embedding generation. Default is \"BAAI/bge-en-icl\".\nReturns:\nList[List[float]]: A list of embeddings, where each embedding is a list of floats.\n\"\"\"\n# Use the OpenAI client to create embeddings for the input batch\nresponse = client.embeddings.create(\nmodel=model,  # Specify the model to use for embedding generation\ninput=chunks_batch  # Provide the batch of text chunks as input\n)\n# Extract embeddings from the response and return them\nembeddings = [item.embedding for item in response.data]\nreturn embeddings",
    "relevance": null,
    "word_count": 186
  },
  {
    "id": "b627c3913da8940ed8060bf3ff65a5aa",
    "rank": 8,
    "doc_index": 14,
    "score": 0.2810547351837158,
    "percent_difference": 48.65,
    "text": "## Basic RAG Pipeline\nWe cannot run small pieces of code repeatedly. Therefore, we need to create a simple RAG pipeline that takes only one parameter, which is our query, and returns the LLM response.",
    "relevance": null,
    "word_count": 42
  },
  {
    "id": "317112cb77fbdf7c5aa23e47ace2ebe3",
    "rank": 9,
    "doc_index": 21,
    "score": 0.27815161645412445,
    "percent_difference": 49.18,
    "text": "## Action Function Logic\nNow that we have defined the action space, we need to implement the logic for each action. This logic will determine how the RAG pipeline should be modified based on the action taken by the RL agent.\nJust to revisit, the four actions are:\n- `rewrite_query`: Reformulate the original query to improve retrieval\n- `expand_context`: Retrieve additional context chunks\n- `filter_context`: Remove irrelevant context chunks\n- `generate_response`: Generate a response based on the current query and context\nLet's create our first action logic for the agent. The first action we will implement is the `rewrite_query` action, which involves reformulating the original user query to improve retrieval performance. This action is crucial for enhancing the relevance of the retrieved context and generating more accurate responses.\nThis action is crucial for enhancing the relevance of the retrieved context and generating more accurate responses.\nLet's code our next action logic, which is to expand the context by retrieving additional chunks. We will use the existing function `retrieve_relevant_chunks` to get more context chunks and then filter out any duplicates from the current context. We will limit the number of new chunks to be added to the context to a specified top_k value.\nWe need to filter the context to keep only the most relevant chunks for the query. This filtering step is crucial to ensure that the context provided to the language model is concise and focused on the most relevant information.\nThis action will help the agent explore more information relevant to the query.",
    "relevance": null,
    "word_count": 291
  },
  {
    "id": "bee956c3b8b20165ba9e74e7a9ae19c9",
    "rank": 10,
    "doc_index": 8,
    "score": 0.2520374357700348,
    "percent_difference": 53.95,
    "text": "## Vector Store Implementation\nSince we are not using any python libraries for vector storage, we will implement a simple vector store using a dictionary.",
    "relevance": null,
    "word_count": 28
  },
  {
    "id": "c326fc8eb6e8417e10b65e0d14afee16",
    "rank": 11,
    "doc_index": 12,
    "score": 0.24005471169948578,
    "percent_difference": 56.14,
    "text": "## LLM Response Generation\nWhen we have a query and a set of relevant document chunks, we can use a large language model (LLM) to generate a response based on the query and the retrieved information. In this section, we will use the OpenAI API to generate a response to a query by providing the query text and the relevant document chunks as context to the LLM.\nFirst we need a function to construct the input prompt for the LLM, which includes the query text and the relevant document chunks as context.\nTo generate an LLM response, we need to implement a function that takes the constructed input prompt and sends it to the OpenAI API for response generation.",
    "relevance": null,
    "word_count": 130
  },
  {
    "id": "e43ea0a9e79cca97b6bb7e5c306625ce",
    "rank": 12,
    "doc_index": 22,
    "score": 0.23690460249781609,
    "percent_difference": 56.71,
    "text": "## Action Function Logic\nCode:\n# Function to rewrite the query for better document retrieval\ndef rewrite_query(\nquery: str,\ncontext_chunks: List[str],\nmodel: str = \"google/gemma-2-2b-it\",\nmax_tokens: int = 100,\ntemperature: float = 0.3\n) -> str:\n\"\"\"\nUse the LLM to rewrite the query for better document retrieval.\nArgs:\nquery (str): The original query text.\ncontext_chunks (List[str]): A list of context chunks retrieved so far.\nmodel (str): The model to use for generating the rewritten query. Default is \"google/gemma-2-2b-it\".\nmax_tokens (int): Maximum number of tokens in the rewritten query. Default is 100.\ntemperature (float): Sampling temperature for response diversity. Default is 0.3.\nReturns:\nstr: The rewritten query optimized for document retrieval.\n\"\"\"\n# Construct a prompt for the LLM to rewrite the query\nrewrite_prompt = f\"\"\"\nYou are a query optimization assistant. Your task is to rewrite the given query to make it more effective\nfor retrieving relevant information. The query will be used for document retrieval.\nOriginal query: {query}\nBased on the context retrieved so far:\n{' '.join(context_chunks[:2]) if context_chunks else 'No context available yet'}\nRewrite the query to be more specific and targeted to retrieve better information.\nRewritten query:\n\"\"\"\n# Use the LLM to generate a rewritten query\nresponse = client.chat.completions.create(\nmodel=model, # Specify the model to use for generating the response\nmax_tokens=max_tokens, # Maximum number of tokens in the response\ntemperature=temperature, # Sampling temperature for response diversity\nmessages=[\n{\n\"role\": \"user\",\n\"content\": rewrite_prompt\n}\n]\n)\n# Extract and return the rewritten query from the response\nrewritten_query = response.choices[0].message.content.strip()\nreturn rewritten_query",
    "relevance": null,
    "word_count": 352
  },
  {
    "id": "865c4654d305748ba3859cb7e373e6a7",
    "rank": 13,
    "doc_index": 23,
    "score": 0.23621856421232224,
    "percent_difference": 56.84,
    "text": "## Policy Network\nPreviously, we defined our state, actions, and reward logic. Next, we need to create a policy network that will select an action based on the current state.\nA policy network is a function that takes the current state and the action space as input and returns the selected action based on the state.\nThe policy network can use a simple heuristic to select an action based on the current state. For example, if there are no previous responses, the policy network can prioritize rewriting the query. If the context has too many chunks, the policy network can choose to filter the context.\nSo our policy network works like this:\n- If there are no previous responses, prioritize rewriting the query.\n- If there are previous responses but the rewards are low, try expanding the context.\n- If the context has too many chunks, try filtering the context.\n- Otherwise, generate a response.",
    "relevance": null,
    "word_count": 178
  },
  {
    "id": "a1ece3a9ba7fbd4b2fe725bdf679dcc0",
    "rank": 14,
    "doc_index": 30,
    "score": 0.22408750280737877,
    "percent_difference": 59.06,
    "text": "## Training Loop\nCode:\n# Function to implement the training loop\ndef training_loop(\nquery_text: str,\nground_truth: str,\nparams: Optional[Dict[str, Union[float, int]]] = None\n) -> Tuple[Dict[str, Dict[str, Union[float, str]]], List[float], List[List[str]], Optional[str]]:\n\"\"\"\nImplement the training loop for RL-enhanced RAG.\nArgs:\nquery_text (str): The input query text for the RAG pipeline.\nground_truth (str): The expected correct answer for the query.\nparams (Optional[Dict[str, Union[float, int]]]): Training parameters such as learning rate,\nnumber of episodes, and discount factor. If None, default parameters are initialized.\nReturns:\nTuple: A tuple containing:\n- policy (Dict[str, Dict[str, Union[float, str]]]): The updated policy after training.\n- rewards_history (List[float]): A list of rewards received in each episode.\n- actions_history (List[List[str]]): A list of actions taken in each episode.\n- best_response (Optional[str]): The best response generated during training.\n\"\"\"\n# Initialize training parameters if not provided\nif params is None:\nparams = initialize_training_params()\n# Initialize variables to track progress\nrewards_history: List[float] = []  # List to store rewards for each episode\nactions_history: List[List[str]] = []  # List to store actions taken in each episode\npolicy: Dict[str, Dict[str, Union[float, str]]] = {}  # Policy dictionary to store actions and rewards\naction_space: List[str] = define_action_space()  # Define the action space\nbest_response: Optional[str] = None  # Variable to store the best response\nbest_reward: float = -1  # Initialize the best reward to a very low value\n# Get initial performance from the simple RAG pipeline for comparison\nsimple_response: str = basic_rag_pipeline(query_text)\nsimple_reward: float = calculate_reward(simple_response, ground_truth)\nprint(f\"Simple RAG reward: {simple_reward:.4f}\")\n# Start the training loop\nfor episode in range(params[\"num_episodes\"]):\n# Reset the environment with the same query\ncontext_chunks: List[str] = retrieve_relevant_chunks(query_text)\nstate: Dict[str, object] = define_state(query_text, context_chunks)\nepisode_reward: float = 0  # Initialize the reward for the current episode\nepisode_actions: List[str] = []  # Initialize the list of actions for the current episode\n# Maximum number of steps per episode to prevent infinite loops\nfor step in range(10):\n# Perform a single RL step\nstate, action, reward, response = rl_step(state, action_space, ground_truth)\nepisode_actions.append(action)  # Record the action taken\n# If a response is generated, end the episode\nif response:\nepisode_reward = reward  # Update the episode reward\n# Track the best response and reward\nif reward > best_reward:\nbest_reward = reward\nbest_response = response\nbreak  # Exit the loop as the episode ends\n# Update rewards and actions history\nrewards_history.append(episode_reward)\nactions_history.append(episode_actions)\n# Print progress every 5 episodes\nif episode % 5 == 0:\nprint(f\"Episode {episode}: Reward = {episode_reward:.4f}, Actions = {episode_actions}\")\n# Compare the best RL-enhanced RAG reward with the simple RAG reward\nimprovement: float = best_reward - simple_reward\nprint(f\"nTraining completed:\")\nprint(f\"Simple RAG reward: {simple_reward:.4f}\")\nprint(f\"Best RL-enhanced RAG reward: {best_reward:.4f}\")\nprint(f\"Improvement: {improvement:.4f} ({improvement * 100:.2f}%)\")\nreturn policy, rewards_history, actions_history, best_response",
    "relevance": null,
    "word_count": 758
  },
  {
    "id": "43bafb5d2ec73f93aae423a82de40d60",
    "rank": 15,
    "doc_index": 39,
    "score": 0.21942131221294403,
    "percent_difference": 59.91,
    "text": "## What can we conclude?\n- The performance of the simple RAG is lower compared to the RL-enhanced RAG on factual queries.\n- The RL-enhanced RAG achieved a 19.5% improvement in the similarity score within 5 episodes.\n- Further improvements can be achieved by:\n- Training for more episodes.\n- Tuning hyperparameters.\n- Time is a key constraint for training.\n- Parallel implementation of the RL algorithm can help reduce training time.",
    "relevance": null,
    "word_count": 82
  },
  {
    "id": "dc9fa303782aef812bdc320b41f43010",
    "rank": 16,
    "doc_index": 17,
    "score": 0.21758383512496948,
    "percent_difference": 60.24,
    "text": "## Evaluate the basic RAG pipeline\nCode:\n# Open the validation data file in read mode and load its content as a dictionary\nwith open('data/val_rl.json', 'r') as file:\nvalidation_data = json.load(file)\n# Test the basic RAG pipeline with a sample query\nsample_query = validation_data['basic_factual_questions'][0]['question']  # Extract the query text\nexpected_answer = validation_data['basic_factual_questions'][0]['answer']  # Extract the ground truth answer\n# print the sample query and expected answer\nprint(f\"Sample Query: {sample_query}n\")\nprint(f\"Expected Answer: {expected_answer}n\")",
    "relevance": null,
    "word_count": 127
  },
  {
    "id": "c4720a78180ce6befedeb4047af49128",
    "rank": 17,
    "doc_index": 16,
    "score": 0.20843073725700378,
    "percent_difference": 61.92,
    "text": "## Evaluate the basic RAG pipeline\nNow that we have coded the basic RAG pipeline, we can use it for evaluation. Our evaluation queries contain different targeted segments, such as `factual_queries` and `complex_nature`. We are going to test the factual knowledge of our RAG pipeline.\nLet's load our evaluation queries and their expected answers.\nLet's test the basic RAG pipeline with this eval query and see how well it performs.\nThe simple RAG pipeline doesn't seem to perform well in its current state. The generated response is not only irrelevant to the ground truth but also misses critical information.\nBut don't worry! In the upcoming steps, we will implement a Reinforcement Learning-based RAG pipeline to address these shortcomings. This will help us improve the retrieval and generation process, making the responses more accurate and contextually relevant.\nStay tuned as we take our RAG pipeline to the next level!",
    "relevance": null,
    "word_count": 172
  },
  {
    "id": "7bb311f167875a6748e16909fb96becd",
    "rank": 18,
    "doc_index": 29,
    "score": 0.20523571968078613,
    "percent_difference": 62.5,
    "text": "## Training Loop\nNow that we have coded every part of the training loop, we can put it all together in a single function that implements the training loop for the RL-enhanced RAG system.\nThis function will take the input query text, the expected ground truth answer, and optionally some training parameters. It will return the updated policy, a list of rewards received in each episode, a list of actions taken in each episode, and the best response generated during training.\nIn more detail, the `training_loop` function will:\n- Initialize training parameters if not provided.\n- Get the initial performance from the simple RAG pipeline for comparison.\n- Start the training loop for the specified number of episodes.\n- Perform a single RL step in each episode.\n- Update rewards and actions history for each episode.\n- Print progress every 5 episodes.\n- Compare the best RL-enhanced RAG reward with the simple RAG reward.\n- Return the updated policy, rewards history, actions history, and the best response generated during training.",
    "relevance": null,
    "word_count": 195
  },
  {
    "id": "1c02ba4ea586ca0e0db888b272258845",
    "rank": 19,
    "doc_index": 13,
    "score": 0.20322918146848679,
    "percent_difference": 62.87,
    "text": "## LLM Response Generation\nCode:\n# Function to construct a prompt with context\ndef construct_prompt(query: str, context_chunks: List[str]) -> str:\n\"\"\"\nConstruct a prompt by combining the query with the retrieved context chunks.\nArgs:\nquery (str): The query text for which the prompt is being constructed.\ncontext_chunks (List[str]): A list of relevant context chunks to include in the prompt.\nReturns:\nstr: The constructed prompt to be used as input for the LLM.\n\"\"\"\n# Combine all context chunks into a single string, separated by newlines\ncontext = \"n\".join(context_chunks)\n# Define the system message to guide the LLM's behavior\nsystem_message = (\n\"You are a helpful assistant. Only use the provided context to answer the question. \"\n\"If the context doesn't contain the information needed, say 'I don't have enough information to answer this question.'\"\n)\n# Construct the final prompt by combining the system message, context, and query\nprompt = f\"System: {system_message}nnContext:n{context}nnQuestion:n{query}nnAnswer:\"\nreturn prompt",
    "relevance": null,
    "word_count": 226
  },
  {
    "id": "5b7d56129807cbc193ed66ea680bb3b0",
    "rank": 20,
    "doc_index": 0,
    "score": 0.20281291007995605,
    "percent_difference": 62.94,
    "text": "# Simple RAG with RL\n[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/release/python-370/) [![Nebius AI](https://img.shields.io/badge/Nebius%20AI-LLM-brightgreen)](https://cloud.nebius.ai/services/llm-embedding) [![OpenAI](https://img.shields.io/badge/OpenAI-API-lightgrey)](https://openai.com/) [![Medium](https://img.shields.io/badge/Medium-Blog-black?logo=medium)](https://medium.com/@fareedkhandev/maximizing-simple-rag-performance-using-rl-rewards-in-python-d4c14cbadf59)\nA simple RAG works in three simple steps:\n1. **Indexing**: Break documents into chunks and convert to vector embeddings.\n2. **Retrieval**: When a question is asked, find the most relevant chunks.\n3. **Generation**: Combine the question with retrieved chunks and let the AI generate an answer using this information.\nThe actual problem is to generate an answer to a given question using the provided documents. Simple RAG often fails to generate accurate answers due to the lack of context in the retrieved chunks. In this notebook, we will use the `RL RAG` approach to generate answers to the given questions using the provided documents.",
    "relevance": null,
    "word_count": 210
  },
  {
    "id": "7a0ce3895dc5287449873f858d8d8af0",
    "rank": 21,
    "doc_index": 36,
    "score": 0.18472763895988464,
    "percent_difference": 66.25,
    "text": "## Evaluating (RL vs Simple) RAG\nCode:\n# Print a message to indicate the start of the RAG pipeline\nprint(\" Running the Retrieval-Augmented Generation (RAG) pipeline...\")\nprint(f\" Query: {sample_query}n\")\n# Run the RAG pipeline and get the response\nresponse = basic_rag_pipeline(sample_query)\n# Print the response with better formatting\nprint(\" AI Response:\")\nprint(\"-\" * 50)\nprint(response.strip())\nprint(\"-\" * 50)\n# Print the ground truth answer for comparison\nprint(\" Ground Truth Answer:\")\nprint(\"-\" * 50)\nprint(expected_answer)\nprint(\"-\" * 50)",
    "relevance": null,
    "word_count": 138
  },
  {
    "id": "24c1212e737e31d4cb217d75f07e7d32",
    "rank": 22,
    "doc_index": 35,
    "score": 0.17274609208106995,
    "percent_difference": 68.44,
    "text": "## Evaluating (RL vs Simple) RAG\nAh, the moment of truth! Let's evaluate the performance of the simple RAG pipeline against the RL-enhanced RAG pipeline on our factual query, where the simple RAG previously failed to provide the correct answer. Let's see if the RL-enhanced RAG pipeline can perform better.\nLet's revisit our evaluation query and see what the simple RAG pipeline generates for it.\nYou can clearly see that the response generated by the RL-enhanced RAG model is more accurate and relevant compared to the simple RAG pipeline. The improvement in similarity to the ground truth is evident, indicating that the RL-enhanced model has learned to generate better responses through training.",
    "relevance": null,
    "word_count": 127
  },
  {
    "id": "c8e8e0e51c5f61c1f70e972043f8798c",
    "rank": 23,
    "doc_index": 32,
    "score": 0.1672145975753665,
    "percent_difference": 69.45,
    "text": "## Performance Comparison Logic\nCode:\n# Function to compare Simple RAG vs RL-Enhanced RAG\ndef compare_rag_approaches(query_text: str, ground_truth: str) -> Tuple[str, str, float, float]:\n\"\"\"\nCompare the outputs of simple RAG versus RL-enhanced RAG.\nArgs:\nquery_text (str): The input query text for the RAG pipeline.\nground_truth (str): The expected correct answer for the query.\nReturns:\nTuple[str, str, float, float]: A tuple containing:\n- simple_response (str): The response generated by the simple RAG pipeline.\n- best_rl_response (str): The best response generated by the RL-enhanced RAG pipeline.\n- simple_similarity (float): The similarity score of the simple RAG response to the ground truth.\n- rl_similarity (float): The similarity score of the RL-enhanced RAG response to the ground truth.\n\"\"\"\nprint(\"=\" * 80)\nprint(f\"Query: {query_text}\")\nprint(\"=\" * 80)\n# Step 1: Generate a response using the simple RAG pipeline\n# The basic RAG pipeline retrieves relevant chunks and generates a response without reinforcement learning.\nsimple_response: str = basic_rag_pipeline(query_text)\n# Calculate the similarity score between the simple RAG response and the ground truth.\nsimple_similarity: float = calculate_reward(simple_response, ground_truth)\nprint(\"nSimple RAG Output:\")\nprint(\"-\" * 40)\nprint(simple_response)\nprint(f\"Similarity to ground truth: {simple_similarity:.4f}\")\n# Step 2: Train the RL-enhanced RAG model\nprint(\"nTraining RL-enhanced RAG model...\")\n# Initialize training parameters (e.g., learning rate, number of episodes, discount factor).\nparams: Dict[str, float | int] = initialize_training_params()\n# Set the number of episodes to a smaller value for demonstration purposes.\nparams[\"num_episodes\"] = 5\n# Run the training loop for the RL-enhanced RAG model.\n# This loop trains the model to optimize its responses using reinforcement learning.\n_, rewards_history, actions_history, best_rl_response = training_loop(\nquery_text, ground_truth, params\n)\n# If no response was generated during training, generate one using the current query and context.\nif best_rl_response is None:\n# Retrieve relevant chunks for the query.\ncontext_chunks: List[str] = retrieve_relevant_chunks(query_text)\n# Construct a prompt using the query and retrieved context.\nprompt: str = construct_prompt(query_text, context_chunks)\n# Generate a response using the language model.\nbest_rl_response: str = generate_response(prompt)\n# Calculate the similarity score between the RL-enhanced RAG response and the ground truth.\nrl_similarity: float = calculate_reward(best_rl_response, ground_truth)\nprint(\"nRL-enhanced RAG Output:\")\nprint(\"-\" * 40)\nprint(best_rl_response)\nprint(f\"Similarity to ground truth: {rl_similarity:.4f}\")\n# Step 3: Evaluate and compare the results\n# Calculate the improvement in similarity score achieved by the RL-enhanced RAG model.\nimprovement: float = rl_similarity - simple_similarity\nprint(\"nEvaluation Results:\")\nprint(\"-\" * 40)\nprint(f\"Simple RAG similarity to ground truth: {simple_similarity:.4f}\")\nprint(f\"RL-enhanced RAG similarity to ground truth: {rl_similarity:.4f}\")\nprint(f\"Improvement: {improvement * 100:.2f}%\")\n# Step 4: Plot the reward history (if there are enough episodes and matplotlib is available)\nif len(rewards_history) > 1:\ntry:\nimport matplotlib.pyplot as plt\n# Create a plot to visualize the reward history during RL training.\nplt.figure(figsize=(10, 6))\nplt.plot(rewards_history)\nplt.title('Reward History During RL Training')\nplt.xlabel('Episode')\nplt.ylabel('Reward')\nplt.grid(True)\nplt.show()\nexcept ImportError:\n# If matplotlib is not available, print a message instead of plotting.\nprint(\"Matplotlib not available for plotting rewards\")\n# Return the results: responses and similarity scores for both approaches.\nreturn simple_response, best_rl_response, simple_similarity, rl_similarity",
    "relevance": null,
    "word_count": 783
  },
  {
    "id": "6411988107e7fdbb428b40af0bd3c4f0",
    "rank": 24,
    "doc_index": 38,
    "score": 0.16694650053977966,
    "percent_difference": 69.5,
    "text": "## Saving the Comparison Results\nCode:\n# Save the results for later comparison\nresults = {\n\"query\": query_text,  # The input query text\n\"ground_truth\": expected_answer,  # The expected correct answer for the query\n\"simple_rag\": {\n\"response\": simple_response,  # The response generated by the simple RAG pipeline\n\"similarity\": float(simple_sim)  # The similarity score of the simple RAG response to the ground truth\n},\n\"rl_rag\": {\n\"response\": rl_response,  # The response generated by the RL-enhanced RAG pipeline\n\"similarity\": float(rl_sim)  # The similarity score of the RL-enhanced RAG response to the ground truth\n},\n\"improvement\": float(rl_sim - simple_sim)  # The improvement in similarity score achieved by RL-enhanced RAG\n}\n# Save the results to a JSON file for future reference\nwith open('rl_rag_results.json', 'w') as f:\njson.dump(results, f, indent=2)  # Write the results dictionary to the file with indentation for readability\n# Print a confirmation message to indicate that the results have been saved\nprint(\"nResults saved to rl_rag_results.json\")",
    "relevance": null,
    "word_count": 216
  },
  {
    "id": "abaee00dcbb3409b200dd509aede7f32",
    "rank": 25,
    "doc_index": 19,
    "score": 0.16236203350126743,
    "percent_difference": 70.33,
    "text": "## State, Action Space, and Reward Methodology\nThe very first step when coding an RL algorithm is to define three things:\n- **State**: It is the current situation of the environment. In our case, the initial state is our simple RAG pipeline (query, context, response).\n- **Action Space**: It is the decision that the agent takes based on the state. In our case, the actions can include changing the model, modifying the context, altering the query, etc.\n- **Reward**: It is the feedback that the agent receives after taking an action. In our case, the reward can be the similarity between the generated response and the ground truth answer.\nOur state will be changing constantly as we perform training. For that, we need to save the state after each `training episode` so that our RL agent can learn from it and avoid making the same mistakes again.\nWe have defined the state representation for the RL agent, including the user query, retrieved context chunks, rewritten query (if any), and histories of responses and rewards. This state will guide the agent in generating better responses.\nNext we need to define the action space for the reinforcement learning agent. The action space consists of the set of possible actions that the agent can take at each step. In this case, we define four actions:\n- `rewrite_query`: Reformulate the original query to improve retrieval\n- `expand_context`: Retrieve additional context chunks\n- `filter_context`: Remove irrelevant context chunks\n- `generate_response`: Generate a response based on the current query and context\nObviously, when our RL agent takes an action, it will be based on the current state and the action space. It will be rewarded based on the quality of the response generated by the RAG pipeline. The reward function will be based on the cosine similarity between the generated response and the ground truth answer.\nOur goal is to maximize the reward by generating responses that are similar to the ground truth answer. Higher reward values indicate that the generated response is more aligned with the expected answer.",
    "relevance": null,
    "word_count": 414
  },
  {
    "id": "1642ac6c38c9449becabd9110efbbfed",
    "rank": 26,
    "doc_index": 31,
    "score": 0.1551719307899475,
    "percent_difference": 71.65,
    "text": "## Performance Comparison Logic\nAlthough we can manually compare the simple RAG pipeline with the RL-based RAG pipeline, a function can definitely help us in this regard. So, let's define a function to compare the performance of the simple RAG pipeline with the RL-enhanced RAG pipeline.\nSo our performance comparison logic is not very complicated but is based on 4 steps:\n1. Generate a response using the simple RAG pipeline.\n2. Train the RL-enhanced RAG model using the training loop.\n3. Evaluate and compare the results.\n4. Plot the reward history (if available).",
    "relevance": null,
    "word_count": 110
  },
  {
    "id": "b03fbbc72feac4414a8c61386b1a2adb",
    "rank": 27,
    "doc_index": 33,
    "score": 0.15384986251592636,
    "percent_difference": 71.89,
    "text": "## Evaluation Framework (**Optional**)\nThis step is optional but in case you want to evaluate all the eval queries in the validation data, you can use the following code.\nFirst, to check the relevance of the retrieved chunks and the ground truth, we need to have a function that evaluates the relevance of the retrieved chunks.\nTo evaluate the accuracy of the generated responses, we can use the cosine similarity between the embeddings of the generated responses and the ground truth. So let's define a function to evaluate the accuracy of the responses based on this similarity metric.\nWe also need to measure the response quality and assign a relevant score for it to be used in the reinforcement learning process.\nThen we can evaluate the performance of the RL-enhanced RAG model on the validation dataset:",
    "relevance": null,
    "word_count": 154
  },
  {
    "id": "97eb7f30caead8fc0e0165f2c9e32978",
    "rank": 28,
    "doc_index": 20,
    "score": 0.15212557961543402,
    "percent_difference": 72.2,
    "text": "## State, Action Space, and Reward Methodology\nCode:\n# Function to define the state representation for reinforcement learning\ndef define_state(\nquery: str,\ncontext_chunks: List[str],\nrewritten_query: str = None,\nprevious_responses: List[str] = None,\nprevious_rewards: List[float] = None\n) -> dict:\n\"\"\"\nDefine the state representation for the reinforcement learning agent.\nArgs:\nquery (str): The original user query.\ncontext_chunks (List[str]): Retrieved context chunks from the knowledge base.\nrewritten_query (str, optional): A reformulated version of the original query.\nprevious_responses (List[str], optional): List of previously generated responses.\nprevious_rewards (List[float], optional): List of rewards received for previous actions.\nReturns:\ndict: A dictionary representing the current state with all relevant information.\n\"\"\"\nstate = {\n\"original_query\": query,                                    # The initial query from the user\n\"current_query\": rewritten_query if rewritten_query else query,  # Current version of the query (may be rewritten)\n\"context\": context_chunks,                                 # Retrieved context chunks from the knowledge base\n\"previous_responses\": previous_responses if previous_responses else [],  # History of generated responses\n\"previous_rewards\": previous_rewards if previous_rewards else []         # History of received rewards\n}\nreturn state",
    "relevance": null,
    "word_count": 256
  },
  {
    "id": "48649d4522008516cd1ab2c1ff25cd49",
    "rank": 29,
    "doc_index": 5,
    "score": 0.14950576424598694,
    "percent_difference": 72.68,
    "text": "## Data Preprocessing\nCode:\n# Function to load documents from a directory\ndef load_documents(directory_path: str) -> List[str]:\n\"\"\"\nLoad all text documents from the specified directory.\nArgs:\ndirectory_path (str): Path to the directory containing text files.\nReturns:\nList[str]: A list of strings, where each string is the content of a text file.\n\"\"\"\ndocuments = []  # Initialize an empty list to store document contents\nfor filename in os.listdir(directory_path):  # Iterate through all files in the directory\nif filename.endswith(\".txt\"):  # Check if the file has a .txt extension\n# Open the file in read mode with UTF-8 encoding and append its content to the list\nwith open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\ndocuments.append(file.read())\nreturn documents  # Return the list of document contents",
    "relevance": null,
    "word_count": 178
  },
  {
    "id": "08dab767ee17491db1997c79996255c0",
    "rank": 30,
    "doc_index": 1,
    "score": 0.143266923725605,
    "percent_difference": 73.82,
    "text": "# Table of Contents\n- [Setting Up the Environment](#setting-up-the-environment)\n- [Data Preprocessing](#data-preprocessing)\n- [Document Embedding Generation](#document-embedding-generation)\n- [Vector Store Implementation](#vector-store-implementation)\n- [Simple Retrieval Implementation](#simple-retrieval-implementation)\n- [Cosine Similarity](#cosine-similarity)\n- [Similarity Search](#similarity-search)\n- [LLM Response Generation](#llm-response-generation)\n- [Basic RAG Pipeline](#basic-rag-pipeline)\n- [Evaluation of Basic RAG](#evaluate-the-basic-rag-pipeline)\n- [Reinforcement Learning for RAG](#reinforcement-learning-for-rag)\n- [State, Action Space, and Reward Methodology](#state-action-space-and-reward-methodology)\n- [Policy Network](#policy-network)\n- [Single RL Step](#single-rl-step)\n- [Training Parameters and Policy Update](#training-parameters-and-policy-update)\n- [Training Loop](#training-loop)\n- [Performance Comparison Logic](#performance-comparison-logic)\n- [Evaluation Framework](#evaluation-framework)\n- [Evaluating RL vs Simple RAG](#evaluating-rl-vs-simple-rag)\n- [Saving Comparison Results](#saving-the-comparison-results)\n- [Conclusion](#what-can-we-conclude)",
    "relevance": null,
    "word_count": 218
  },
  {
    "id": "6450bd7c014c53e1945e8e2cd5f8d370",
    "rank": 31,
    "doc_index": 28,
    "score": 0.11605328321456909,
    "percent_difference": 78.8,
    "text": "## Training Parameters and Policy Update\nCode:\n# Function to initialize training parameters\ndef initialize_training_params() -> Dict[str, Union[float, int]]:\n\"\"\"\nInitialize training parameters such as learning rate, number of episodes, and discount factor.\nReturns:\nDict[str, Union[float, int]]: A dictionary containing the initialized training parameters.\n\"\"\"\nparams = {\n\"learning_rate\": 0.01,  # Learning rate for policy updates\n\"num_episodes\": 100,   # Total number of training episodes\n\"discount_factor\": 0.99  # Discount factor for future rewards\n}\nreturn params",
    "relevance": null,
    "word_count": 117
  },
  {
    "id": "4b84ab124a77f6f38866cbe8978445f9",
    "rank": 32,
    "doc_index": 18,
    "score": 0.10930126160383224,
    "percent_difference": 80.03,
    "text": "## Reinforcement Learning for RAG\nReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, the agent is not explicitly told which actions to take, but instead must discover which actions yield the most reward through trial and error.\nFollow are the main components of a reinforcement learning system:\n1. **Agent**: The learner or decision-maker\n2. **Environment**: The world with which the agent interacts\n3. **State (S)**: The current situation of the agent in the environment\n4. **Action (A)**: A set of possible moves the agent can make\n5. **Reward (R)**: Feedback from the environment after each action\n6. **Policy (p)**: Strategy that the agent follows to determine the next action\nThe goal in reinforcement learning is to learn a policy p that maximizes the expected cumulative reward:\n$$pi^* = argmax_pi mathbb{E}left[ sum_{t=0}^{T} gamma^t R_t right]$$\nWhere:\n- $pi^*$ is the optimal policy\n- $gamma$ is the discount factor (0 <= g <= 1)\n- $R_t$ is the reward at time step t\n- $T$ is the final time step\nIn the context of RAG systems, reinforcement learning can be used to:\n- Improve retrieval by learning which documents are most helpful\n- Refine prompt construction based on user feedback\n- Optimize the generation process by learning from successful responses",
    "relevance": null,
    "word_count": 319
  },
  {
    "id": "c90f59d2547026d00223372fde566dab",
    "rank": 33,
    "doc_index": 37,
    "score": 0.10767406225204468,
    "percent_difference": 80.33,
    "text": "## Saving the Comparison Results\nAfter implementing the RL algorithm, we can save the comparison results to check the performance of the RL implementation later.",
    "relevance": null,
    "word_count": 28
  },
  {
    "id": "b4166b4c3df5c5aba214836986bcecfd",
    "rank": 34,
    "doc_index": 25,
    "score": 0.10377289354801178,
    "percent_difference": 81.04,
    "text": "## Single RL Step\nWe have coded an important component of the RL pipeline. For any developer who has done any kind of training, there exists a training loop where each iteration is a single step in which the RL agent takes an action, rewards are calculated, states are updated, and so on. So, we need to code a single step of our training loop. Let's do that.\nIn our single step function, we first select an action using the policy network. The policy network uses an epsilon-greedy strategy to balance exploration and exploitation. If the random number is less than epsilon, we choose a random action from the action space for exploration. Otherwise, we select the best action based on the current state using a simple heuristic.",
    "relevance": null,
    "word_count": 146
  },
  {
    "id": "4e0cc01480ac77e8c07e378744ab984e",
    "rank": 35,
    "doc_index": 26,
    "score": 0.10367349411050479,
    "percent_difference": 81.06,
    "text": "## Single RL Step\nCode:\n# Function to perform a single RL step\ndef rl_step(\nstate: dict,\naction_space: List[str],\nground_truth: str\n) -> tuple[dict, str, float, str]:\n\"\"\"\nPerform a single RL step: select an action, execute it, and calculate the reward.\nArgs:\nstate (dict): The current state of the environment, including query, context, responses, and rewards.\naction_space (List[str]): The list of possible actions the agent can take.\nground_truth (str): The expected correct answer to calculate the reward.\nReturns:\ntuple: A tuple containing:\n- state (dict): The updated state after executing the action.\n- action (str): The action selected by the policy network.\n- reward (float): The reward received for the action.\n- response (str): The response generated (if applicable).\n\"\"\"\n# Select an action using the policy network\naction: str = policy_network(state, action_space)\nresponse: str = None  # Initialize response as None\nreward: float = 0  # Initialize reward as 0\n# Execute the selected action\nif action == \"rewrite_query\":\n# Rewrite the query to improve retrieval\nrewritten_query: str = rewrite_query(state[\"original_query\"], state[\"context\"])\nstate[\"current_query\"] = rewritten_query  # Update the current query in the state\n# Retrieve new context based on the rewritten query\nnew_context: List[str] = retrieve_relevant_chunks(rewritten_query)\nstate[\"context\"] = new_context  # Update the context in the state\nelif action == \"expand_context\":\n# Expand the context by retrieving additional chunks\nexpanded_context: List[str] = expand_context(state[\"current_query\"], state[\"context\"])\nstate[\"context\"] = expanded_context  # Update the context in the state\nelif action == \"filter_context\":\n# Filter the context to keep only the most relevant chunks\nfiltered_context: List[str] = filter_context(state[\"current_query\"], state[\"context\"])\nstate[\"context\"] = filtered_context  # Update the context in the state\nelif action == \"generate_response\":\n# Construct a prompt using the current query and context\nprompt: str = construct_prompt(state[\"current_query\"], state[\"context\"])\n# Generate a response using the LLM\nresponse: str = generate_response(prompt)\n# Calculate the reward based on the similarity between the response and the ground truth\nreward: float = calculate_reward(response, ground_truth)\n# Update the state with the new response and reward\nstate[\"previous_responses\"].append(response)\nstate[\"previous_rewards\"].append(reward)\n# Return the updated state, selected action, reward, and response\nreturn state, action, reward, response",
    "relevance": null,
    "word_count": 554
  },
  {
    "id": "516fc4be5cd1c29eeab957127c7e8b9a",
    "rank": 36,
    "doc_index": 3,
    "score": 0.09812755882740021,
    "percent_difference": 82.07,
    "text": "## Setting Up the Environment\nCode:\n# Importing the os module for interacting with the operating system\nimport os\n# Importing the OpenAI module for working with OpenAI's API\nfrom openai import OpenAI\n# Importing numpy for numerical operations\nimport numpy as np\n# Importing json for working with JSON data\nimport json\n# Typing module for type hints\nfrom typing import Dict, List, Tuple, Optional, Union",
    "relevance": null,
    "word_count": 74
  },
  {
    "id": "db5b35bdae3c0c3dc058ae6fbe546d97",
    "rank": 37,
    "doc_index": 24,
    "score": 0.08980123574535052,
    "percent_difference": 83.59,
    "text": "## Policy Network\nCode:\n# Function to define a policy network to select an action based on the state\ndef policy_network(\nstate: dict,\naction_space: List[str],\nepsilon: float = 0.2\n) -> str:\n\"\"\"\nDefine a policy network to select an action based on the current state using an epsilon-greedy strategy.\nArgs:\nstate (dict): The current state of the environment, including query, context, responses, and rewards.\naction_space (List[str]): The list of possible actions the agent can take.\nepsilon (float): The probability of choosing a random action for exploration. Default is 0.2.\nReturns:\nstr: The selected action from the action space.\n\"\"\"\n# Use epsilon-greedy strategy: random exploration vs. exploitation\nif np.random.random() < epsilon:\n# Exploration: randomly select an action from the action space\naction = np.random.choice(action_space)\nelse:\n# Exploitation: select the best action based on the current state using a simple heuristic\n# If there are no previous responses, prioritize rewriting the query\nif len(state[\"previous_responses\"]) == 0:\naction = \"rewrite_query\"\n# If there are previous responses but the rewards are low, try expanding the context\nelif state[\"previous_rewards\"] and max(state[\"previous_rewards\"]) < 0.7:\naction = \"expand_context\"\n# If the context has too many chunks, try filtering the context\nelif len(state[\"context\"]) > 5:\naction = \"filter_context\"\n# Otherwise, generate a response\nelse:\naction = \"generate_response\"\nreturn action",
    "relevance": null,
    "word_count": 309
  },
  {
    "id": "205a6d9a86a3db561d89a7702f6c75ab",
    "rank": 38,
    "doc_index": 27,
    "score": 0.08535144850611687,
    "percent_difference": 84.4,
    "text": "## Training Parameters and Policy Update\nWe need to define some training parameters for our training loop and also define a function to update the policy based on the rewards received.\nThough the training parameters function is **optional**, it can be used for advanced implementations of the RL pipeline.\nSimilar to how our state changes after each step in the RL process, the policy also needs to be updated based on the rewards received. The update_policy function takes the current policy, state, action, reward, and learning rate as input and returns the updated policy.\nIn the above `update_policy` logic, we store the action taken and the reward received for each query in the policy dictionary. In a more advanced RL algorithm, the policy update would involve more sophisticated methods such as policy gradients or Q-learning.\nFinally, we need to implement progress tracking logic to monitor the training process. This will help us understand how the model is learning and improving over time.",
    "relevance": null,
    "word_count": 186
  },
  {
    "id": "3be5abd430283bfe60c06898c01489f2",
    "rank": 39,
    "doc_index": 11,
    "score": 0.020593097433447838,
    "percent_difference": 96.24,
    "text": "## Simple Retrieval Implementation\nCode:\n# Function to compute cosine similarity between two vectors\ndef cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n\"\"\"\nCompute the cosine similarity between two vectors.\nArgs:\nvec1 (np.ndarray): The first vector.\nvec2 (np.ndarray): The second vector.\nReturns:\nfloat: The cosine similarity between the two vectors, ranging from -1 to 1.\n\"\"\"\n# Compute the dot product of the two vectors\ndot_product = np.dot(vec1, vec2)\n# Compute the magnitude (norm) of the first vector\nnorm_vec1 = np.linalg.norm(vec1)\n# Compute the magnitude (norm) of the second vector\nnorm_vec2 = np.linalg.norm(vec2)\n# Return the cosine similarity as the ratio of the dot product to the product of the norms\nreturn dot_product / (norm_vec1 * norm_vec2)",
    "relevance": null,
    "word_count": 159
  },
  {
    "id": "c0b14e1cd967cd67390081ac29a1f6a9",
    "rank": 40,
    "doc_index": 2,
    "score": 0.003752522636204958,
    "percent_difference": 99.31,
    "text": "## Setting Up the Environment\nFirst, we need to import the necessary libraries and set up the environment. We will be using HuggingFace Models hosted under **Nebius** platform. Obviously, you can use your own models as long as they are compatible with OpenAI's API.\nNext, we need to initialize the client responsible for response and embedding generation.",
    "relevance": null,
    "word_count": 70
  }
]