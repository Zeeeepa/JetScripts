[
  "## Document Processing Pipeline\nCode:\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n\"\"\"\nProcess a document for fusion retrieval.\nArgs:\npdf_path (str): Path to the PDF file\nchunk_size (int): Size of each chunk in characters\nchunk_overlap (int): Overlap between chunks in characters\nReturns:\nTuple[List[Dict], SimpleVectorStore, BM25Okapi]: Chunks, vector store, and BM25 index\n\"\"\"\n# Extract text from the PDF file\ntext = extract_text_from_pdf(pdf_path)\n# Clean the extracted text to remove extra whitespace and special characters\ncleaned_text = clean_text(text)\n# Split the cleaned text into overlapping chunks\nchunks = chunk_text(cleaned_text, chunk_size, chunk_overlap)\n# Extract the text content from each chunk for embedding creation\nchunk_texts = [chunk[\"text\"] for chunk in chunks]\nprint(\"Creating embeddings for chunks...\")\n# Create embeddings for the chunk texts\nembeddings = create_embeddings(chunk_texts)\n# Initialize the vector store\nvector_store = SimpleVectorStore()\n# Add the chunks and their embeddings to the vector store\nvector_store.add_items(chunks, embeddings)\nprint(f\"Added {len(chunks)} items to vector store\")\n# Create a BM25 index from the chunks\nbm25_index = create_bm25_index(chunks)",
  "# Return the chunks, vector store, and BM25 index\nreturn chunks, vector_store, bm25_index\n## BM25 Implementation\nCode:\ndef create_bm25_index(chunks):\n\"\"\"\nCreate a BM25 index from the given chunks.\nArgs:\nchunks (List[Dict]): List of text chunks\nReturns:\nBM25Okapi: A BM25 index\n\"\"\"\n# Extract text from each chunk\ntexts = [chunk[\"text\"] for chunk in chunks]\n# Tokenize each document by splitting on whitespace\ntokenized_docs = [text.split() for text in texts]\n# Create the BM25 index using the tokenized documents\nbm25 = BM25Okapi(tokenized_docs)\n# Print the number of documents in the BM25 index\nprint(f\"Created BM25 index with {len(texts)} documents\")\nreturn bm25",
  "## Main Retrieval Function\nCode:\ndef answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n\"\"\"\nAnswer a query using fusion RAG.\nArgs:\nquery (str): User query\nchunks (List[Dict]): Text chunks\nvector_store (SimpleVectorStore): Vector store\nbm25_index (BM25Okapi): BM25 index\nk (int): Number of documents to retrieve\nalpha (float): Weight for vector scores\nReturns:\nDict: Query results including retrieved documents and response\n\"\"\"\n# Retrieve documents using fusion retrieval method\nretrieved_docs = fusion_retrieval(query, chunks, vector_store, bm25_index, k=k, alpha=alpha)\n# Format the context from the retrieved documents by joining their text with separators\ncontext = \"nn---nn\".join([doc[\"text\"] for doc in retrieved_docs])\n# Generate a response based on the query and the formatted context\nresponse = generate_response(query, context)\n# Return the query, retrieved documents, and the generated response\nreturn {\n\"query\": query,\n\"retrieved_documents\": retrieved_docs,\n\"response\": response\n}",
  "## Creating Our Vector Store\nCode:\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreate embeddings for the given texts.\nArgs:\ntexts (str or List[str]): Input text(s)\nmodel (str): Embedding model name\nReturns:\nList[List[float]]: Embedding vectors\n\"\"\"\n# Handle both string and list inputs\ninput_texts = texts if isinstance(texts, list) else [texts]\n# Process in batches if needed (OpenAI API limits)\nbatch_size = 100\nall_embeddings = []\n# Iterate over the input texts in batches\nfor i in range(0, len(input_texts), batch_size):\nbatch = input_texts[i:i + batch_size]  # Get the current batch of texts\n# Create embeddings for the current batch\nresponse = client.embeddings.create(\nmodel=model,\ninput=batch\n)\n# Extract embeddings from the response\nbatch_embeddings = [item.embedding for item in response.data]\nall_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n# If input was a string, return just the first embedding\nif isinstance(texts, str):\nreturn all_embeddings[0]\n# Otherwise return all embeddings\nreturn all_embeddings",
  "## Document Processing Functions\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtract text content from a PDF file.\nArgs:\npdf_path (str): Path to the PDF file\nReturns:\nstr: Extracted text content\n\"\"\"\nprint(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\npdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\ntext = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(pdf_document.page_count):\npage = pdf_document[page_num]  # Get the page object\ntext += page.get_text()  # Extract text from the page and append to the text string\nreturn text  # Return the extracted text content\n## Complete Evaluation Pipeline\nCode:\ndef evaluate_fusion_retrieval(pdf_path, test_queries, reference_answers=None, k=5, alpha=0.5):\n\"\"\"\nEvaluate fusion retrieval compared to other methods.\nArgs:\npdf_path (str): Path to the PDF file\ntest_queries (List[str]): List of test queries\nreference_answers (List[str], optional): Reference answers\nk (int): Number of documents to retrieve\nalpha (float): Weight for vector scores in fusion retrieval\nReturns:\nDict: Evaluation results\n\"\"\"\nprint(\"=== EVALUATING FUSION RETRIEVAL ===n\")",
  "# Process the document to extract text, create chunks, and build vector and BM25 indices\nchunks, vector_store, bm25_index = process_document(pdf_path)\n# Initialize a list to store results for each query\nresults = []\n# Iterate over each test query\nfor i, query in enumerate(test_queries):\nprint(f\"nn=== Evaluating Query {i+1}/{len(test_queries)} ===\")\nprint(f\"Query: {query}\")\n# Get the reference answer if available\nreference = None\nif reference_answers and i < len(reference_answers):\nreference = reference_answers[i]\n# Compare retrieval methods for the current query\ncomparison = compare_retrieval_methods(\nquery,\nchunks,\nvector_store,\nbm25_index,\nk=k,\nalpha=alpha,\nreference_answer=reference\n)\n# Append the comparison results to the results list\nresults.append(comparison)\n# Print the responses from different retrieval methods\nprint(\"n=== Vector-based Response ===\")\nprint(comparison[\"vector_result\"][\"response\"])\nprint(\"n=== BM25 Response ===\")\nprint(comparison[\"bm25_result\"][\"response\"])\nprint(\"n=== Fusion Response ===\")\nprint(comparison[\"fusion_result\"][\"response\"])\nprint(\"n=== Comparison ===\")\nprint(comparison[\"comparison\"])\n# Generate an overall analysis of the fusion retrieval performance\noverall_analysis = generate_overall_analysis(results)\n# Return the results and overall analysis\nreturn {\n\"results\": results,\n\"overall_analysis\": overall_analysis\n}",
  "## Evaluation Functions\nCode:\ndef compare_retrieval_methods(query, chunks, vector_store, bm25_index, k=5, alpha=0.5, reference_answer=None):\n\"\"\"\nCompare different retrieval methods for a query.\nArgs:\nquery (str): User query\nchunks (List[Dict]): Text chunks\nvector_store (SimpleVectorStore): Vector store\nbm25_index (BM25Okapi): BM25 index\nk (int): Number of documents to retrieve\nalpha (float): Weight for vector scores in fusion retrieval\nreference_answer (str, optional): Reference answer for comparison\nReturns:\nDict: Comparison results\n\"\"\"\nprint(f\"n=== Comparing retrieval methods for query: {query} ===n\")\n# Run vector-only RAG\nprint(\"nRunning vector-only RAG...\")\nvector_result = vector_only_rag(query, vector_store, k)\n# Run BM25-only RAG\nprint(\"nRunning BM25-only RAG...\")\nbm25_result = bm25_only_rag(query, chunks, bm25_index, k)\n# Run fusion RAG\nprint(\"nRunning fusion RAG...\")\nfusion_result = answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k, alpha)",
  "# Compare responses from different retrieval methods\nprint(\"nComparing responses...\")\ncomparison = evaluate_responses(\nquery,\nvector_result[\"response\"],\nbm25_result[\"response\"],\nfusion_result[\"response\"],\nreference_answer\n)\n# Return the comparison results\nreturn {\n\"query\": query,\n\"vector_result\": vector_result,\n\"bm25_result\": bm25_result,\n\"fusion_result\": fusion_result,\n\"comparison\": comparison\n}\n## Fusion Retrieval Function\nCode:\ndef fusion_retrieval(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n\"\"\"\nPerform fusion retrieval combining vector-based and BM25 search.\nArgs:\nquery (str): Query string\nchunks (List[Dict]): Original text chunks\nvector_store (SimpleVectorStore): Vector store\nbm25_index (BM25Okapi): BM25 index\nk (int): Number of results to return\nalpha (float): Weight for vector scores (0-1), where 1-alpha is BM25 weight\nReturns:\nList[Dict]: Top k results based on combined scores\n\"\"\"\nprint(f\"Performing fusion retrieval for query: {query}\")\n# Define small epsilon to avoid division by zero\nepsilon = 1e-8",
  "# Get vector search results\nquery_embedding = create_embeddings(query)  # Create embedding for the query\nvector_results = vector_store.similarity_search_with_scores(query_embedding, k=len(chunks))  # Perform vector search\n# Get BM25 search results\nbm25_results = bm25_search(bm25_index, chunks, query, k=len(chunks))  # Perform BM25 search\n# Create dictionaries to map document index to score\nvector_scores_dict = {result[\"metadata\"][\"index\"]: result[\"similarity\"] for result in vector_results}\nbm25_scores_dict = {result[\"metadata\"][\"index\"]: result[\"bm25_score\"] for result in bm25_results}\n# Ensure all documents have scores for both methods\nall_docs = vector_store.get_all_documents()\ncombined_results = []\nfor i, doc in enumerate(all_docs):\nvector_score = vector_scores_dict.get(i, 0.0)  # Get vector score or 0 if not found\nbm25_score = bm25_scores_dict.get(i, 0.0)  # Get BM25 score or 0 if not found\ncombined_results.append({\n\"text\": doc[\"text\"],\n\"metadata\": doc[\"metadata\"],\n\"vector_score\": vector_score,\n\"bm25_score\": bm25_score,\n\"index\": i\n})",
  "# Extract scores as arrays\nvector_scores = np.array([doc[\"vector_score\"] for doc in combined_results])\nbm25_scores = np.array([doc[\"bm25_score\"] for doc in combined_results])\n# Normalize scores\nnorm_vector_scores = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\nnorm_bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n# Compute combined scores\ncombined_scores = alpha * norm_vector_scores + (1 - alpha) * norm_bm25_scores\n# Add combined scores to results\nfor i, score in enumerate(combined_scores):\ncombined_results[i][\"combined_score\"] = float(score)\n# Sort by combined score (descending)\ncombined_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n# Return top k results\ntop_results = combined_results[:k]\nprint(f\"Retrieved {len(top_results)} documents with fusion retrieval\")\nreturn top_results\n## Comparing Retrieval Methods\nCode:\ndef vector_only_rag(query, vector_store, k=5):\n\"\"\"\nAnswer a query using only vector-based RAG.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store\nk (int): Number of documents to retrieve\nReturns:\nDict: Query results\n\"\"\"",
  "# Create query embedding\nquery_embedding = create_embeddings(query)\n# Retrieve documents using vector-based similarity search\nretrieved_docs = vector_store.similarity_search_with_scores(query_embedding, k=k)\n# Format the context from the retrieved documents by joining their text with separators\ncontext = \"nn---nn\".join([doc[\"text\"] for doc in retrieved_docs])\n# Generate a response based on the query and the formatted context\nresponse = generate_response(query, context)\n# Return the query, retrieved documents, and the generated response\nreturn {\n\"query\": query,\n\"retrieved_documents\": retrieved_docs,\n\"response\": response\n}\n# Fusion Retrieval: Combining Vector and Keyword Search\nIn this notebook, I implement a fusion retrieval system that combines the strengths of semantic vector search with keyword-based BM25 retrieval. This approach improves retrieval quality by capturing both conceptual similarity and exact keyword matches.\n## Why Fusion Retrieval Matters\nTraditional RAG systems typically rely on vector search alone, but this has limitations:\n- Vector search excels at semantic similarity but may miss exact keyword matches\n- Keyword search is great for specific terms but lacks semantic understanding\n- Different queries perform better with different retrieval methods\nFusion retrieval gives us the best of both worlds by:\n- Performing both vector-based and keyword-based retrieval\n- Normalizing the scores from each approach\n- Combining them with a weighted formula\n- Ranking documents based on the combined score",
  "## Setting Up the Environment\nCode:\nimport os\nimport numpy as np\nfrom rank_bm25 import BM25Okapi\nimport fitz\nfrom openai import OpenAI\nimport re\nimport json\nimport time\nfrom sklearn.metrics.pairwise import cosine_similarity\n## Evaluating Fusion Retrieval\nCode:\n# Path to PDF document\n# Path to PDF document containing AI information for knowledge retrieval testing\npdf_path = \"data/AI_Information.pdf\"\n# Define a single AI-related test query\ntest_queries = [\n\"What are the main applications of transformer models in natural language processing?\"  # AI-specific query\n]\n# Optional reference answer\nreference_answers = [\n\"Transformer models have revolutionized natural language processing with applications including machine translation, text summarization, question answering, sentiment analysis, and text generation. They excel at capturing long-range dependencies in text and have become the foundation for models like BERT, GPT, and T5.\",\n]\n# Set parameters\nk = 5  # Number of documents to retrieve\nalpha = 0.5  # Weight for vector scores (0.5 means equal weight between vector and BM25)\n# Run evaluation\nevaluation_results = evaluate_fusion_retrieval(\npdf_path=pdf_path,\ntest_queries=test_queries,\nreference_answers=reference_answers,\nk=k,\nalpha=alpha\n)\n# Print overall analysis\nprint(\"nn=== OVERALL ANALYSIS ===n\")\nprint(evaluation_results[\"overall_analysis\"])",
  "## Response Generation\nCode:\ndef generate_response(query, context):\n\"\"\"\nGenerate a response based on the query and context.\nArgs:\nquery (str): User query\ncontext (str): Context from retrieved documents\nReturns:\nstr: Generated response\n\"\"\"\n# Define the system prompt to guide the AI assistant\nsystem_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context.\nIf the context doesn't contain relevant information to answer the question fully, acknowledge this limitation.\"\"\"\n# Format the user prompt with the context and query\nuser_prompt = f\"\"\"Context:\n{context}\nQuestion: {query}\nPlease answer the question based on the provided context.\"\"\"\n# Generate the response using the OpenAI API\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n{\"role\": \"user\", \"content\": user_prompt}  # User message with context and query\n],\ntemperature=0.1  # Set the temperature for response generation\n)\n# Return the generated response\nreturn response.choices[0].message.content\n## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.\n## Setting Up the Environment\nWe begin by importing necessary libraries."
]