[
  {
    "id": "944f25eb13eebbf8771cfd0f23c3a04f",
    "rank": 1,
    "doc_index": 5,
    "score": 0.6079847812652588,
    "percent_difference": 0.0,
    "text": "## Chunking the Extracted Text\nOnce we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy.",
    "relevance": null,
    "word_count": 26
  },
  {
    "id": "d20123565d5fa6f9dce11a5e541cf11d",
    "rank": 2,
    "doc_index": 9,
    "score": 0.529926061630249,
    "percent_difference": 12.84,
    "text": "## Extracting and Chunking Text from a PDF File\nNow, we load the PDF, extract text, and split it into chunks.",
    "relevance": null,
    "word_count": 26
  },
  {
    "id": "9b27730e7633bcc4e44da8b067ac906d",
    "rank": 3,
    "doc_index": 10,
    "score": 0.5252268314361572,
    "percent_difference": 13.61,
    "text": "## Extracting and Chunking Text from a PDF File\nCode:\n# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\ntext_chunks = chunk_text(extracted_text, 1000, 200)\n# Print the number of text chunks created\nprint(\"Number of text chunks:\", len(text_chunks))\n# Print the first text chunk\nprint(\"nFirst text chunk:\")\nprint(text_chunks[0])",
    "relevance": null,
    "word_count": 109
  },
  {
    "id": "fc525f4d8d7a82a57f0948613616fe75",
    "rank": 4,
    "doc_index": 6,
    "score": 0.4944360554218292,
    "percent_difference": 18.68,
    "text": "## Chunking the Extracted Text\nCode:\ndef chunk_text(text, n, overlap):\n\"\"\"\nChunks the given text into segments of n characters with overlap.\nArgs:\ntext (str): The text to be chunked.\nn (int): The number of characters in each chunk.\noverlap (int): The number of overlapping characters between chunks.\nReturns:\nList[str]: A list of text chunks.\n\"\"\"\nchunks = []  # Initialize an empty list to store the chunks\n# Loop through the text with a step size of (n - overlap)\nfor i in range(0, len(text), n - overlap):\n# Append a chunk of text from index i to i + n to the chunks list\nchunks.append(text[i:i + n])\nreturn chunks  # Return the list of text chunks",
    "relevance": null,
    "word_count": 169
  },
  {
    "id": "bf1908f40821643368b5d66d527ece34",
    "rank": 5,
    "doc_index": 15,
    "score": 0.44358089566230774,
    "percent_difference": 27.04,
    "text": "## Running a Query on Extracted Chunks\nCode:\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\ndata = json.load(f)\n# Extract the first query from the validation data\nquery = data[0]['question']\n# Perform semantic search to find the top 2 most relevant text chunks for the query\ntop_chunks = semantic_search(query, text_chunks, response.data, k=2)\n# Print the query\nprint(\"Query:\", query)\n# Print the top 2 most relevant text chunks\nfor i, chunk in enumerate(top_chunks):\nprint(f\"Context {i + 1}:n{chunk}n=====================================\")",
    "relevance": null,
    "word_count": 131
  },
  {
    "id": "d9cdea86cc3822c5e3ac1b315f8102b2",
    "rank": 6,
    "doc_index": 11,
    "score": 0.4374611973762512,
    "percent_difference": 28.05,
    "text": "## Creating Embeddings for Text Chunks\nEmbeddings transform text into numerical vectors, which allow for efficient similarity search.",
    "relevance": null,
    "word_count": 21
  },
  {
    "id": "e9d9087f58d21f5dc0124d1fede6425f",
    "rank": 7,
    "doc_index": 12,
    "score": 0.3754081726074219,
    "percent_difference": 38.25,
    "text": "## Creating Embeddings for Text Chunks\nCode:\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreates embeddings for the given text using the specified OpenAI model.\nArgs:\ntext (str): The input text for which embeddings are to be created.\nmodel (str): The model to be used for creating embeddings. Default is \"BAAI/bge-en-icl\".\nReturns:\ndict: The response from the OpenAI API containing the embeddings.\n\"\"\"\n# Create embeddings for the input text using the specified model\nresponse = client.embeddings.create(\nmodel=model,\ninput=text\n)\nreturn response  # Return the response containing the embeddings\n# Create embeddings for the text chunks\nresponse = create_embeddings(text_chunks)",
    "relevance": null,
    "word_count": 130
  },
  {
    "id": "590f4599aabdb60dd8d3cb50bce662f1",
    "rank": 8,
    "doc_index": 0,
    "score": 0.3287642300128937,
    "percent_difference": 45.93,
    "text": "# Introduction to Simple RAG\nRetrieval-Augmented Generation (RAG) is a hybrid approach that combines information retrieval with generative models. It enhances the performance of language models by incorporating external knowledge, which improves accuracy and factual correctness.\nIn a Simple RAG setup, we follow these steps:\n1. **Data Ingestion**: Load and preprocess the text data.\n2. **Chunking**: Break the data into smaller chunks to improve retrieval performance.\n3. **Embedding Creation**: Convert the text chunks into numerical representations using an embedding model.\n4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n5. **Response Generation**: Use a language model to generate a response based on retrieved text.\nThis notebook implements a Simple RAG approach, evaluates the model's response, and explores various improvements.",
    "relevance": null,
    "word_count": 167
  },
  {
    "id": "8554438c02e2093b2a7b3719348ae5a9",
    "rank": 9,
    "doc_index": 13,
    "score": 0.31285378336906433,
    "percent_difference": 48.54,
    "text": "## Performing Semantic Search\nWe implement cosine similarity to find the most relevant text chunks for a user query.",
    "relevance": null,
    "word_count": 21
  },
  {
    "id": "ba473f0472e1ba29da861ad3e205e798",
    "rank": 10,
    "doc_index": 16,
    "score": 0.2660173363983631,
    "percent_difference": 56.25,
    "text": "## Generating a Response Based on Retrieved Chunks\nCode:\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates a response from the AI model based on the system prompt and user message.\nArgs:\nsystem_prompt (str): The system prompt to guide the AI's behavior.\nuser_message (str): The user's message or query.\nmodel (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\nReturns:\ndict: The response from the AI model.\n\"\"\"\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_message}\n]\n)\nreturn response\n# Create the user prompt based on the top chunks\nuser_prompt = \"n\".join([f\"Context {i + 1}:n{chunk}n=====================================n\" for i, chunk in enumerate(top_chunks)])\nuser_prompt = f\"{user_prompt}nQuestion: {query}\"\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)",
    "relevance": null,
    "word_count": 275
  },
  {
    "id": "ddc12ca461fde46206b59ce349e18085",
    "rank": 11,
    "doc_index": 3,
    "score": 0.2566831111907959,
    "percent_difference": 57.78,
    "text": "## Extracting Text from a PDF File\nTo implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library.",
    "relevance": null,
    "word_count": 37
  },
  {
    "id": "9eedf73b166ccdc909218eba6e69dd72",
    "rank": 12,
    "doc_index": 4,
    "score": 0.18455010652542114,
    "percent_difference": 69.65,
    "text": "## Extracting Text from a PDF File\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file and prints the first `num_chars` characters.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text",
    "relevance": null,
    "word_count": 143
  },
  {
    "id": "a6030ec345f66aaba95320f7e51e8156",
    "rank": 13,
    "doc_index": 18,
    "score": 0.09838847070932388,
    "percent_difference": 83.82,
    "text": "## Evaluating the AI Response\nCode:\n# Define the system prompt for the evaluation system\nevaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\nevaluation_prompt = f\"User Query: {query}nAI Response:n{ai_response.choices[0].message.content}nTrue Response: {data[0]['ideal_answer']}n{evaluate_system_prompt}\"\n# Generate the evaluation response using the evaluation system prompt and evaluation prompt\nevaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n# Print the evaluation response\nprint(evaluation_response.choices[0].message.content)",
    "relevance": null,
    "word_count": 187
  },
  {
    "id": "0430b10c43379bd27f0a6b6f3d3f3ae2",
    "rank": 14,
    "doc_index": 17,
    "score": 0.09619999676942825,
    "percent_difference": 84.18,
    "text": "## Evaluating the AI Response\nWe compare the AI response with the expected answer and assign a score.",
    "relevance": null,
    "word_count": 20
  },
  {
    "id": "94052a55d4f2f654a85c9cc659c6f18e",
    "rank": 15,
    "doc_index": 2,
    "score": 0.048928402364254,
    "percent_difference": 91.95,
    "text": "## Setting Up the Environment\nCode:\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI",
    "relevance": null,
    "word_count": 22
  },
  {
    "id": "ec06847c02eaf5195ff95315d2a8295a",
    "rank": 16,
    "doc_index": 7,
    "score": 0.03732878342270851,
    "percent_difference": 93.86,
    "text": "## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.",
    "relevance": null,
    "word_count": 19
  },
  {
    "id": "53b6e66239969bc2f4e5a8dd71ab2cf1",
    "rank": 17,
    "doc_index": 1,
    "score": 0.027201073244214058,
    "percent_difference": 95.53,
    "text": "## Setting Up the Environment\nWe begin by importing necessary libraries.",
    "relevance": null,
    "word_count": 13
  },
  {
    "id": "d6037fd3d4c82ec472616906e7c3c09f",
    "rank": 18,
    "doc_index": 14,
    "score": 0.006242861971259117,
    "percent_difference": 98.97,
    "text": "## Performing Semantic Search\nCode:\ndef cosine_similarity(vec1, vec2):\n\"\"\"\nCalculates the cosine similarity between two vectors.\nArgs:\nvec1 (np.ndarray): The first vector.\nvec2 (np.ndarray): The second vector.\nReturns:\nfloat: The cosine similarity between the two vectors.\n\"\"\"\n# Compute the dot product of the two vectors and divide by the product of their norms\nreturn np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))",
    "relevance": null,
    "word_count": 96
  }
]