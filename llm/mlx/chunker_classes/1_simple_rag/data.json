[
  "## Chunking the Extracted Text\nOnce we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy.\n## Extracting and Chunking Text from a PDF File\nNow, we load the PDF, extract text, and split it into chunks.\n## Extracting and Chunking Text from a PDF File\nCode:\n# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\ntext_chunks = chunk_text(extracted_text, 1000, 200)\n# Print the number of text chunks created\nprint(\"Number of text chunks:\", len(text_chunks))\n# Print the first text chunk\nprint(\"nFirst text chunk:\")\nprint(text_chunks[0])\n## Chunking the Extracted Text\nCode:\ndef chunk_text(text, n, overlap):\n\"\"\"\nChunks the given text into segments of n characters with overlap.\nArgs:\ntext (str): The text to be chunked.\nn (int): The number of characters in each chunk.\noverlap (int): The number of overlapping characters between chunks.\nReturns:\nList[str]: A list of text chunks.\n\"\"\"\nchunks = []  # Initialize an empty list to store the chunks",
  "# Loop through the text with a step size of (n - overlap)\nfor i in range(0, len(text), n - overlap):\n# Append a chunk of text from index i to i + n to the chunks list\nchunks.append(text[i:i + n])\nreturn chunks  # Return the list of text chunks\n## Running a Query on Extracted Chunks\nCode:\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\ndata = json.load(f)\n# Extract the first query from the validation data\nquery = data[0]['question']\n# Perform semantic search to find the top 2 most relevant text chunks for the query\ntop_chunks = semantic_search(query, text_chunks, response.data, k=2)\n# Print the query\nprint(\"Query:\", query)\n# Print the top 2 most relevant text chunks\nfor i, chunk in enumerate(top_chunks):\nprint(f\"Context {i + 1}:n{chunk}n=====================================\")",
  "## Creating Embeddings for Text Chunks\nEmbeddings transform text into numerical vectors, which allow for efficient similarity search.\n## Creating Embeddings for Text Chunks\nCode:\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreates embeddings for the given text using the specified OpenAI model.\nArgs:\ntext (str): The input text for which embeddings are to be created.\nmodel (str): The model to be used for creating embeddings. Default is \"BAAI/bge-en-icl\".\nReturns:\ndict: The response from the OpenAI API containing the embeddings.\n\"\"\"\n# Create embeddings for the input text using the specified model\nresponse = client.embeddings.create(\nmodel=model,\ninput=text\n)\nreturn response  # Return the response containing the embeddings\n# Create embeddings for the text chunks\nresponse = create_embeddings(text_chunks)",
  "# Introduction to Simple RAG\nRetrieval-Augmented Generation (RAG) is a hybrid approach that combines information retrieval with generative models. It enhances the performance of language models by incorporating external knowledge, which improves accuracy and factual correctness.\nIn a Simple RAG setup, we follow these steps:\n1. **Data Ingestion**: Load and preprocess the text data.\n2. **Chunking**: Break the data into smaller chunks to improve retrieval performance.\n3. **Embedding Creation**: Convert the text chunks into numerical representations using an embedding model.\n4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n5. **Response Generation**: Use a language model to generate a response based on retrieved text.\nThis notebook implements a Simple RAG approach, evaluates the model's response, and explores various improvements.\n## Performing Semantic Search\nWe implement cosine similarity to find the most relevant text chunks for a user query.\n## Generating a Response Based on Retrieved Chunks\nCode:",
  "# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerates a response from the AI model based on the system prompt and user message.\nArgs:\nsystem_prompt (str): The system prompt to guide the AI's behavior.\nuser_message (str): The user's message or query.\nmodel (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\nReturns:\ndict: The response from the AI model.\n\"\"\"\nresponse = client.chat.completions.create(\nmodel=model,\ntemperature=0,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_message}\n]\n)\nreturn response\n# Create the user prompt based on the top chunks\nuser_prompt = \"n\".join([f\"Context {i + 1}:n{chunk}n=====================================n\" for i, chunk in enumerate(top_chunks)])\nuser_prompt = f\"{user_prompt}nQuestion: {query}\"\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)",
  "## Extracting Text from a PDF File\nTo implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library.\n## Extracting Text from a PDF File\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file and prints the first `num_chars` characters.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text\n## Evaluating the AI Response\nCode:",
  "# Define the system prompt for the evaluation system\nevaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\nevaluation_prompt = f\"User Query: {query}nAI Response:n{ai_response.choices[0].message.content}nTrue Response: {data[0]['ideal_answer']}n{evaluate_system_prompt}\"\n# Generate the evaluation response using the evaluation system prompt and evaluation prompt\nevaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n# Print the evaluation response\nprint(evaluation_response.choices[0].message.content)\n## Evaluating the AI Response\nWe compare the AI response with the expected answer and assign a score.\n## Setting Up the Environment\nCode:\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\n## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.\n## Setting Up the Environment\nWe begin by importing necessary libraries.",
  "## Performing Semantic Search\nCode:\ndef cosine_similarity(vec1, vec2):\n\"\"\"\nCalculates the cosine similarity between two vectors.\nArgs:\nvec1 (np.ndarray): The first vector.\nvec2 (np.ndarray): The second vector.\nReturns:\nfloat: The cosine similarity between the two vectors.\n\"\"\"\n# Compute the dot product of the two vectors and divide by the product of their norms\nreturn np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
]