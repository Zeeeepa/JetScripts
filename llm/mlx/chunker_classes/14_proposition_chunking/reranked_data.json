[
  {
    "id": "944f25eb13eebbf8771cfd0f23c3a04f",
    "rank": 1,
    "doc_index": 5,
    "score": 0.6079847812652588,
    "percent_difference": 0.0,
    "text": "## Chunking the Extracted Text\nOnce we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy.",
    "relevance": null,
    "word_count": 26
  },
  {
    "id": "25152a2ca157b291437b549f502617b7",
    "rank": 2,
    "doc_index": 6,
    "score": 0.5477058291435242,
    "percent_difference": 9.91,
    "text": "## Chunking the Extracted Text\nCode:\ndef chunk_text(text, chunk_size=800, overlap=100):\n\"\"\"\nSplit text into overlapping chunks.\nArgs:\ntext (str): Input text to chunk\nchunk_size (int): Size of each chunk in characters\noverlap (int): Overlap between chunks in characters\nReturns:\nList[Dict]: List of chunk dictionaries with text and metadata\n\"\"\"\nchunks = []  # Initialize an empty list to store the chunks\n# Iterate over the text with the specified chunk size and overlap\nfor i in range(0, len(text), chunk_size - overlap):\nchunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\nif chunk:  # Ensure we don't add empty chunks\nchunks.append({\n\"text\": chunk,  # The chunk text\n\"chunk_id\": len(chunks) + 1,  # Unique ID for the chunk\n\"start_char\": i,  # Starting character index of the chunk\n\"end_char\": i + len(chunk)  # Ending character index of the chunk\n})\nprint(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\nreturn chunks  # Return the list of chunks",
    "relevance": null,
    "word_count": 240
  },
  {
    "id": "d39779bb570c0e88a0b6dcae9286b164",
    "rank": 3,
    "doc_index": 0,
    "score": 0.4793897271156311,
    "percent_difference": 21.15,
    "text": "# Proposition Chunking for Enhanced RAG\nIn this notebook, I implement proposition chunking - an advanced technique to break down documents into atomic, factual statements for more accurate retrieval. Unlike traditional chunking that simply divides text by character count, proposition chunking preserves the semantic integrity of individual facts.\nProposition chunking delivers more precise retrieval by:\n1. Breaking content into atomic, self-contained facts\n2. Creating smaller, more granular units for retrieval\n3. Enabling more precise matching between queries and relevant content\n4. Filtering out low-quality or incomplete propositions\nLet's build a complete implementation without relying on LangChain or FAISS.",
    "relevance": null,
    "word_count": 112
  },
  {
    "id": "2355704ce486f3e34bdf4a22f24f7f4b",
    "rank": 4,
    "doc_index": 15,
    "score": 0.42657482624053955,
    "percent_difference": 29.84,
    "text": "## Building Vector Stores for Both Approaches\nCode:\ndef build_vector_stores(chunks, propositions):\n\"\"\"\nBuild vector stores for both chunk-based and proposition-based approaches.\nArgs:\nchunks (List[Dict]): Original document chunks\npropositions (List[Dict]): Quality-filtered propositions\nReturns:\nTuple[SimpleVectorStore, SimpleVectorStore]: Chunk and proposition vector stores\n\"\"\"\n# Create vector store for chunks\nchunk_store = SimpleVectorStore()\n# Extract chunk texts and create embeddings\nchunk_texts = [chunk[\"text\"] for chunk in chunks]\nprint(f\"Creating embeddings for {len(chunk_texts)} chunks...\")\nchunk_embeddings = create_embeddings(chunk_texts)\n# Add chunks to vector store with metadata\nchunk_metadata = [{\"chunk_id\": chunk[\"chunk_id\"], \"type\": \"chunk\"} for chunk in chunks]\nchunk_store.add_items(chunk_texts, chunk_embeddings, chunk_metadata)\n# Create vector store for propositions\nprop_store = SimpleVectorStore()\n# Extract proposition texts and create embeddings\nprop_texts = [prop[\"text\"] for prop in propositions]\nprint(f\"Creating embeddings for {len(prop_texts)} propositions...\")\nprop_embeddings = create_embeddings(prop_texts)\n# Add propositions to vector store with metadata\nprop_metadata = [\n{\n\"type\": \"proposition\",\n\"source_chunk_id\": prop[\"source_chunk_id\"],\n\"quality_scores\": prop[\"quality_scores\"]\n}\nfor prop in propositions\n]\nprop_store.add_items(prop_texts, prop_embeddings, prop_metadata)\nreturn chunk_store, prop_store",
    "relevance": null,
    "word_count": 284
  },
  {
    "id": "7c1e506e49113e1b709e37ce6d03b312",
    "rank": 5,
    "doc_index": 18,
    "score": 0.37550153732299807,
    "percent_difference": 38.24,
    "text": "## Complete End-to-End Evaluation Pipeline\nCode:\ndef run_proposition_chunking_evaluation(pdf_path, test_queries, reference_answers=None):\n\"\"\"\nRun a complete evaluation of proposition chunking vs standard chunking.\nArgs:\npdf_path (str): Path to the PDF file\ntest_queries (List[str]): List of test queries\nreference_answers (List[str], optional): Reference answers for queries\nReturns:\nDict: Evaluation results\n\"\"\"\nprint(\"=== Starting Proposition Chunking Evaluation ===n\")\n# Process document into propositions and chunks\nchunks, propositions = process_document_into_propositions(pdf_path)\n# Build vector stores for chunks and propositions\nchunk_store, prop_store = build_vector_stores(chunks, propositions)\n# Initialize a list to store results for each query\nresults = []\n# Run tests for each query\nfor i, query in enumerate(test_queries):\nprint(f\"nn=== Testing Query {i+1}/{len(test_queries)} ===\")\nprint(f\"Query: {query}\")\n# Get retrieval results from both chunk-based and proposition-based approaches\nretrieval_results = compare_retrieval_approaches(query, chunk_store, prop_store)\n# Generate responses based on the retrieved proposition-based results\nprint(\"nGenerating response from proposition-based results...\")\nprop_response = generate_response(\nquery,\nretrieval_results[\"proposition_results\"],\n\"proposition\"\n)\n# Generate responses based on the retrieved chunk-based results\nprint(\"Generating response from chunk-based results...\")\nchunk_response = generate_response(\nquery,\nretrieval_results[\"chunk_results\"],\n\"chunk\"\n)\n# Get reference answer if available\nreference = None\nif reference_answers and i < len(reference_answers):\nreference = reference_answers[i]\n# Evaluate the generated responses\nprint(\"nEvaluating responses...\")\nevaluation = evaluate_responses(query, prop_response, chunk_response, reference)\n# Compile results for the current query\nquery_result = {\n\"query\": query,\n\"proposition_results\": retrieval_results[\"proposition_results\"],\n\"chunk_results\": retrieval_results[\"chunk_results\"],\n\"proposition_response\": prop_response,\n\"chunk_response\": chunk_response,\n\"reference_answer\": reference,\n\"evaluation\": evaluation\n}\n# Append the results to the overall results list\nresults.append(query_result)\n# Print the responses and evaluation for the current query\nprint(\"n=== Proposition-Based Response ===\")\nprint(prop_response)\nprint(\"n=== Chunk-Based Response ===\")\nprint(chunk_response)\nprint(\"n=== Evaluation ===\")\nprint(evaluation)\n# Generate overall analysis of the evaluation\nprint(\"nn=== Generating Overall Analysis ===\")\noverall_analysis = generate_overall_analysis(results)\nprint(\"n\" + overall_analysis)\n# Return the evaluation results, overall analysis, and counts of propositions and chunks\nreturn {\n\"results\": results,\n\"overall_analysis\": overall_analysis,\n\"proposition_count\": len(propositions),\n\"chunk_count\": len(chunks)\n}",
    "relevance": null,
    "word_count": 529
  },
  {
    "id": "24c1dfb8777ca12fcdb93c880aa64101",
    "rank": 6,
    "doc_index": 9,
    "score": 0.36182302236557007,
    "percent_difference": 40.49,
    "text": "## Simple Vector Store Implementation\nWe'll create a basic vector store to manage document chunks and their embeddings.",
    "relevance": null,
    "word_count": 21
  },
  {
    "id": "51dccd8b6cc899ba0fcf5c2c1eb27303",
    "rank": 7,
    "doc_index": 14,
    "score": 0.34433382749557495,
    "percent_difference": 43.36,
    "text": "## Complete Proposition Processing Pipeline\nCode:\ndef process_document_into_propositions(pdf_path, chunk_size=800, chunk_overlap=100,\nquality_thresholds=None):\n\"\"\"\nProcess a document into quality-checked propositions.\nArgs:\npdf_path (str): Path to the PDF file\nchunk_size (int): Size of each chunk in characters\nchunk_overlap (int): Overlap between chunks in characters\nquality_thresholds (Dict): Threshold scores for proposition quality\nReturns:\nTuple[List[Dict], List[Dict]]: Original chunks and proposition chunks\n\"\"\"\n# Set default quality thresholds if not provided\nif quality_thresholds is None:\nquality_thresholds = {\n\"accuracy\": 7,\n\"clarity\": 7,\n\"completeness\": 7,\n\"conciseness\": 7\n}\n# Extract text from the PDF file\ntext = extract_text_from_pdf(pdf_path)\n# Create chunks from the extracted text\nchunks = chunk_text(text, chunk_size, chunk_overlap)\n# Initialize a list to store all propositions\nall_propositions = []\nprint(\"Generating propositions from chunks...\")\nfor i, chunk in enumerate(chunks):\nprint(f\"Processing chunk {i+1}/{len(chunks)}...\")\n# Generate propositions for the current chunk\nchunk_propositions = generate_propositions(chunk)\nprint(f\"Generated {len(chunk_propositions)} propositions\")\n# Process each generated proposition\nfor prop in chunk_propositions:\nproposition_data = {\n\"text\": prop,\n\"source_chunk_id\": chunk[\"chunk_id\"],\n\"source_text\": chunk[\"text\"]\n}\nall_propositions.append(proposition_data)\n# Evaluate the quality of the generated propositions\nprint(\"nEvaluating proposition quality...\")\nquality_propositions = []\nfor i, prop in enumerate(all_propositions):\nif i % 10 == 0:  # Status update every 10 propositions\nprint(f\"Evaluating proposition {i+1}/{len(all_propositions)}...\")\n# Evaluate the quality of the current proposition\nscores = evaluate_proposition(prop[\"text\"], prop[\"source_text\"])\nprop[\"quality_scores\"] = scores\n# Check if the proposition passes the quality thresholds\npasses_quality = True\nfor metric, threshold in quality_thresholds.items():\nif scores.get(metric, 0) < threshold:\npasses_quality = False\nbreak\nif passes_quality:\nquality_propositions.append(prop)\nelse:\nprint(f\"Proposition failed quality check: {prop['text'][:50]}...\")\nprint(f\"nRetained {len(quality_propositions)}/{len(all_propositions)} propositions after quality filtering\")\nreturn chunks, quality_propositions",
    "relevance": null,
    "word_count": 480
  },
  {
    "id": "ca73d5f0987b63c94e67f531f8ea6e05",
    "rank": 8,
    "doc_index": 19,
    "score": 0.2675636534889539,
    "percent_difference": 55.99,
    "text": "## Evaluation of Proposition Chunking\nCode:\n# Path to the AI information document that will be processed\npdf_path = \"data/AI_Information.pdf\"\n# Define test queries covering different aspects of AI to evaluate proposition chunking\ntest_queries = [\n\"What are the main ethical concerns in AI development?\",\n# \"How does explainable AI improve trust in AI systems?\",\n# \"What are the key challenges in developing fair AI systems?\",\n# \"What role does human oversight play in AI safety?\"\n]\n# Reference answers for more thorough evaluation and comparison of results\n# These provide a ground truth to measure the quality of generated responses\nreference_answers = [\n\"The main ethical concerns in AI development include bias and fairness, privacy, transparency, accountability, safety, and the potential for misuse or harmful applications.\",\n# \"Explainable AI improves trust by making AI decision-making processes transparent and understandable to users, helping them verify fairness, identify potential biases, and better understand AI limitations.\",\n# \"Key challenges in developing fair AI systems include addressing data bias, ensuring diverse representation in training data, creating transparent algorithms, defining fairness across different contexts, and balancing competing fairness criteria.\",\n# \"Human oversight plays a critical role in AI safety by monitoring system behavior, verifying outputs, intervening when necessary, setting ethical boundaries, and ensuring AI systems remain aligned with human values and intentions throughout their operation.\"\n]\n# Run the evaluation\nevaluation_results = run_proposition_chunking_evaluation(\npdf_path=pdf_path,\ntest_queries=test_queries,\nreference_answers=reference_answers\n)\n# Print the overall analysis\nprint(\"nn=== Overall Analysis ===\")\nprint(evaluation_results[\"overall_analysis\"])",
    "relevance": null,
    "word_count": 309
  },
  {
    "id": "ddc12ca461fde46206b59ce349e18085",
    "rank": 9,
    "doc_index": 3,
    "score": 0.2566831111907959,
    "percent_difference": 57.78,
    "text": "## Extracting Text from a PDF File\nTo implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library.",
    "relevance": null,
    "word_count": 37
  },
  {
    "id": "be93c0cf02f04beec509e3e2c98ac0c1",
    "rank": 10,
    "doc_index": 12,
    "score": 0.24773017689585686,
    "percent_difference": 59.25,
    "text": "## Proposition Generation\nCode:\ndef generate_propositions(chunk):\n\"\"\"\nGenerate atomic, self-contained propositions from a text chunk.\nArgs:\nchunk (Dict): Text chunk with content and metadata\nReturns:\nList[str]: List of generated propositions\n\"\"\"\n# System prompt to instruct the AI on how to generate propositions\nsystem_prompt = \"\"\"Please break down the following text into simple, self-contained propositions.\nEnsure that each proposition meets the following criteria:\n1. Express a Single Fact: Each proposition should state one specific fact or claim.\n2. Be Understandable Without Context: The proposition should be self-contained, meaning it can be understood without needing additional context.\n3. Use Full Names, Not Pronouns: Avoid pronouns or ambiguous references; use full entity names.\n4. Include Relevant Dates/Qualifiers: If applicable, include necessary dates, times, and qualifiers to make the fact precise.\n5. Contain One Subject-Predicate Relationship: Focus on a single subject and its corresponding action or attribute, without conjunctions or multiple clauses.\nOutput ONLY the list of propositions without any additional text or explanations.\"\"\"\n# User prompt containing the text chunk to be converted into propositions\nuser_prompt = f\"Text to convert into propositions:nn{chunk['text']}\"\n# Generate response from the model\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",  # Using a stronger model for accurate proposition generation\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0\n)\n# Extract propositions from the response\nraw_propositions = response.choices[0].message.content.strip().split('n')\n# Clean up propositions (remove numbering, bullets, etc.)\nclean_propositions = []\nfor prop in raw_propositions:\n# Remove numbering (1., 2., etc.) and bullet points\ncleaned = re.sub(r'^s*(d+.|-|*)s*', '', prop).strip()\nif cleaned and len(cleaned) > 10:  # Simple filter for empty or very short propositions\nclean_propositions.append(cleaned)\nreturn clean_propositions",
    "relevance": null,
    "word_count": 408
  },
  {
    "id": "33fb8f5906929687ecc6fcd5dfc96be3",
    "rank": 11,
    "doc_index": 11,
    "score": 0.20443127304315567,
    "percent_difference": 66.38,
    "text": "## Creating Embeddings\nCode:\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreate embeddings for the given texts.\nArgs:\ntexts (str or List[str]): Input text(s)\nmodel (str): Embedding model name\nReturns:\nList[List[float]]: Embedding vector(s)\n\"\"\"\n# Handle both string and list inputs\ninput_texts = texts if isinstance(texts, list) else [texts]\n# Process in batches if needed (OpenAI API limits)\nbatch_size = 100\nall_embeddings = []\n# Iterate over the input texts in batches\nfor i in range(0, len(input_texts), batch_size):\nbatch = input_texts[i:i + batch_size]  # Get the current batch of texts\n# Create embeddings for the current batch\nresponse = client.embeddings.create(\nmodel=model,\ninput=batch\n)\n# Extract embeddings from the response\nbatch_embeddings = [item.embedding for item in response.data]\nall_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n# If input was a single string, return just the first embedding\nif isinstance(texts, str):\nreturn all_embeddings[0]\n# Otherwise, return all embeddings\nreturn all_embeddings",
    "relevance": null,
    "word_count": 225
  },
  {
    "id": "9eedf73b166ccdc909218eba6e69dd72",
    "rank": 12,
    "doc_index": 4,
    "score": 0.18455010652542114,
    "percent_difference": 69.65,
    "text": "## Extracting Text from a PDF File\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file and prints the first `num_chars` characters.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text",
    "relevance": null,
    "word_count": 143
  },
  {
    "id": "aa3a173d7f8720f5a811932b0a98a688",
    "rank": 13,
    "doc_index": 17,
    "score": 0.17849225799242655,
    "percent_difference": 70.64,
    "text": "## Response Generation and Evaluation\nCode:\ndef generate_response(query, results, result_type=\"proposition\"):\n\"\"\"\nGenerate a response based on retrieved results.\nArgs:\nquery (str): User query\nresults (List[Dict]): Retrieved items\nresult_type (str): Type of results ('proposition' or 'chunk')\nReturns:\nstr: Generated response\n\"\"\"\n# Combine retrieved texts into a single context string\ncontext = \"nn\".join([result[\"text\"] for result in results])\n# System prompt to instruct the AI on how to generate the response\nsystem_prompt = f\"\"\"You are an AI assistant answering questions based on retrieved information.\nYour answer should be based on the following {result_type}s that were retrieved from a knowledge base.\nIf the retrieved information doesn't answer the question, acknowledge this limitation.\"\"\"\n# User prompt containing the query and the retrieved context\nuser_prompt = f\"\"\"Query: {query}\nRetrieved {result_type}s:\n{context}\nPlease answer the query based on the retrieved information.\"\"\"\n# Generate the response using the OpenAI client\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0.2\n)\n# Return the generated response text\nreturn response.choices[0].message.content",
    "relevance": null,
    "word_count": 281
  },
  {
    "id": "90823e922d90a118ed03108157b9f0ca",
    "rank": 14,
    "doc_index": 10,
    "score": 0.14936501234769822,
    "percent_difference": 75.43,
    "text": "## Simple Vector Store Implementation\nCode:\nclass SimpleVectorStore:\n\"\"\"\nA simple vector store implementation using NumPy.\n\"\"\"\ndef __init__(self):\n# Initialize lists to store vectors, texts, and metadata\nself.vectors = []\nself.texts = []\nself.metadata = []\ndef add_item(self, text, embedding, metadata=None):\n\"\"\"\nAdd an item to the vector store.\nArgs:\ntext (str): The text content\nembedding (List[float]): The embedding vector\nmetadata (Dict, optional): Additional metadata\n\"\"\"\n# Append the embedding, text, and metadata to their respective lists\nself.vectors.append(np.array(embedding))\nself.texts.append(text)\nself.metadata.append(metadata or {})\ndef add_items(self, texts, embeddings, metadata_list=None):\n\"\"\"\nAdd multiple items to the vector store.\nArgs:\ntexts (List[str]): List of text contents\nembeddings (List[List[float]]): List of embedding vectors\nmetadata_list (List[Dict], optional): List of metadata dictionaries\n\"\"\"\n# If no metadata list is provided, create an empty dictionary for each text\nif metadata_list is None:\nmetadata_list = [{} for _ in range(len(texts))]\n# Add each text, embedding, and metadata to the store\nfor text, embedding, metadata in zip(texts, embeddings, metadata_list):\nself.add_item(text, embedding, metadata)\ndef similarity_search(self, query_embedding, k=5):\n\"\"\"\nFind the most similar items to a query embedding.\nArgs:\nquery_embedding (List[float]): Query embedding vector\nk (int): Number of results to return\nReturns:\nList[Dict]: Top k most similar items\n\"\"\"\n# Return an empty list if there are no vectors in the store\nif not self.vectors:\nreturn []\n# Convert query embedding to a numpy array\nquery_vector = np.array(query_embedding)\n# Calculate similarities using cosine similarity\nsimilarities = []\nfor i, vector in enumerate(self.vectors):\nsimilarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\nsimilarities.append((i, similarity))\n# Sort by similarity in descending order\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n# Collect the top k results\nresults = []\nfor i in range(min(k, len(similarities))):\nidx, score = similarities[i]\nresults.append({\n\"text\": self.texts[idx],\n\"metadata\": self.metadata[idx],\n\"similarity\": float(score)  # Convert to float for JSON serialization\n})\nreturn results",
    "relevance": null,
    "word_count": 517
  },
  {
    "id": "d28c815c2dbeedad2cbcd66e42e7c5f2",
    "rank": 15,
    "doc_index": 16,
    "score": 0.13935431838035583,
    "percent_difference": 77.08,
    "text": "## Query and Retrieval Functions\nCode:\ndef retrieve_from_store(query, vector_store, k=5):\n\"\"\"\nRetrieve relevant items from a vector store based on query.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store to search\nk (int): Number of results to retrieve\nReturns:\nList[Dict]: Retrieved items with scores and metadata\n\"\"\"\n# Create query embedding\nquery_embedding = create_embeddings(query)\n# Search vector store for the top k most similar items\nresults = vector_store.similarity_search(query_embedding, k=k)\nreturn results",
    "relevance": null,
    "word_count": 107
  },
  {
    "id": "b46e4f1d5b6978a3339d23b09c947953",
    "rank": 16,
    "doc_index": 13,
    "score": 0.13643767684698105,
    "percent_difference": 77.56,
    "text": "## Quality Checking for Propositions\nCode:\ndef evaluate_proposition(proposition, original_text):\n\"\"\"\nEvaluate a proposition's quality based on accuracy, clarity, completeness, and conciseness.\nArgs:\nproposition (str): The proposition to evaluate\noriginal_text (str): The original text for comparison\nReturns:\nDict: Scores for each evaluation dimension\n\"\"\"\n# System prompt to instruct the AI on how to evaluate the proposition\nsystem_prompt = \"\"\"You are an expert at evaluating the quality of propositions extracted from text.\nRate the given proposition on the following criteria (scale 1-10):\n- Accuracy: How well the proposition reflects information in the original text\n- Clarity: How easy it is to understand the proposition without additional context\n- Completeness: Whether the proposition includes necessary details (dates, qualifiers, etc.)\n- Conciseness: Whether the proposition is concise without losing important information\nThe response must be in valid JSON format with numerical scores for each criterion:\n{\"accuracy\": X, \"clarity\": X, \"completeness\": X, \"conciseness\": X}\n\"\"\"\n# User prompt containing the proposition and the original text\nuser_prompt = f\"\"\"Proposition: {proposition}\nOriginal Text: {original_text}\nPlease provide your evaluation scores in JSON format.\"\"\"\n# Generate response from the model\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\nresponse_format={\"type\": \"json_object\"},\ntemperature=0\n)\n# Parse the JSON response\ntry:\nscores = json.loads(response.choices[0].message.content.strip())\nreturn scores\nexcept json.JSONDecodeError:\n# Fallback if JSON parsing fails\nreturn {\n\"accuracy\": 5,\n\"clarity\": 5,\n\"completeness\": 5,\n\"conciseness\": 5\n}",
    "relevance": null,
    "word_count": 370
  },
  {
    "id": "5d26c9bc9eceddf8b8a3f6b55767c300",
    "rank": 17,
    "doc_index": 2,
    "score": 0.05382559448480606,
    "percent_difference": 91.15,
    "text": "## Setting Up the Environment\nCode:\nimport os\nimport numpy as np\nimport json\nimport fitz\nfrom openai import OpenAI\nimport re",
    "relevance": null,
    "word_count": 24
  },
  {
    "id": "ec06847c02eaf5195ff95315d2a8295a",
    "rank": 18,
    "doc_index": 7,
    "score": 0.03732878342270851,
    "percent_difference": 93.86,
    "text": "## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.",
    "relevance": null,
    "word_count": 19
  },
  {
    "id": "53b6e66239969bc2f4e5a8dd71ab2cf1",
    "rank": 19,
    "doc_index": 1,
    "score": 0.027201073244214058,
    "percent_difference": 95.53,
    "text": "## Setting Up the Environment\nWe begin by importing necessary libraries.",
    "relevance": null,
    "word_count": 13
  }
]