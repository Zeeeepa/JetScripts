[
  "## Standard (Non-Hierarchical) RAG for Comparison\nCode:\ndef standard_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, k=15):\n\"\"\"\nStandard RAG pipeline without hierarchical retrieval.\nArgs:\nquery (str): User query\npdf_path (str): Path to the PDF document\nchunk_size (int): Size of each chunk\nchunk_overlap (int): Overlap between chunks\nk (int): Number of chunks to retrieve\nReturns:\nDict: Results including response and retrieved chunks\n\"\"\"\n# Extract pages from the PDF document\npages = extract_text_from_pdf(pdf_path)\n# Create chunks directly from all pages\nchunks = []\nfor page in pages:\n# Chunk the text of the page\npage_chunks = chunk_text(\npage[\"text\"],\npage[\"metadata\"],\nchunk_size,\nchunk_overlap\n)\n# Extend the chunks list with the chunks from the current page\nchunks.extend(page_chunks)\nprint(f\"Created {len(chunks)} chunks for standard RAG\")\n# Create a vector store to hold the chunks\nstore = SimpleVectorStore()\n# Create embeddings for the chunks\nprint(\"Creating embeddings for chunks...\")\ntexts = [chunk[\"text\"] for chunk in chunks]\nembeddings = create_embeddings(texts)\n# Add chunks to the vector store\nfor i, chunk in enumerate(chunks):\nstore.add_item(\ntext=chunk[\"text\"],\nembedding=embeddings[i],\nmetadata=chunk[\"metadata\"]\n)",
  "# Create an embedding for the query\nquery_embedding = create_embeddings(query)\n# Retrieve the most relevant chunks based on the query embedding\nretrieved_chunks = store.similarity_search(query_embedding, k=k)\nprint(f\"Retrieved {len(retrieved_chunks)} chunks with standard RAG\")\n# Generate a response based on the retrieved chunks\nresponse = generate_response(query, retrieved_chunks)\n# Return the results including the query, response, and retrieved chunks\nreturn {\n\"query\": query,\n\"response\": response,\n\"retrieved_chunks\": retrieved_chunks\n}\n## Hierarchical Document Processing\nCode:\ndef process_document_hierarchically(pdf_path, chunk_size=1000, chunk_overlap=200):\n\"\"\"\nProcess a document into hierarchical indices.\nArgs:\npdf_path (str): Path to the PDF file\nchunk_size (int): Size of each detailed chunk\nchunk_overlap (int): Overlap between chunks\nReturns:\nTuple[SimpleVectorStore, SimpleVectorStore]: Summary and detailed vector stores\n\"\"\"\n# Extract pages from PDF\npages = extract_text_from_pdf(pdf_path)\n# Create summaries for each page\nprint(\"Generating page summaries...\")\nsummaries = []\nfor i, page in enumerate(pages):\nprint(f\"Summarizing page {i+1}/{len(pages)}...\")\nsummary_text = generate_page_summary(page[\"text\"])\n# Create summary metadata\nsummary_metadata = page[\"metadata\"].copy()\nsummary_metadata.update({\"is_summary\": True})",
  "# Append the summary text and metadata to the summaries list\nsummaries.append({\n\"text\": summary_text,\n\"metadata\": summary_metadata\n})\n# Create detailed chunks for each page\ndetailed_chunks = []\nfor page in pages:\n# Chunk the text of the page\npage_chunks = chunk_text(\npage[\"text\"],\npage[\"metadata\"],\nchunk_size,\nchunk_overlap\n)\n# Extend the detailed_chunks list with the chunks from the current page\ndetailed_chunks.extend(page_chunks)\nprint(f\"Created {len(detailed_chunks)} detailed chunks\")\n# Create embeddings for summaries\nprint(\"Creating embeddings for summaries...\")\nsummary_texts = [summary[\"text\"] for summary in summaries]\nsummary_embeddings = create_embeddings(summary_texts)\n# Create embeddings for detailed chunks\nprint(\"Creating embeddings for detailed chunks...\")\nchunk_texts = [chunk[\"text\"] for chunk in detailed_chunks]\nchunk_embeddings = create_embeddings(chunk_texts)\n# Create vector stores\nsummary_store = SimpleVectorStore()\ndetailed_store = SimpleVectorStore()\n# Add summaries to summary store\nfor i, summary in enumerate(summaries):\nsummary_store.add_item(\ntext=summary[\"text\"],\nembedding=summary_embeddings[i],\nmetadata=summary[\"metadata\"]\n)",
  "# Add chunks to detailed store\nfor i, chunk in enumerate(detailed_chunks):\ndetailed_store.add_item(\ntext=chunk[\"text\"],\nembedding=chunk_embeddings[i],\nmetadata=chunk[\"metadata\"]\n)\nprint(f\"Created vector stores with {len(summaries)} summaries and {len(detailed_chunks)} chunks\")\nreturn summary_store, detailed_store\n## Complete RAG Pipeline with Hierarchical Retrieval\nCode:\ndef hierarchical_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200,\nk_summaries=3, k_chunks=5, regenerate=False):\n\"\"\"\nComplete hierarchical RAG pipeline.\nArgs:\nquery (str): User query\npdf_path (str): Path to the PDF document\nchunk_size (int): Size of each detailed chunk\nchunk_overlap (int): Overlap between chunks\nk_summaries (int): Number of summaries to retrieve\nk_chunks (int): Number of chunks to retrieve per summary\nregenerate (bool): Whether to regenerate vector stores\nReturns:\nDict: Results including response and retrieved chunks\n\"\"\"\n# Create store filenames for caching\nsummary_store_file = f\"{os.path.basename(pdf_path)}_summary_store.pkl\"\ndetailed_store_file = f\"{os.path.basename(pdf_path)}_detailed_store.pkl\"\n# Process document and create stores if needed\nif regenerate or not os.path.exists(summary_store_file) or not os.path.exists(detailed_store_file):\nprint(\"Processing document and creating vector stores...\")",
  "# Process the document to create hierarchical indices and vector stores\nsummary_store, detailed_store = process_document_hierarchically(\npdf_path, chunk_size, chunk_overlap\n)\n# Save the summary store to a file for future use\nwith open(summary_store_file, 'wb') as f:\npickle.dump(summary_store, f)\n# Save the detailed store to a file for future use\nwith open(detailed_store_file, 'wb') as f:\npickle.dump(detailed_store, f)\nelse:\n# Load existing summary store from file\nprint(\"Loading existing vector stores...\")\nwith open(summary_store_file, 'rb') as f:\nsummary_store = pickle.load(f)\n# Load existing detailed store from file\nwith open(detailed_store_file, 'rb') as f:\ndetailed_store = pickle.load(f)\n# Retrieve relevant chunks hierarchically using the query\nretrieved_chunks = retrieve_hierarchically(\nquery, summary_store, detailed_store, k_summaries, k_chunks\n)\n# Generate a response based on the retrieved chunks\nresponse = generate_response(query, retrieved_chunks)\n# Return results including the query, response, retrieved chunks, and counts of summaries and detailed chunks\nreturn {\n\"query\": query,\n\"response\": response,\n\"retrieved_chunks\": retrieved_chunks,\n\"summary_count\": len(summary_store.texts),\n\"detailed_count\": len(detailed_store.texts)\n}",
  "## Response Generation with Context\nCode:\ndef generate_response(query, retrieved_chunks):\n\"\"\"\nGenerate a response based on the query and retrieved chunks.\nArgs:\nquery (str): User query\nretrieved_chunks (List[Dict]): Retrieved chunks from hierarchical search\nReturns:\nstr: Generated response\n\"\"\"\n# Extract text from chunks and prepare context parts\ncontext_parts = []\nfor i, chunk in enumerate(retrieved_chunks):\npage_num = chunk[\"metadata\"][\"page\"]  # Get the page number from metadata\ncontext_parts.append(f\"[Page {page_num}]: {chunk['text']}\")  # Format the chunk text with page number\n# Combine all context parts into a single context string\ncontext = \"nn\".join(context_parts)\n# Define the system message to guide the AI assistant\nsystem_message = \"\"\"You are a helpful AI assistant answering questions based on the provided context.\nUse the information from the context to answer the user's question accurately.\nIf the context doesn't contain relevant information, acknowledge that.\nInclude page numbers when referencing specific information.\"\"\"",
  "# Generate the response using the OpenAI API\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\nmessages=[\n{\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n{\"role\": \"user\", \"content\": f\"Context:nn{context}nnQuestion: {query}\"}  # User message with context and query\n],\ntemperature=0.2  # Set the temperature for response generation\n)\n# Return the generated response content\nreturn response.choices[0].message.content\n# Hierarchical Indices for RAG\nIn this notebook, I implement a hierarchical indexing approach for RAG systems. This technique improves retrieval by using a two-tier search method: first identifying relevant document sections through summaries, then retrieving specific details from those sections.\nTraditional RAG approaches treat all text chunks equally, which can lead to:\n- Lost context when chunks are too small\n- Irrelevant results when the document collection is large\n- Inefficient searches across the entire corpus\nHierarchical retrieval solves these problems by:\n- Creating concise summaries for larger document sections\n- First searching these summaries to identify relevant sections\n- Then retrieving detailed information only from those sections\n- Maintaining context while preserving specific details",
  "## Summarization Function\nCode:\ndef generate_page_summary(page_text):\n\"\"\"\nGenerate a concise summary of a page.\nArgs:\npage_text (str): Text content of the page\nReturns:\nstr: Generated summary\n\"\"\"\n# Define the system prompt to instruct the summarization model\nsystem_prompt = \"\"\"You are an expert summarization system.\nCreate a detailed summary of the provided text.\nFocus on capturing the main topics, key information, and important facts.\nYour summary should be comprehensive enough to understand what the page contains\nbut more concise than the original.\"\"\"\n# Truncate input text if it exceeds the maximum token limit\nmax_tokens = 6000\ntruncated_text = page_text[:max_tokens] if len(page_text) > max_tokens else page_text\n# Make a request to the OpenAI API to generate the summary\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n{\"role\": \"user\", \"content\": f\"Please summarize this text:nn{truncated_text}\"}  # User message with the text to summarize\n],\ntemperature=0.3  # Set the temperature for response generation\n)\n# Return the generated summary content\nreturn response.choices[0].message.content",
  "## Hierarchical Retrieval\nCode:\ndef retrieve_hierarchically(query, summary_store, detailed_store, k_summaries=3, k_chunks=5):\n\"\"\"\nRetrieve information using hierarchical indices.\nArgs:\nquery (str): User query\nsummary_store (SimpleVectorStore): Store of document summaries\ndetailed_store (SimpleVectorStore): Store of detailed chunks\nk_summaries (int): Number of summaries to retrieve\nk_chunks (int): Number of chunks to retrieve per summary\nReturns:\nList[Dict]: Retrieved chunks with relevance scores\n\"\"\"\nprint(f\"Performing hierarchical retrieval for query: {query}\")\n# Create query embedding\nquery_embedding = create_embeddings(query)\n# First, retrieve relevant summaries\nsummary_results = summary_store.similarity_search(\nquery_embedding,\nk=k_summaries\n)\nprint(f\"Retrieved {len(summary_results)} relevant summaries\")\n# Collect pages from relevant summaries\nrelevant_pages = [result[\"metadata\"][\"page\"] for result in summary_results]\n# Create a filter function to only keep chunks from relevant pages\ndef page_filter(metadata):\nreturn metadata[\"page\"] in relevant_pages\n# Then, retrieve detailed chunks from only those relevant pages\ndetailed_results = detailed_store.similarity_search(\nquery_embedding,\nk=k_chunks * len(relevant_pages),\nfilter_func=page_filter\n)\nprint(f\"Retrieved {len(detailed_results)} detailed chunks from relevant pages\")",
  "# For each result, add which summary/page it came from\nfor result in detailed_results:\npage = result[\"metadata\"][\"page\"]\nmatching_summaries = [s for s in summary_results if s[\"metadata\"][\"page\"] == page]\nif matching_summaries:\nresult[\"summary\"] = matching_summaries[0][\"text\"]\nreturn detailed_results\n## Creating Embeddings\nCode:\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreate embeddings for the given texts.\nArgs:\ntexts (List[str]): Input texts\nmodel (str): Embedding model name\nReturns:\nList[List[float]]: Embedding vectors\n\"\"\"\n# Handle empty input\nif not texts:\nreturn []\n# Process in batches if needed (OpenAI API limits)\nbatch_size = 100\nall_embeddings = []\n# Iterate over the input texts in batches\nfor i in range(0, len(texts), batch_size):\nbatch = texts[i:i + batch_size]  # Get the current batch of texts\n# Create embeddings for the current batch\nresponse = client.embeddings.create(\nmodel=model,\ninput=batch\n)\n# Extract embeddings from the response\nbatch_embeddings = [item.embedding for item in response.data]\nall_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\nreturn all_embeddings  # Return all embeddings",
  "## Evaluation Functions\nCode:\ndef compare_approaches(query, pdf_path, reference_answer=None):\n\"\"\"\nCompare hierarchical and standard RAG approaches.\nArgs:\nquery (str): User query\npdf_path (str): Path to the PDF document\nreference_answer (str, optional): Reference answer for evaluation\nReturns:\nDict: Comparison results\n\"\"\"\nprint(f\"n=== Comparing RAG approaches for query: {query} ===\")\n# Run hierarchical RAG\nprint(\"nRunning hierarchical RAG...\")\nhierarchical_result = hierarchical_rag(query, pdf_path)\nhier_response = hierarchical_result[\"response\"]\n# Run standard RAG\nprint(\"nRunning standard RAG...\")\nstandard_result = standard_rag(query, pdf_path)\nstd_response = standard_result[\"response\"]\n# Compare results from hierarchical and standard RAG\ncomparison = compare_responses(query, hier_response, std_response, reference_answer)",
  "# Return a dictionary with the comparison results\nreturn {\n\"query\": query,  # The original query\n\"hierarchical_response\": hier_response,  # Response from hierarchical RAG\n\"standard_response\": std_response,  # Response from standard RAG\n\"reference_answer\": reference_answer,  # Reference answer for evaluation\n\"comparison\": comparison,  # Comparison analysis\n\"hierarchical_chunks_count\": len(hierarchical_result[\"retrieved_chunks\"]),  # Number of chunks retrieved by hierarchical RAG\n\"standard_chunks_count\": len(standard_result[\"retrieved_chunks\"])  # Number of chunks retrieved by standard RAG\n}\n## Document Processing Functions\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtract text content from a PDF file with page separation.\nArgs:\npdf_path (str): Path to the PDF file\nReturns:\nList[Dict]: List of pages with text content and metadata\n\"\"\"\nprint(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\npdf = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\npages = []  # Initialize an empty list to store the pages with text content\n# Iterate over each page in the PDF\nfor page_num in range(len(pdf)):\npage = pdf[page_num]  # Get the current page\ntext = page.get_text()  # Extract text from the current page",
  "# Skip pages with very little text (less than 50 characters)\nif len(text.strip()) > 50:\n# Append the page text and metadata to the list\npages.append({\n\"text\": text,\n\"metadata\": {\n\"source\": pdf_path,  # Source file path\n\"page\": page_num + 1  # Page number (1-based index)\n}\n})\nprint(f\"Extracted {len(pages)} pages with content\")  # Print the number of pages extracted\nreturn pages  # Return the list of pages with text content and metadata\n## Evaluation of Hierarchical and Standard RAG Approaches\nCode:\n# Path to the PDF document containing AI information\npdf_path = \"data/AI_Information.pdf\"\n# Example query about AI for testing the hierarchical RAG approach\nquery = \"What are the key applications of transformer models in natural language processing?\"\nresult = hierarchical_rag(query, pdf_path)\nprint(\"n=== Response ===\")\nprint(result[\"response\"])\n# Test query for formal evaluation (using only one query as requested)\ntest_queries = [\n\"How do transformers handle sequential data compared to RNNs?\"\n]",
  "# Reference answer for the test query to enable comparison\nreference_answers = [\n\"Transformers handle sequential data differently from RNNs by using self-attention mechanisms instead of recurrent connections. This allows transformers to process all tokens in parallel rather than sequentially, capturing long-range dependencies more efficiently and enabling better parallelization during training. Unlike RNNs, transformers don't suffer from vanishing gradient problems with long sequences.\"\n]\n# Run the evaluation comparing hierarchical and standard RAG approaches\nevaluation_results = run_evaluation(\npdf_path=pdf_path,\ntest_queries=test_queries,\nreference_answers=reference_answers\n)\n# Print the overall analysis of the comparison\nprint(\"n=== OVERALL ANALYSIS ===\")\nprint(evaluation_results[\"overall_analysis\"])",
  "## Simple Vector Store Implementation\nCode:\nclass SimpleVectorStore:\n\"\"\"\nA simple vector store implementation using NumPy.\n\"\"\"\ndef __init__(self):\nself.vectors = []  # List to store vector embeddings\nself.texts = []  # List to store text content\nself.metadata = []  # List to store metadata\ndef add_item(self, text, embedding, metadata=None):\n\"\"\"\nAdd an item to the vector store.\nArgs:\ntext (str): Text content\nembedding (List[float]): Vector embedding\nmetadata (Dict, optional): Additional metadata\n\"\"\"\nself.vectors.append(np.array(embedding))  # Append the embedding as a numpy array\nself.texts.append(text)  # Append the text content\nself.metadata.append(metadata or {})  # Append the metadata or an empty dict if None\ndef similarity_search(self, query_embedding, k=5, filter_func=None):\n\"\"\"\nFind the most similar items to a query embedding.\nArgs:\nquery_embedding (List[float]): Query embedding vector\nk (int): Number of results to return\nfilter_func (callable, optional): Function to filter results\nReturns:\nList[Dict]: Top k most similar items\n\"\"\"\nif not self.vectors:\nreturn []  # Return an empty list if there are no vectors\n# Convert query embedding to numpy array\nquery_vector = np.array(query_embedding)\n# Calculate similarities using cosine similarity\nsimilarities = []\nfor i, vector in enumerate(self.vectors):",
  "# Skip if doesn't pass the filter\nif filter_func and not filter_func(self.metadata[i]):\ncontinue\n# Calculate cosine similarity\nsimilarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\nsimilarities.append((i, similarity))  # Append index and similarity score\n# Sort by similarity (descending)\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n# Return top k results\nresults = []\nfor i in range(min(k, len(similarities))):\nidx, score = similarities[i]\nresults.append({\n\"text\": self.texts[idx],  # Add the text content\n\"metadata\": self.metadata[idx],  # Add the metadata\n\"similarity\": float(score)  # Add the similarity score\n})\nreturn results  # Return the list of top k results\n## Setting Up the Environment\nCode:\nimport os\nimport numpy as np\nimport json\nimport fitz\nfrom openai import OpenAI\nimport re\nimport pickle\n## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.\n## Setting Up the Environment\nWe begin by importing necessary libraries."
]