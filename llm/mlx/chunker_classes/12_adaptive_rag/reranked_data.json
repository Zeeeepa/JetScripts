[
  {
    "id": "944f25eb13eebbf8771cfd0f23c3a04f",
    "rank": 1,
    "doc_index": 5,
    "score": 0.607984721660614,
    "percent_difference": 0.0,
    "text": "## Chunking the Extracted Text\nOnce we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy.",
    "relevance": null,
    "word_count": 26
  },
  {
    "id": "fc525f4d8d7a82a57f0948613616fe75",
    "rank": 2,
    "doc_index": 6,
    "score": 0.4944360554218292,
    "percent_difference": 18.68,
    "text": "## Chunking the Extracted Text\nCode:\ndef chunk_text(text, n, overlap):\n\"\"\"\nChunks the given text into segments of n characters with overlap.\nArgs:\ntext (str): The text to be chunked.\nn (int): The number of characters in each chunk.\noverlap (int): The number of overlapping characters between chunks.\nReturns:\nList[str]: A list of text chunks.\n\"\"\"\nchunks = []  # Initialize an empty list to store the chunks\n# Loop through the text with a step size of (n - overlap)\nfor i in range(0, len(text), n - overlap):\n# Append a chunk of text from index i to i + n to the chunks list\nchunks.append(text[i:i + n])\nreturn chunks  # Return the list of text chunks",
    "relevance": null,
    "word_count": 169
  },
  {
    "id": "ba771cd99f2ef6efa684970b3f9cd516",
    "rank": 3,
    "doc_index": 12,
    "score": 0.3963159720102946,
    "percent_difference": 34.81,
    "text": "## Document Processing Pipeline\nCode:\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n\"\"\"\nProcess a document for use with adaptive retrieval.\nArgs:\npdf_path (str): Path to the PDF file.\nchunk_size (int): Size of each chunk in characters.\nchunk_overlap (int): Overlap between chunks in characters.\nReturns:\nTuple[List[str], SimpleVectorStore]: Document chunks and vector store.\n\"\"\"\n# Extract text from the PDF file\nprint(\"Extracting text from PDF...\")\nextracted_text = extract_text_from_pdf(pdf_path)\n# Chunk the extracted text\nprint(\"Chunking text...\")\nchunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\nprint(f\"Created {len(chunks)} text chunks\")\n# Create embeddings for the text chunks\nprint(\"Creating embeddings for chunks...\")\nchunk_embeddings = create_embeddings(chunks)\n# Initialize the vector store\nstore = SimpleVectorStore()\n# Add each chunk and its embedding to the vector store with metadata\nfor i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\nstore.add_item(\ntext=chunk,\nembedding=embedding,\nmetadata={\"index\": i, \"source\": pdf_path}\n)\nprint(f\"Added {len(chunks)} chunks to the vector store\")\n# Return the chunks and the vector store\nreturn chunks, store",
    "relevance": null,
    "word_count": 262
  },
  {
    "id": "24c1dfb8777ca12fcdb93c880aa64101",
    "rank": 4,
    "doc_index": 9,
    "score": 0.36182302236557007,
    "percent_difference": 40.49,
    "text": "## Simple Vector Store Implementation\nWe'll create a basic vector store to manage document chunks and their embeddings.",
    "relevance": null,
    "word_count": 21
  },
  {
    "id": "ddc12ca461fde46206b59ce349e18085",
    "rank": 5,
    "doc_index": 3,
    "score": 0.2566831111907959,
    "percent_difference": 57.78,
    "text": "## Extracting Text from a PDF File\nTo implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library.",
    "relevance": null,
    "word_count": 37
  },
  {
    "id": "672fb68c0d3edb0dfcd8f3e255f9d3ab",
    "rank": 6,
    "doc_index": 22,
    "score": 0.20639917502800623,
    "percent_difference": 66.05,
    "text": "## Complete RAG Pipeline with Adaptive Retrieval\nCode:\ndef rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n\"\"\"\nComplete RAG pipeline with adaptive retrieval.\nArgs:\npdf_path (str): Path to PDF document\nquery (str): User query\nk (int): Number of documents to retrieve\nuser_context (str): Optional user context\nReturns:\nDict: Results including query, retrieved documents, query type, and response\n\"\"\"\nprint(\"n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\nprint(f\"Query: {query}\")\n# Process the document to extract text, chunk it, and create embeddings\nchunks, vector_store = process_document(pdf_path)\n# Classify the query to determine its type\nquery_type = classify_query(query)\nprint(f\"Query classified as: {query_type}\")\n# Retrieve documents using the adaptive retrieval strategy based on the query type\nretrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n# Generate a response based on the query, retrieved documents, and query type\nresponse = generate_response(query, retrieved_docs, query_type)\n# Compile the results into a dictionary\nresult = {\n\"query\": query,\n\"query_type\": query_type,\n\"retrieved_documents\": retrieved_docs,\n\"response\": response\n}\nprint(\"n=== RESPONSE ===\")\nprint(response)\nreturn result",
    "relevance": null,
    "word_count": 256
  },
  {
    "id": "9eedf73b166ccdc909218eba6e69dd72",
    "rank": 7,
    "doc_index": 4,
    "score": 0.18455010652542114,
    "percent_difference": 69.65,
    "text": "## Extracting Text from a PDF File\nCode:\ndef extract_text_from_pdf(pdf_path):\n\"\"\"\nExtracts text from a PDF file and prints the first `num_chars` characters.\nArgs:\npdf_path (str): Path to the PDF file.\nReturns:\nstr: Extracted text from the PDF.\n\"\"\"\n# Open the PDF file\nmypdf = fitz.open(pdf_path)\nall_text = \"\"  # Initialize an empty string to store the extracted text\n# Iterate through each page in the PDF\nfor page_num in range(mypdf.page_count):\npage = mypdf[page_num]  # Get the page\ntext = page.get_text(\"text\")  # Extract text from the page\nall_text += text  # Append the extracted text to the all_text string\nreturn all_text  # Return the extracted text",
    "relevance": null,
    "word_count": 143
  },
  {
    "id": "d9f885ef3b3c1daf1a8b996fa4fac27a",
    "rank": 8,
    "doc_index": 14,
    "score": 0.1724497377872467,
    "percent_difference": 71.64,
    "text": "## Implementing Specialized Retrieval Strategies\n### 1. Factual Strategy - Focus on Precision",
    "relevance": null,
    "word_count": 17
  },
  {
    "id": "9be6e6ab8587a94c4ce96aa98187b3b8",
    "rank": 9,
    "doc_index": 11,
    "score": 0.16910908371210098,
    "percent_difference": 72.19,
    "text": "## Creating Embeddings\nCode:\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n\"\"\"\nCreates embeddings for the given text.\nArgs:\ntext (str or List[str]): The input text(s) for which embeddings are to be created.\nmodel (str): The model to be used for creating embeddings.\nReturns:\nList[float] or List[List[float]]: The embedding vector(s).\n\"\"\"\n# Handle both string and list inputs by converting string input to a list\ninput_text = text if isinstance(text, list) else [text]\n# Create embeddings for the input text using the specified model\nresponse = client.embeddings.create(\nmodel=model,\ninput=input_text\n)\n# If the input was a single string, return just the first embedding\nif isinstance(text, str):\nreturn response.data[0].embedding\n# Otherwise, return all embeddings for the list of texts\nreturn [item.embedding for item in response.data]",
    "relevance": null,
    "word_count": 186
  },
  {
    "id": "bb23cfcbae7fc4fc6da6f160ee38e2f9",
    "rank": 10,
    "doc_index": 0,
    "score": 0.16677996516227722,
    "percent_difference": 72.57,
    "text": "# Adaptive Retrieval for Enhanced RAG Systems\nIn this notebook, I implement an Adaptive Retrieval system that dynamically selects the most appropriate retrieval strategy based on the type of query. This approach significantly enhances our RAG system's ability to provide accurate and relevant responses across a diverse range of questions.\nDifferent questions demand different retrieval strategies. Our system:\n1. Classifies the query type (Factual, Analytical, Opinion, or Contextual)\n2. Selects the appropriate retrieval strategy\n3. Executes specialized retrieval techniques\n4. Generates a tailored response",
    "relevance": null,
    "word_count": 99
  },
  {
    "id": "e9e53f2200be12467a1ef32f7f1b9214",
    "rank": 11,
    "doc_index": 15,
    "score": 0.1346165438493093,
    "percent_difference": 77.86,
    "text": "## Implementing Specialized Retrieval Strategies\nCode:\ndef factual_retrieval_strategy(query, vector_store, k=4):\n\"\"\"\nRetrieval strategy for factual queries focusing on precision.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store\nk (int): Number of documents to return\nReturns:\nList[Dict]: Retrieved documents\n\"\"\"\nprint(f\"Executing Factual retrieval strategy for: '{query}'\")\n# Use LLM to enhance the query for better precision\nsystem_prompt = \"\"\"You are an expert at enhancing search queries.\nYour task is to reformulate the given factual query to make it more precise and\nspecific for information retrieval. Focus on key entities and their relationships.\nProvide ONLY the enhanced query without any explanation.\n\"\"\"\nuser_prompt = f\"Enhance this factual query: {query}\"\n# Generate the enhanced query using the LLM\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0\n)\n# Extract and print the enhanced query\nenhanced_query = response.choices[0].message.content.strip()\nprint(f\"Enhanced query: {enhanced_query}\")\n# Create embeddings for the enhanced query\nquery_embedding = create_embeddings(enhanced_query)\n# Perform initial similarity search to retrieve documents\ninitial_results = vector_store.similarity_search(query_embedding, k=k*2)\n# Initialize a list to store ranked results\nranked_results = []\n# Score and rank documents by relevance using LLM\nfor doc in initial_results:\nrelevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\nranked_results.append({\n\"text\": doc[\"text\"],\n\"metadata\": doc[\"metadata\"],\n\"similarity\": doc[\"similarity\"],\n\"relevance_score\": relevance_score\n})\n# Sort the results by relevance score in descending order\nranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n# Return the top k results\nreturn ranked_results[:k]",
    "relevance": null,
    "word_count": 392
  },
  {
    "id": "8639a20bc3a0ee39d5fc2564f3d8df49",
    "rank": 12,
    "doc_index": 10,
    "score": 0.1255435824394226,
    "percent_difference": 79.35,
    "text": "## Simple Vector Store Implementation\nCode:\nclass SimpleVectorStore:\n\"\"\"\nA simple vector store implementation using NumPy.\n\"\"\"\ndef __init__(self):\n\"\"\"\nInitialize the vector store.\n\"\"\"\nself.vectors = []  # List to store embedding vectors\nself.texts = []  # List to store original texts\nself.metadata = []  # List to store metadata for each text\ndef add_item(self, text, embedding, metadata=None):\n\"\"\"\nAdd an item to the vector store.\nArgs:\ntext (str): The original text.\nembedding (List[float]): The embedding vector.\nmetadata (dict, optional): Additional metadata.\n\"\"\"\nself.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\nself.texts.append(text)  # Add the original text to texts list\nself.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\ndef similarity_search(self, query_embedding, k=5, filter_func=None):\n\"\"\"\nFind the most similar items to a query embedding.\nArgs:\nquery_embedding (List[float]): Query embedding vector.\nk (int): Number of results to return.\nfilter_func (callable, optional): Function to filter results.\nReturns:\nList[Dict]: Top k most similar items with their texts and metadata.\n\"\"\"\nif not self.vectors:\nreturn []  # Return empty list if no vectors are stored\n# Convert query embedding to numpy array\nquery_vector = np.array(query_embedding)\n# Calculate similarities using cosine similarity\nsimilarities = []\nfor i, vector in enumerate(self.vectors):\n# Apply filter if provided\nif filter_func and not filter_func(self.metadata[i]):\ncontinue\n# Calculate cosine similarity\nsimilarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\nsimilarities.append((i, similarity))  # Append index and similarity score\n# Sort by similarity (descending)\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n# Return top k results\nresults = []\nfor i in range(min(k, len(similarities))):\nidx, score = similarities[i]\nresults.append({\n\"text\": self.texts[idx],  # Add the text\n\"metadata\": self.metadata[idx],  # Add the metadata\n\"similarity\": score  # Add the similarity score\n})\nreturn results  # Return the list of top k results",
    "relevance": null,
    "word_count": 470
  },
  {
    "id": "de217d9e4416269ef1fb227727066c00",
    "rank": 13,
    "doc_index": 16,
    "score": 0.1212316632270813,
    "percent_difference": 80.06,
    "text": "### 2. Analytical Strategy - Comprehensive Coverage\nCode:\ndef analytical_retrieval_strategy(query, vector_store, k=4):\n\"\"\"\nRetrieval strategy for analytical queries focusing on comprehensive coverage.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store\nk (int): Number of documents to return\nReturns:\nList[Dict]: Retrieved documents\n\"\"\"\nprint(f\"Executing Analytical retrieval strategy for: '{query}'\")\n# Define the system prompt to guide the AI in generating sub-questions\nsystem_prompt = \"\"\"You are an expert at breaking down complex questions.\nGenerate sub-questions that explore different aspects of the main analytical query.\nThese sub-questions should cover the breadth of the topic and help retrieve\ncomprehensive information.\nReturn a list of exactly 3 sub-questions, one per line.\n\"\"\"\n# Create the user prompt with the main query\nuser_prompt = f\"Generate sub-questions for this analytical query: {query}\"\n# Generate the sub-questions using the LLM\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0.3\n)\n# Extract and clean the sub-questions\nsub_queries = response.choices[0].message.content.strip().split('n')\nsub_queries = [q.strip() for q in sub_queries if q.strip()]\nprint(f\"Generated sub-queries: {sub_queries}\")\n# Retrieve documents for each sub-query\nall_results = []\nfor sub_query in sub_queries:\n# Create embeddings for the sub-query\nsub_query_embedding = create_embeddings(sub_query)\n# Perform similarity search for the sub-query\nresults = vector_store.similarity_search(sub_query_embedding, k=2)\nall_results.extend(results)\n# Ensure diversity by selecting from different sub-query results\n# Remove duplicates (same text content)\nunique_texts = set()\ndiverse_results = []\nfor result in all_results:\nif result[\"text\"] not in unique_texts:\nunique_texts.add(result[\"text\"])\ndiverse_results.append(result)\n# If we need more results to reach k, add more from initial results\nif len(diverse_results) < k:\n# Direct retrieval for the main query\nmain_query_embedding = create_embeddings(query)\nmain_results = vector_store.similarity_search(main_query_embedding, k=k)\nfor result in main_results:\nif result[\"text\"] not in unique_texts and len(diverse_results) < k:\nunique_texts.add(result[\"text\"])\ndiverse_results.append(result)\n# Return the top k diverse results\nreturn diverse_results[:k]",
    "relevance": null,
    "word_count": 476
  },
  {
    "id": "78487b0eb8e79fcabe46ab195050ad84",
    "rank": 14,
    "doc_index": 17,
    "score": 0.11743821799755097,
    "percent_difference": 80.68,
    "text": "### 3. Opinion Strategy - Diverse Perspectives\nCode:\ndef opinion_retrieval_strategy(query, vector_store, k=4):\n\"\"\"\nRetrieval strategy for opinion queries focusing on diverse perspectives.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store\nk (int): Number of documents to return\nReturns:\nList[Dict]: Retrieved documents\n\"\"\"\nprint(f\"Executing Opinion retrieval strategy for: '{query}'\")\n# Define the system prompt to guide the AI in identifying different perspectives\nsystem_prompt = \"\"\"You are an expert at identifying different perspectives on a topic.\nFor the given query about opinions or viewpoints, identify different perspectives\nthat people might have on this topic.\nReturn a list of exactly 3 different viewpoint angles, one per line.\n\"\"\"\n# Create the user prompt with the main query\nuser_prompt = f\"Identify different perspectives on: {query}\"\n# Generate the different perspectives using the LLM\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0.3\n)\n# Extract and clean the viewpoints\nviewpoints = response.choices[0].message.content.strip().split('n')\nviewpoints = [v.strip() for v in viewpoints if v.strip()]\nprint(f\"Identified viewpoints: {viewpoints}\")\n# Retrieve documents representing each viewpoint\nall_results = []\nfor viewpoint in viewpoints:\n# Combine the main query with the viewpoint\ncombined_query = f\"{query} {viewpoint}\"\n# Create embeddings for the combined query\nviewpoint_embedding = create_embeddings(combined_query)\n# Perform similarity search for the combined query\nresults = vector_store.similarity_search(viewpoint_embedding, k=2)\n# Mark results with the viewpoint they represent\nfor result in results:\nresult[\"viewpoint\"] = viewpoint\n# Add the results to the list of all results\nall_results.extend(results)\n# Select a diverse range of opinions\n# Ensure we get at least one document from each viewpoint if possible\nselected_results = []\nfor viewpoint in viewpoints:\n# Filter documents by viewpoint\nviewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\nif viewpoint_docs:\nselected_results.append(viewpoint_docs[0])\n# Fill remaining slots with highest similarity docs\nremaining_slots = k - len(selected_results)\nif remaining_slots > 0:\n# Sort remaining docs by similarity\nremaining_docs = [r for r in all_results if r not in selected_results]\nremaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\nselected_results.extend(remaining_docs[:remaining_slots])\n# Return the top k results\nreturn selected_results[:k]",
    "relevance": null,
    "word_count": 514
  },
  {
    "id": "d0004a81a533454d72a32821ed59b5a1",
    "rank": 15,
    "doc_index": 24,
    "score": 0.11506027728319168,
    "percent_difference": 81.08,
    "text": "## Evaluating the Adaptive Retrieval System (Customized Queries)\nThe final step to use the adaptive RAG evaluation system is to call the evaluate_adaptive_vs_standard() function with your PDF document and test queries:",
    "relevance": null,
    "word_count": 37
  },
  {
    "id": "76d4c601be1c4a02ad6dd5583dc8bc3c",
    "rank": 16,
    "doc_index": 21,
    "score": 0.1046269416809082,
    "percent_difference": 82.79,
    "text": "## Response Generation\nCode:\ndef generate_response(query, results, query_type, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nGenerate a response based on query, retrieved documents, and query type.\nArgs:\nquery (str): User query\nresults (List[Dict]): Retrieved documents\nquery_type (str): Type of query\nmodel (str): LLM model\nReturns:\nstr: Generated response\n\"\"\"\n# Prepare context from retrieved documents by joining their texts with separators\ncontext = \"nn---nn\".join([r[\"text\"] for r in results])\n# Create custom system prompt based on query type\nif query_type == \"Factual\":\nsystem_prompt = \"\"\"You are a helpful assistant providing factual information.\nAnswer the question based on the provided context. Focus on accuracy and precision.\nIf the context doesn't contain the information needed, acknowledge the limitations.\"\"\"\nelif query_type == \"Analytical\":\nsystem_prompt = \"\"\"You are a helpful assistant providing analytical insights.\nBased on the provided context, offer a comprehensive analysis of the topic.\nCover different aspects and perspectives in your explanation.\nIf the context has gaps, acknowledge them while providing the best analysis possible.\"\"\"\nelif query_type == \"Opinion\":\nsystem_prompt = \"\"\"You are a helpful assistant discussing topics with multiple viewpoints.\nBased on the provided context, present different perspectives on the topic.\nEnsure fair representation of diverse opinions without showing bias.\nAcknowledge where the context presents limited viewpoints.\"\"\"\nelif query_type == \"Contextual\":\nsystem_prompt = \"\"\"You are a helpful assistant providing contextually relevant information.\nAnswer the question considering both the query and its context.\nMake connections between the query context and the information in the provided documents.\nIf the context doesn't fully address the specific situation, acknowledge the limitations.\"\"\"\nelse:\nsystem_prompt = \"\"\"You are a helpful assistant. Answer the question based on the provided context. If you cannot answer from the context, acknowledge the limitations.\"\"\"\n# Create user prompt by combining the context and the query\nuser_prompt = f\"\"\"\nContext:\n{context}\nQuestion: {query}\nPlease provide a helpful response based on the context.\n\"\"\"\n# Generate response using the OpenAI client\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0.2\n)\n# Return the generated response content\nreturn response.choices[0].message.content",
    "relevance": null,
    "word_count": 500
  },
  {
    "id": "ae11f0e06f15f7bdc96083aa4c47350e",
    "rank": 17,
    "doc_index": 23,
    "score": 0.09495666474103928,
    "percent_difference": 84.38,
    "text": "## Evaluation Framework\nCode:\ndef evaluate_adaptive_vs_standard(pdf_path, test_queries, reference_answers=None):\n\"\"\"\nCompare adaptive retrieval with standard retrieval on a set of test queries.\nThis function processes a document, runs both standard and adaptive retrieval methods\non each test query, and compares their performance. If reference answers are provided,\nit also evaluates the quality of responses against these references.\nArgs:\npdf_path (str): Path to PDF document to be processed as the knowledge source\ntest_queries (List[str]): List of test queries to evaluate both retrieval methods\nreference_answers (List[str], optional): Reference answers for evaluation metrics\nReturns:\nDict: Evaluation results containing individual query results and overall comparison\n\"\"\"\nprint(\"=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\")\n# Process document to extract text, create chunks and build the vector store\nchunks, vector_store = process_document(pdf_path)\n# Initialize collection for storing comparison results\nresults = []\n# Process each test query with both retrieval methods\nfor i, query in enumerate(test_queries):\nprint(f\"nnQuery {i+1}: {query}\")\n# --- Standard retrieval approach ---\nprint(\"n--- Standard Retrieval ---\")\n# Create embedding for the query\nquery_embedding = create_embeddings(query)\n# Retrieve documents using simple vector similarity\nstandard_docs = vector_store.similarity_search(query_embedding, k=4)\n# Generate response using a generic approach\nstandard_response = generate_response(query, standard_docs, \"General\")\n# --- Adaptive retrieval approach ---\nprint(\"n--- Adaptive Retrieval ---\")\n# Classify the query to determine its type (Factual, Analytical, Opinion, Contextual)\nquery_type = classify_query(query)\n# Retrieve documents using the strategy appropriate for this query type\nadaptive_docs = adaptive_retrieval(query, vector_store, k=4)\n# Generate a response tailored to the query type\nadaptive_response = generate_response(query, adaptive_docs, query_type)\n# Store complete results for this query\nresult = {\n\"query\": query,\n\"query_type\": query_type,\n\"standard_retrieval\": {\n\"documents\": standard_docs,\n\"response\": standard_response\n},\n\"adaptive_retrieval\": {\n\"documents\": adaptive_docs,\n\"response\": adaptive_response\n}\n}\n# Add reference answer if available for this query\nif reference_answers and i < len(reference_answers):\nresult[\"reference_answer\"] = reference_answers[i]\nresults.append(result)\n# Display preview of both responses for quick comparison\nprint(\"n--- Responses ---\")\nprint(f\"Standard: {standard_response[:200]}...\")\nprint(f\"Adaptive: {adaptive_response[:200]}...\")\n# Calculate comparative metrics if reference answers are available\nif reference_answers:\ncomparison = compare_responses(results)\nprint(\"n=== EVALUATION RESULTS ===\")\nprint(comparison)\n# Return the complete evaluation results\nreturn {\n\"results\": results,\n\"comparison\": comparison if reference_answers else \"No reference answers provided for evaluation\"\n}",
    "relevance": null,
    "word_count": 569
  },
  {
    "id": "d78888b70ce5dfbfa97988ef72565af8",
    "rank": 18,
    "doc_index": 19,
    "score": 0.07916687428951263,
    "percent_difference": 86.98,
    "text": "## Helper Functions for Document Scoring\nCode:\ndef score_document_relevance(query, document, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nScore document relevance to a query using LLM.\nArgs:\nquery (str): User query\ndocument (str): Document text\nmodel (str): LLM model\nReturns:\nfloat: Relevance score from 0-10\n\"\"\"\n# System prompt to instruct the model on how to rate relevance\nsystem_prompt = \"\"\"You are an expert at evaluating document relevance.\nRate the relevance of a document to a query on a scale from 0 to 10, where:\n0 = Completely irrelevant\n10 = Perfectly addresses the query\nReturn ONLY a numerical score between 0 and 10, nothing else.\n\"\"\"\n# Truncate document if it's too long\ndoc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n# User prompt containing the query and document preview\nuser_prompt = f\"\"\"\nQuery: {query}\nDocument: {doc_preview}\nRelevance score (0-10):\n\"\"\"\n# Generate response from the model\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0\n)\n# Extract the score from the model's response\nscore_text = response.choices[0].message.content.strip()\n# Extract numeric score using regex\nmatch = re.search(r'(d+(.d+)?)', score_text)\nif match:\nscore = float(match.group(1))\nreturn min(10, max(0, score))  # Ensure score is within 0-10\nelse:\n# Default score if extraction fails\nreturn 5.0",
    "relevance": null,
    "word_count": 329
  },
  {
    "id": "9b69f72ba54fc7c04c1a534a3333400e",
    "rank": 19,
    "doc_index": 18,
    "score": 0.07649819925427437,
    "percent_difference": 87.42,
    "text": "### 4. Contextual Strategy - User Context Integration\nCode:\ndef contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n\"\"\"\nRetrieval strategy for contextual queries integrating user context.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store\nk (int): Number of documents to return\nuser_context (str): Additional user context\nReturns:\nList[Dict]: Retrieved documents\n\"\"\"\nprint(f\"Executing Contextual retrieval strategy for: '{query}'\")\n# If no user context provided, try to infer it from the query\nif not user_context:\nsystem_prompt = \"\"\"You are an expert at understanding implied context in questions.\nFor the given query, infer what contextual information might be relevant or implied\nbut not explicitly stated. Focus on what background would help answering this query.\nReturn a brief description of the implied context.\"\"\"\nuser_prompt = f\"Infer the implied context in this query: {query}\"\n# Generate the inferred context using the LLM\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0.1\n)\n# Extract and print the inferred context\nuser_context = response.choices[0].message.content.strip()\nprint(f\"Inferred context: {user_context}\")\n# Reformulate the query to incorporate context\nsystem_prompt = \"\"\"You are an expert at reformulating questions with context.\nGiven a query and some contextual information, create a more specific query that\nincorporates the context to get more relevant information.\nReturn ONLY the reformulated query without explanation.\"\"\"\nuser_prompt = f\"\"\"\nQuery: {query}\nContext: {user_context}\nReformulate the query to incorporate this context:\"\"\"\n# Generate the contextualized query using the LLM\nresponse = client.chat.completions.create(\nmodel=\"meta-llama/Llama-3.2-3B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0\n)\n# Extract and print the contextualized query\ncontextualized_query = response.choices[0].message.content.strip()\nprint(f\"Contextualized query: {contextualized_query}\")\n# Retrieve documents based on the contextualized query\nquery_embedding = create_embeddings(contextualized_query)\ninitial_results = vector_store.similarity_search(query_embedding, k=k*2)\n# Rank documents considering both relevance and user context\nranked_results = []\nfor doc in initial_results:\n# Score document relevance considering the context\ncontext_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\nranked_results.append({\n\"text\": doc[\"text\"],\n\"metadata\": doc[\"metadata\"],\n\"similarity\": doc[\"similarity\"],\n\"context_relevance\": context_relevance\n})\n# Sort by context relevance and return top k results\nranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\nreturn ranked_results[:k]",
    "relevance": null,
    "word_count": 575
  },
  {
    "id": "eac856ebe4b8723c01a501fdc70756f8",
    "rank": 20,
    "doc_index": 20,
    "score": 0.0742046485344569,
    "percent_difference": 87.79,
    "text": "## The Core Adaptive Retriever\nCode:\ndef adaptive_retrieval(query, vector_store, k=4, user_context=None):\n\"\"\"\nPerform adaptive retrieval by selecting and executing the appropriate strategy.\nArgs:\nquery (str): User query\nvector_store (SimpleVectorStore): Vector store\nk (int): Number of documents to retrieve\nuser_context (str): Optional user context for contextual queries\nReturns:\nList[Dict]: Retrieved documents\n\"\"\"\n# Classify the query to determine its type\nquery_type = classify_query(query)\nprint(f\"Query classified as: {query_type}\")\n# Select and execute the appropriate retrieval strategy based on the query type\nif query_type == \"Factual\":\n# Use the factual retrieval strategy for precise information\nresults = factual_retrieval_strategy(query, vector_store, k)\nelif query_type == \"Analytical\":\n# Use the analytical retrieval strategy for comprehensive coverage\nresults = analytical_retrieval_strategy(query, vector_store, k)\nelif query_type == \"Opinion\":\n# Use the opinion retrieval strategy for diverse perspectives\nresults = opinion_retrieval_strategy(query, vector_store, k)\nelif query_type == \"Contextual\":\n# Use the contextual retrieval strategy, incorporating user context\nresults = contextual_retrieval_strategy(query, vector_store, k, user_context)\nelse:\n# Default to factual retrieval strategy if classification fails\nresults = factual_retrieval_strategy(query, vector_store, k)\nreturn results  # Return the retrieved documents",
    "relevance": null,
    "word_count": 258
  },
  {
    "id": "c27bd90b0da404091bab9edd1bfe7460",
    "rank": 21,
    "doc_index": 13,
    "score": 0.06447586913903554,
    "percent_difference": 89.4,
    "text": "## Query Classification\nCode:\ndef classify_query(query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n\"\"\"\nClassify a query into one of four categories: Factual, Analytical, Opinion, or Contextual.\nArgs:\nquery (str): User query\nmodel (str): LLM model to use\nReturns:\nstr: Query category\n\"\"\"\n# Define the system prompt to guide the AI's classification\nsystem_prompt = \"\"\"You are an expert at classifying questions.\nClassify the given query into exactly one of these categories:\n- Factual: Queries seeking specific, verifiable information.\n- Analytical: Queries requiring comprehensive analysis or explanation.\n- Opinion: Queries about subjective matters or seeking diverse viewpoints.\n- Contextual: Queries that depend on user-specific context.\nReturn ONLY the category name, without any explanation or additional text.\n\"\"\"\n# Create the user prompt with the query to be classified\nuser_prompt = f\"Classify this query: {query}\"\n# Generate the classification response from the AI model\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": user_prompt}\n],\ntemperature=0\n)\n# Extract and strip the category from the response\ncategory = response.choices[0].message.content.strip()\n# Define the list of valid categories\nvalid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n# Ensure the returned category is valid\nfor valid in valid_categories:\nif valid in category:\nreturn valid\n# Default to \"Factual\" if classification fails\nreturn \"Factual\"",
    "relevance": null,
    "word_count": 308
  },
  {
    "id": "cb1b2274fac9b7335bab90b6a6782638",
    "rank": 22,
    "doc_index": 25,
    "score": 0.0545782707631588,
    "percent_difference": 91.02,
    "text": "## Evaluating the Adaptive Retrieval System (Customized Queries)\nCode:\n# Path to your knowledge source document\n# This PDF file contains the information that the RAG system will use\npdf_path = \"data/AI_Information.pdf\"\n# Define test queries covering different query types to demonstrate\n# how adaptive retrieval handles various query intentions\ntest_queries = [\n\"What is Explainable AI (XAI)?\",                                              # Factual query - seeking definition/specific information\n# \"How do AI ethics and governance frameworks address potential societal impacts?\",  # Analytical query - requiring comprehensive analysis\n# \"Is AI development moving too fast for proper regulation?\",                   # Opinion query - seeking diverse perspectives\n# \"How might explainable AI help in healthcare decisions?\",                     # Contextual query - benefits from context-awareness\n]\n# Reference answers for more thorough evaluation\n# These can be used to objectively assess response quality against a known standard\nreference_answers = [\n\"Explainable AI (XAI) aims to make AI systems transparent and understandable by providing clear explanations of how decisions are made. This helps users trust and effectively manage AI technologies.\",\n# \"AI ethics and governance frameworks address potential societal impacts by establishing guidelines and principles to ensure AI systems are developed and used responsibly. These frameworks focus on fairness, accountability, transparency, and the protection of human rights to mitigate risks and promote beneficial output.5.\",\n# \"Opinions on whether AI development is moving too fast for proper regulation vary. Some argue that rapid advancements outpace regulatory efforts, leading to potential risks and ethical concerns. Others believe that innovation should continue at its current pace, with regulations evolving alongside to address emerging challenges.\",\n# \"Explainable AI can significantly aid healthcare decisions by providing transparent and understandable insights into AI-driven recommendations. This transparency helps healthcare professionals trust AI systems, make informed decisions, and improve patient output by understanding the rationale behind AI suggestions.\"\n]",
    "relevance": null,
    "word_count": 355
  },
  {
    "id": "5d26c9bc9eceddf8b8a3f6b55767c300",
    "rank": 23,
    "doc_index": 2,
    "score": 0.05382562428712845,
    "percent_difference": 91.15,
    "text": "## Setting Up the Environment\nCode:\nimport os\nimport numpy as np\nimport json\nimport fitz\nfrom openai import OpenAI\nimport re",
    "relevance": null,
    "word_count": 24
  },
  {
    "id": "ec06847c02eaf5195ff95315d2a8295a",
    "rank": 24,
    "doc_index": 7,
    "score": 0.0373288132250309,
    "percent_difference": 93.86,
    "text": "## Setting Up the OpenAI API Client\nWe initialize the OpenAI client to generate embeddings and responses.",
    "relevance": null,
    "word_count": 19
  },
  {
    "id": "53b6e66239969bc2f4e5a8dd71ab2cf1",
    "rank": 25,
    "doc_index": 1,
    "score": 0.027201125398278236,
    "percent_difference": 95.53,
    "text": "## Setting Up the Environment\nWe begin by importing necessary libraries.",
    "relevance": null,
    "word_count": 13
  }
]