{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "af775680",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/evaluation/QuestionGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f3f797ad",
      "metadata": {},
      "source": [
        "# QuestionGeneration\n",
        "\n",
        "This notebook walks through the process of generating a list of questions that could be asked about your data. This is useful for setting up an evaluation pipeline using the `FaithfulnessEvaluator` and `RelevancyEvaluator` evaluation tools."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2cfee1",
      "metadata": {},
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d4360bd7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install llama-index-llms-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "848aa824",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d6415d5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# attach to the same event-loop\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9080b39e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8d0b2364-4806-4656-81e7-3f6e4b910b5b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_Settings(_llm=Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x10b7feff0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x137eea020>, completion_to_prompt=<function default_completion_to_prompt at 0x137fd2ac0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://localhost:11434', model='llama3.1', temperature=0.0, context_window=4096, request_timeout=300.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None), _embed_model=OllamaEmbedding(model_name='nomic-embed-text', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x10b7feff0>, num_workers=None, base_url='http://localhost:11434', ollama_additional_kwargs={}), _callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x10b7feff0>, _tokenizer=None, _node_parser=SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x10b7feff0>, id_func=<function default_id_func at 0x14095b7e0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;„ÄÇÔºüÔºÅ]+[,.;„ÄÇÔºüÔºÅ]?'), _prompt_helper=None, _transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x10b7feff0>, id_func=<function default_id_func at 0x14095b7e0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;„ÄÇÔºüÔºÅ]+[,.;„ÄÇÔºüÔºÅ]?')])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.core.evaluation import DatasetGenerator, RelevancyEvaluator\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Response\n",
        "# from llama_index.llms.openai import OpenAI\n",
        "\n",
        "from jet.llm.ollama.base import initialize_ollama_settings, create_llm\n",
        "initialize_ollama_settings()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5ace450",
      "metadata": {},
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a22346f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !mkdir -p 'data/paul_graham/'\n",
        "# !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c005fa3e",
      "metadata": {},
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "834f4c8c-8c10-4f8d-bf43-444aaa1234b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"/Users/jethroestrada/Desktop/External_Projects/AI/chatbot/open-webui/backend/crewAI/docs\"\n",
        "reader = SimpleDirectoryReader(data_dir)\n",
        "documents = reader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9cc71140-d614-4696-9ade-d5bdc251d398",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jethroestrada/Desktop/External_Projects/AI/repo-libs/llama_index/llama-index-core/llama_index/core/evaluation/dataset_generation.py:200: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
            "  return cls(\n"
          ]
        }
      ],
      "source": [
        "data_generator = DatasetGenerator.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f481b532-9be2-4ec3-b551-fd44060099bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_questions \u001b[38;5;241m=\u001b[39m \u001b[43mdata_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_questions_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/External_Projects/AI/repo-libs/llama_index/llama-index-core/llama_index/core/evaluation/dataset_generation.py:315\u001b[0m, in \u001b[0;36mDatasetGenerator.generate_questions_from_nodes\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_questions_from_nodes\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates questions for each document.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magenerate_questions_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/External_Projects/AI/repo-libs/llama_index/llama-index-core/llama_index/core/async_utils.py:33\u001b[0m, in \u001b[0;36masyncio_run\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     30\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# If we're here, there's an existing loop but it's not running\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# If we can't get the event loop, we're likely in a different thread, or its already running\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Desktop/External_Projects/AI/chatbot/open-webui/.venv/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/External_Projects/AI/chatbot/open-webui/.venv/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/selectors.py:566\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "eval_questions = data_generator.generate_questions_from_nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63720bd6-c060-4cc2-8a60-a39e935ee3e6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['What were the two main things the author worked on before college?',\n",
              " 'How did the author describe their early attempts at writing short stories?',\n",
              " 'What type of computer did the author first work on for programming?',\n",
              " 'What language did the author use for programming on the IBM 1401?',\n",
              " \"What was the author's experience with programming on the 1401?\",\n",
              " 'What type of computer did the author eventually get for themselves?',\n",
              " \"What was the author's initial plan for college?\",\n",
              " 'What made the author change their mind about studying philosophy?',\n",
              " \"What sparked the author's interest in AI?\",\n",
              " 'What did the author realize about AI during their first year of grad school?',\n",
              " 'What were the two art schools that the author applied to?',\n",
              " 'How did the author end up at RISD?',\n",
              " 'What was the purpose of the foundation classes at RISD?',\n",
              " 'How did the author manage to pass the entrance exam for the Accademia di Belli Arti?',\n",
              " 'What was the arrangement between the students and faculty at the Accademia?',\n",
              " \"What was the author's experience painting still lives in Florence?\",\n",
              " 'What did the author learn about visual perception while painting still lives?',\n",
              " 'Why did the author decide to leave the Accademia and return to the US?',\n",
              " 'What did the author learn about technology companies while working at Interleaf?',\n",
              " 'What lesson did the author learn about the low end and high end in the software industry?',\n",
              " \"What was the author's motivation for writing another book on Lisp?\",\n",
              " 'How did the author come up with the idea for starting a company to put art galleries online?',\n",
              " 'What was the initial reaction of art galleries to the idea of being online?',\n",
              " 'How did the author and his team come up with the concept of a web app?',\n",
              " 'What were the three main parts of the software developed by the author and his team?',\n",
              " 'How did the author and his team learn about retail and improve their software based on user feedback?',\n",
              " 'Why did the author initially believe that the absolute number of users was the most important factor for a startup?',\n",
              " \"What was the growth rate of the author's company and why was it significant?\",\n",
              " \"How did the author's decision to hire more people impact the financial stability of the company?\",\n",
              " \"What was the outcome of the company's acquisition by Yahoo in 1998?\",\n",
              " \"What was the author's initial reaction when Yahoo bought their startup?\",\n",
              " \"How did the author's lifestyle change after Yahoo bought their startup?\",\n",
              " 'Why did the author leave Yahoo and what did they plan to do?',\n",
              " \"What was the author's experience like when they returned to New York after becoming rich?\",\n",
              " 'What idea did the author have in the spring of 2000 and why did they decide to start a new company?',\n",
              " \"Why did the author decide to build a subset of the new company's vision as an open source project?\",\n",
              " \"How did the author's perception of publishing essays change with the advent of the internet?\",\n",
              " \"What is the author's perspective on working on things that are not prestigious?\",\n",
              " 'What other projects did the author work on besides writing essays?',\n",
              " 'What type of building did the author buy in Cambridge?',\n",
              " \"What was the concept behind the big party at the narrator's house in October 2003?\",\n",
              " \"How did Jessica Livingston's perception of startups change after meeting friends of the narrator?\",\n",
              " 'What were some of the ideas that the narrator shared with Jessica about fixing venture capital?',\n",
              " 'How did the idea of starting their own investment firm come about for the narrator and Jessica?',\n",
              " 'What was the Summer Founders Program and how did it attract applicants?',\n",
              " \"How did Y Combinator's batch model help solve the problem of isolation for startup founders?\",\n",
              " \"What advantages did YC's scale bring, both in terms of community and customer acquisition?\",\n",
              " 'Why did the narrator consider Hacker News to be a source of stress?',\n",
              " \"How did the narrator's role in YC differ from other types of work they had done?\",\n",
              " 'What advice did Robert Morris offer the narrator during his visit in 2010?',\n",
              " 'What was the advice given to the author by Rtm regarding their involvement with Y Combinator?',\n",
              " 'Why did the author decide to hand over Y Combinator to someone else?',\n",
              " \"What event in the author's personal life prompted them to reevaluate their priorities?\",\n",
              " 'How did the author spend most of 2014?',\n",
              " 'What project did the author work on from March 2015 to October 2019?',\n",
              " 'How did the author manage to write an interpreter for Lisp in itself?',\n",
              " \"What was the author's experience like living in England?\",\n",
              " \"When was the author's project, Bel, finally finished?\",\n",
              " 'What did the author do during the fall of 2019?',\n",
              " \"How would you describe the author's journey and decision-making process throughout the document?\",\n",
              " \"How did the author's experience with editing Lisp expressions differ from traditional app editing?\",\n",
              " 'Why did the author receive negative comments when claiming that Lisp was better than other languages?',\n",
              " 'What is the difference between putting something online and publishing it online?',\n",
              " 'How did the customs of venture capital practice and essay writing reflect outdated constraints?',\n",
              " 'Why did Y Combinator change its name to avoid a regional association?',\n",
              " \"What was the significance of the orange color chosen for Y Combinator's logo?\",\n",
              " 'Why did Y Combinator become a fund for a couple of years before returning to self-funding?',\n",
              " 'What is the purpose of Y Combinator in relation to the concept of \"deal flow\"?',\n",
              " 'How did the combination of running a forum and writing essays lead to a problem for the author?',\n",
              " \"What was the author's biggest regret about leaving Y Combinator?\"]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b98f89-d5b8-4d29-92f6-ad76d5060e9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# gpt-4\n",
        "gpt4 = create_llm(temperature=0, model=\"llama3.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eb3e616-64e5-4bf4-a67b-661e9b3657e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator_gpt4 = RelevancyEvaluator(llm=gpt4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f0e53f-77a6-40d5-94ae-3f81b01af75c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create vector index\n",
        "vector_index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af730b2e-6949-4865-b7af-bb2bc60a9173",
      "metadata": {},
      "outputs": [],
      "source": [
        "# define jupyter display function\n",
        "def display_eval_df(query: str, response: Response, eval_result: str) -> None:\n",
        "    eval_df = pd.DataFrame(\n",
        "        {\n",
        "            \"Query\": query,\n",
        "            \"Response\": str(response),\n",
        "            \"Source\": (\n",
        "                response.source_nodes[0].node.get_content()[:1000] + \"...\"\n",
        "            ),\n",
        "            \"Evaluation Result\": eval_result,\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "    eval_df = eval_df.style.set_properties(\n",
        "        **{\n",
        "            \"inline-size\": \"600px\",\n",
        "            \"overflow-wrap\": \"break-word\",\n",
        "        },\n",
        "        subset=[\"Response\", \"Source\"]\n",
        "    )\n",
        "    display(eval_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "180a5d2e-9286-477b-9cd0-a5976d18d845",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine()\n",
        "response_vector = query_engine.query(eval_questions[1])\n",
        "eval_result = evaluator_gpt4.evaluate_response(\n",
        "    query=eval_questions[1], response=response_vector\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c764b8b3-69b1-4ac8-b88b-3f9e204b8bfb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_138fa_row0_col1, #T_138fa_row0_col2 {\n",
              "  inline-size: 600px;\n",
              "  overflow-wrap: break-word;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_138fa\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_138fa_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
              "      <th id=\"T_138fa_level0_col1\" class=\"col_heading level0 col1\" >Response</th>\n",
              "      <th id=\"T_138fa_level0_col2\" class=\"col_heading level0 col2\" >Source</th>\n",
              "      <th id=\"T_138fa_level0_col3\" class=\"col_heading level0 col3\" >Evaluation Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_138fa_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_138fa_row0_col0\" class=\"data row0 col0\" >How did the author describe their early attempts at writing short stories?</td>\n",
              "      <td id=\"T_138fa_row0_col1\" class=\"data row0 col1\" >The author described their early attempts at writing short stories as awful. They mentioned that their stories had hardly any plot and were mostly about characters with strong feelings, which they thought made the stories deep.</td>\n",
              "      <td id=\"T_138fa_row0_col2\" class=\"data row0 col2\" >What I Worked On\n",
              "\n",
              "February 2021\n",
              "\n",
              "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
              "\n",
              "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines ‚Äî CPU, disk drives, printer, card reader ‚Äî sitting up on a raised floor under bright fluorescent lights.\n",
              "\n",
              "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the...</td>\n",
              "      <td id=\"T_138fa_row0_col3\" class=\"data row0 col3\" >YES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fcb78d7f130>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_eval_df(eval_questions[1], response_vector, eval_result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
