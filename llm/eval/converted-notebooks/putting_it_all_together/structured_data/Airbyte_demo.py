from llama_index import VectorStoreIndex
from llama_index.objects import (
    SQLTableNodeMapping,
    ObjectIndex,
    SQLTableSchema,
)
from llama_index.indices.struct_store.sql_query import (
    SQLTableRetrieverQueryEngine,
)
from IPython.display import Markdown, display
from llama_index.indices.struct_store.sql_query import NLSQLTableQueryEngine
from llama_index import SQLDatabase
from sqlalchemy import select, create_engine, MetaData, Table
from IPython.display import Image
from jet.logger import logger
from jet.llm.ollama.base import initialize_ollama_settings
initialize_ollama_settings()

# Airbyte SQL Index Guide
#
# We will show how to generate SQL queries on a Snowflake db generated by Airbyte.


# Airbyte ingestion

# Here we show how to ingest data from Github into a Snowflake db using Airbyte.


Image(filename="img/airbyte_1.png")

# Let's create a new connection. Here we will be dumping our Zendesk tickets into a Snowflake db.

Image(filename="img/github_1.png")

Image(filename="img/github_2.png")

Image(filename="img/snowflake_1.png")

Image(filename="img/snowflake_2.png")

# Choose the streams you want to sync.

Image(filename="img/airbyte_7.png")

Image(filename="img/github_3.png")

# Sync your data.

Image(filename="img/airbyte_9.png")

Image(filename="img/airbyte_8.png")

# Snowflake-SQLAlchemy version fix

# Hack to make snowflake-sqlalchemy work despite incompatible sqlalchemy versions
#
# Taken from https://github.com/snowflakedb/snowflake-sqlalchemy/issues/380#issuecomment-1470762025


def snowflake_sqlalchemy_20_monkey_patches():
    import sqlalchemy.util.compat

    sqlalchemy.util.compat.string_types = (str,)
    sqlalchemy.types.String.RETURNS_UNICODE = True

    import snowflake.sqlalchemy.snowdialect

    snowflake.sqlalchemy.snowdialect.SnowflakeDialect.returns_unicode_strings = (
        True
    )

    import snowflake.sqlalchemy.snowdialect

    def has_table(self, connection, table_name, schema=None, info_cache=None):
        """
        Checks if the table exists
        """
        return self._has_object(connection, "TABLE", table_name, schema)

    snowflake.sqlalchemy.snowdialect.SnowflakeDialect.has_table = has_table


try:
    snowflake_sqlalchemy_20_monkey_patches()
except Exception as e:
    raise ValueError("Please run `pip install snowflake-sqlalchemy`")

# Define database
#
# We pass the Snowflake uri to the SQL db constructor

snowflake_uri = "snowflake://<user_login_name>:<password>@<account_identifier>/<database_name>/<schema_name>?warehouse=<warehouse_name>&role=<role_name>"

# First we try connecting with sqlalchemy to check the db works.


engine = create_engine(snowflake_uri)
metadata = MetaData(bind=None)
table = Table("ZENDESK_TICKETS", metadata, autoload=True, autoload_with=engine)
stmt = select(table.columns)


with engine.connect() as connection:
    results = connection.execute(stmt).fetchone()
    print(results)
    print(results.keys())

# Define SQL DB
#
# Once we have defined the SQLDatabase, we can wrap it in a query engine to query it.
# If we know what tables we want to use we can use `NLSQLTableQueryEngine`.
# This will generate a SQL query on the specified tables.


sql_database = SQLDatabase(engine)

# Synthesize Query

# We then show a natural language query, which is translated to a SQL query under the hood with our text-to-SQL prompt.


query_engine = NLSQLTableQueryEngine(
    sql_database=sql_database,
    tables=["github_issues", "github_comments", "github_users"],
)
query_str = "Which issues have the most comments? Give the top 10 and use a join on url."
response = query_engine.query(query_str)
display(Markdown(f"<b>{response}</b>"))

query_engine = NLSQLTableQueryEngine(
    sql_database=sql_database,
    synthesize_response=False,
    tables=["github_issues", "github_comments", "github_users"],
)
response = query_engine.query(query_str)
display(Markdown(f"<b>{response}</b>"))

sql_query = response.metadata["sql_query"]
display(Markdown(f"<b>{sql_query}</b>"))

# We can also use LLM prediction to figure out what tables to use.

# We first need to create an ObjectIndex of SQLTableSchema. In this case we only pass in the table names.
# The query engine will fetch the relevant table schema at query time.


table_node_mapping = SQLTableNodeMapping(sql_database)
all_table_names = sql_database.get_usable_table_names()
table_schema_objs = []
for table_name in all_table_names:
    table_schema_objs.append(SQLTableSchema(table_name=table_name))

obj_index = ObjectIndex.from_objects(
    table_schema_objs,
    table_node_mapping,
    VectorStoreIndex,
)
table_retriever_query_engine = SQLTableRetrieverQueryEngine(
    sql_database, obj_index.as_retriever(similarity_top_k=1)
)
response = query_engine.query(query_str)

display(Markdown(f"<b>{response}</b>"))
sql_query = response.metadata["sql_query"]
display(Markdown(f"<b>{sql_query}</b>"))

logger.info("\n\n[DONE]", bright=True)
