{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG Systems\n",
    "\n",
    "Evaluation and benchmarking are crucial in developing LLM applications. Optimizing performance for applications like RAG (Retrieval Augmented Generation) requires a robust measurement mechanism.\n",
    "\n",
    "LlamaIndex provides essential modules to assess the quality of generated outputs and evaluate content retrieval quality. It categorizes its evaluation into two main types:\n",
    "\n",
    "*   **Response Evaluation** : Assesses quality of Generated Outputs\n",
    "*   **Retrieval Evaluation** : Assesses Retrieval quality\n",
    "\n",
    "[Documentation\n",
    "](https://docs.llamaindex.ai/en/latest/module_guides/evaluating/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Evaluation\n",
    "\n",
    "Evaluating results from LLMs is distinct from traditional machine learning's straightforward outcomes. LlamaIndex employs evaluation modules, using a benchmark LLM like GPT-4, to gauge answer accuracy. Notably, these modules often blend query, context, and response, minimizing the need for ground-truth labels.\n",
    "\n",
    "The evaluation modules manifest in the following categories:\n",
    "\n",
    "*   **Faithfulness:** Assesses whether the response remains true to the retrieved contexts, ensuring there's no distortion or \"hallucination.\"\n",
    "*   **Relevancy:** Evaluates the relevance of both the retrieved context and the generated answer to the initial query.\n",
    "*   **Correctness:** Determines if the generated answer aligns with the reference answer based on the query (this does require labels).\n",
    "\n",
    "Furthermore, LlamaIndex has the capability to autonomously generate questions from your data, paving the way for an evaluation pipeline to assess the RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;208mEvent: pre_start_hook\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;40mpre_start_hook triggered at: 2025-02-09|07:12:34\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from jet.llm.ollama import initialize_ollama_settings, Ollama\n",
    "initialize_ollama_settings()\n",
    "\n",
    "# Set up the root logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)  # Set logger level to INFO\n",
    "\n",
    "# Clear out any existing handlers\n",
    "logger.handlers = []\n",
    "\n",
    "# Set up the StreamHandler to output to sys.stdout (Colab's output)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)  # Set handler level to INFO\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    RetrieverEvaluator,\n",
    "    generate_question_context_pairs,\n",
    ")\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Response,\n",
    ")\n",
    "\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p 'data/paul_graham/'\n",
    "# !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"/Users/jethroestrada/Desktop/External_Projects/Jet_Projects/JetScripts/data/jet-resume/data/\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae330a961066408da7a077a2bb4174e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jethroestrada/Desktop/External_Projects/AI/repo-libs/llama_index/llama-index-core/llama_index/core/evaluation/dataset_generation.py:200: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting tokens for string: The original query is as follows: You are a Teacher/Professor. Your task is to setup                         2 questions for an upcoming                         quiz/examination. The questions should be diverse in nature                             across the document. Restrict the questions to the                                 context information provided.\n",
      "We have provided an existing answer: \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Context information is below.\n",
      "---------------------\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge.\n",
      "generate only questions based on the below query.\n",
      "You are a Teacher/Professor. Your task is to setup                         2 questions for an upcoming                         quiz/examination. The questions should be diverse in nature                             across the document. Restrict the questions to the                                 context information provided.\n",
      "\n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m513\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m2203\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m513\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mHere\u001b[0m\u001b[1m\u001b[38;5;40m are\u001b[0m\u001b[1m\u001b[38;5;40m two\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m based\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m1\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m What\u001b[0m\u001b[1m\u001b[38;5;40m was\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m name\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m social\u001b[0m\u001b[1m\u001b[38;5;40m networking\u001b[0m\u001b[1m\u001b[38;5;40m app\u001b[0m\u001b[1m\u001b[38;5;40m developed\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m individual\u001b[0m\u001b[1m\u001b[38;5;40m during\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m tenure\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m8\u001b[0m\u001b[1m\u001b[38;5;40mWeek\u001b[0m\u001b[1m\u001b[38;5;40mApp\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m(\u001b[0m\u001b[1m\u001b[38;5;40mAnswer\u001b[0m\u001b[1m\u001b[38;5;40m can\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m found\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mJan\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m9\u001b[0m\u001b[1m\u001b[38;5;40m -\u001b[0m\u001b[1m\u001b[38;5;40m Jun\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m202\u001b[0m\u001b[1m\u001b[38;5;40m0\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m section\u001b[0m\u001b[1m\u001b[38;5;40m)\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m What\u001b[0m\u001b[1m\u001b[38;5;40m programming\u001b[0m\u001b[1m\u001b[38;5;40m languages\u001b[0m\u001b[1m\u001b[38;5;40m were\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m individual\u001b[0m\u001b[1m\u001b[38;5;40m during\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m time\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m Entertainment\u001b[0m\u001b[1m\u001b[38;5;40m Gateway\u001b[0m\u001b[1m\u001b[38;5;40m Group\u001b[0m\u001b[1m\u001b[38;5;40m (\u001b[0m\u001b[1m\u001b[38;5;40mnow\u001b[0m\u001b[1m\u001b[38;5;40m Y\u001b[0m\u001b[1m\u001b[38;5;40mon\u001b[0m\u001b[1m\u001b[38;5;40mdu\u001b[0m\u001b[1m\u001b[38;5;40m)?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m(\u001b[0m\u001b[1m\u001b[38;5;40mAnswer\u001b[0m\u001b[1m\u001b[38;5;40m can\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m found\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mJun\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m -\u001b[0m\u001b[1m\u001b[38;5;40m Nov\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m4\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m section\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m406\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2609\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m9.23s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m41.49ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;82m697.00ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m8.49s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m482\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m98\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m580\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting tokens for string: The original query is as follows: Here are two questions based on the provided context:\n",
      "We have provided an existing answer: \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Context information is below.\n",
      "---------------------\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: Here are two questions based on the provided context:\n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m474\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1911\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m474\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mIt\u001b[0m\u001b[1m\u001b[38;5;40m seems\u001b[0m\u001b[1m\u001b[38;5;40m like\u001b[0m\u001b[1m\u001b[38;5;40m you\u001b[0m\u001b[1m\u001b[38;5;40m want\u001b[0m\u001b[1m\u001b[38;5;40m me\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m answer\u001b[0m\u001b[1m\u001b[38;5;40m two\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m based\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m However\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m I\u001b[0m\u001b[1m\u001b[38;5;40m don\u001b[0m\u001b[1m\u001b[38;5;40m't\u001b[0m\u001b[1m\u001b[38;5;40m see\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m your\u001b[0m\u001b[1m\u001b[38;5;40m prompt\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m Could\u001b[0m\u001b[1m\u001b[38;5;40m you\u001b[0m\u001b[1m\u001b[38;5;40m please\u001b[0m\u001b[1m\u001b[38;5;40m provide\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m related\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m resume\u001b[0m\u001b[1m\u001b[38;5;40m?\u001b[0m\u001b[1m\u001b[38;5;40m I\u001b[0m\u001b[1m\u001b[38;5;40m'll\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m happy\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m help\u001b[0m\u001b[1m\u001b[38;5;40m!\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m246\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2157\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.79s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m41.82ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;82m424.00ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.32s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m445\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m51\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m496\u001b[0m\n",
      "\n",
      "Counting tokens for string: The original query is as follows: What programming languages were used by the individual during their time at Entertainment Gateway Group (now Yondu)?\n",
      "We have provided an existing answer: \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Context information is below.\n",
      "---------------------\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What programming languages were used by the individual during their time at Entertainment Gateway Group (now Yondu)?\n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m486\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1974\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m486\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mBased\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m specifically\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m section\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mJun\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m -\u001b[0m\u001b[1m\u001b[38;5;40m Nov\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m4\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m under\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mWork\u001b[0m\u001b[1m\u001b[38;5;40m History\u001b[0m\u001b[1m\u001b[38;5;40m,\"\u001b[0m\u001b[1m\u001b[38;5;40m it\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m mentioned\u001b[0m\u001b[1m\u001b[38;5;40m that\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m individual\u001b[0m\u001b[1m\u001b[38;5;40m worked\u001b[0m\u001b[1m\u001b[38;5;40m with\u001b[0m\u001b[1m\u001b[38;5;40m Java\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m JavaScript\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m jQuery\u001b[0m\u001b[1m\u001b[38;5;40m during\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m time\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m Entertainment\u001b[0m\u001b[1m\u001b[38;5;40m Gateway\u001b[0m\u001b[1m\u001b[38;5;40m Group\u001b[0m\u001b[1m\u001b[38;5;40m (\u001b[0m\u001b[1m\u001b[38;5;40mnow\u001b[0m\u001b[1m\u001b[38;5;40m Y\u001b[0m\u001b[1m\u001b[38;5;40mon\u001b[0m\u001b[1m\u001b[38;5;40mdu\u001b[0m\u001b[1m\u001b[38;5;40m).\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m250\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2224\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m5.04s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m38.72ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;82m425.00ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.58s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m456\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m54\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m510\u001b[0m\n",
      "\n",
      "Counting tokens for string: The original query is as follows: (Answer can be found in the \"Jan 2019 - Jun 2020\" section)\n",
      "We have provided an existing answer: \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Context information is below.\n",
      "---------------------\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: (Answer can be found in the \"Jan 2019 - Jun 2020\" section)\n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m484\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1916\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m484\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mBased\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m I\u001b[0m\u001b[1m\u001b[38;5;40m will\u001b[0m\u001b[1m\u001b[38;5;40m look\u001b[0m\u001b[1m\u001b[38;5;40m for\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m answer\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m specified\u001b[0m\u001b[1m\u001b[38;5;40m section\u001b[0m\u001b[1m\u001b[38;5;40m.\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThe\u001b[0m\u001b[1m\u001b[38;5;40m section\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m###\u001b[0m\u001b[1m\u001b[38;5;40m Jan\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m9\u001b[0m\u001b[1m\u001b[38;5;40m -\u001b[0m\u001b[1m\u001b[38;5;40m Jun\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m202\u001b[0m\u001b[1m\u001b[38;5;40m0\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mAccording\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m this\u001b[0m\u001b[1m\u001b[38;5;40m section\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m answer\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m-\u001b[0m\u001b[1m\u001b[38;5;40m Task\u001b[0m\u001b[1m\u001b[38;5;40m:\u001b[0m\u001b[1m\u001b[38;5;40m Developed\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m social\u001b[0m\u001b[1m\u001b[38;5;40m networking\u001b[0m\u001b[1m\u001b[38;5;40m app\u001b[0m\u001b[1m\u001b[38;5;40m (\u001b[0m\u001b[1m\u001b[38;5;40mGrad\u001b[0m\u001b[1m\u001b[38;5;40mu\u001b[0m\u001b[1m\u001b[38;5;40mapp\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m for\u001b[0m\u001b[1m\u001b[38;5;40m students\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m parents\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m teachers\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m schools\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m The\u001b[0m\u001b[1m\u001b[38;5;40m app\u001b[0m\u001b[1m\u001b[38;5;40m serves\u001b[0m\u001b[1m\u001b[38;5;40m as\u001b[0m\u001b[1m\u001b[38;5;40m an\u001b[0m\u001b[1m\u001b[38;5;40m online\u001b[0m\u001b[1m\u001b[38;5;40m journal\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m experience\u001b[0m\u001b[1m\u001b[38;5;40m as\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m student\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m institution\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m370\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2286\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m7.52s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m37.86ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;82m429.00ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m7.05s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m455\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m83\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m538\u001b[0m\n",
      "\n",
      "Counting tokens for string: The original query is as follows: (Answer can be found in the \"Jun 2012 - Nov 2014\" section)\n",
      "We have provided an existing answer: \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Context information is below.\n",
      "---------------------\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: (Answer can be found in the \"Jun 2012 - Nov 2014\" section)\n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m484\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1916\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m484\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mBased\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m I\u001b[0m\u001b[1m\u001b[38;5;40m will\u001b[0m\u001b[1m\u001b[38;5;40m look\u001b[0m\u001b[1m\u001b[38;5;40m for\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m answer\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mJun\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m -\u001b[0m\u001b[1m\u001b[38;5;40m Nov\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m4\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m section\u001b[0m\u001b[1m\u001b[38;5;40m.\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThe\u001b[0m\u001b[1m\u001b[38;5;40m relevant\u001b[0m\u001b[1m\u001b[38;5;40m text\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m###\u001b[0m\u001b[1m\u001b[38;5;40m Jun\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m -\u001b[0m\u001b[1m\u001b[38;5;40m Nov\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m4\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m-\u001b[0m\u001b[1m\u001b[38;5;40m Entertainment\u001b[0m\u001b[1m\u001b[38;5;40m Gateway\u001b[0m\u001b[1m\u001b[38;5;40m Group\u001b[0m\u001b[1m\u001b[38;5;40m (\u001b[0m\u001b[1m\u001b[38;5;40mnow\u001b[0m\u001b[1m\u001b[38;5;40m Y\u001b[0m\u001b[1m\u001b[38;5;40mon\u001b[0m\u001b[1m\u001b[38;5;40mdu\u001b[0m\u001b[1m\u001b[38;5;40m)\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m-\u001b[0m\u001b[1m\u001b[38;5;40m Position\u001b[0m\u001b[1m\u001b[38;5;40m:\u001b[0m\u001b[1m\u001b[38;5;40m Web\u001b[0m\u001b[1m\u001b[38;5;40m Developer\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m-\u001b[0m\u001b[1m\u001b[38;5;40m Task\u001b[0m\u001b[1m\u001b[38;5;40m:\u001b[0m\u001b[1m\u001b[38;5;40m Work\u001b[0m\u001b[1m\u001b[38;5;40med\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m features\u001b[0m\u001b[1m\u001b[38;5;40m for\u001b[0m\u001b[1m\u001b[38;5;40m an\u001b[0m\u001b[1m\u001b[38;5;40m insurance\u001b[0m\u001b[1m\u001b[38;5;40m web\u001b[0m\u001b[1m\u001b[38;5;40m app\u001b[0m\u001b[1m\u001b[38;5;40m.\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m-\u001b[0m\u001b[1m\u001b[38;5;40m Key\u001b[0m\u001b[1m\u001b[38;5;40m technologies\u001b[0m\u001b[1m\u001b[38;5;40m:\u001b[0m\u001b[1m\u001b[38;5;40m Java\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m JavaScript\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m jQuery\u001b[0m\u001b[1m\u001b[38;5;40m\"\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mTherefore\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m answer\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m your\u001b[0m\u001b[1m\u001b[38;5;40m query\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mJava\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m369\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2285\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m8.56s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m37.19ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;82m420.00ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m8.10s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m455\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m95\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m550\u001b[0m\n",
      "\n",
      "Counting tokens for string: The original query is as follows: What was the name of the social networking app developed by the individual during their tenure at 8WeekApp?\n",
      "We have provided an existing answer: \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Context information is below.\n",
      "---------------------\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What was the name of the social networking app developed by the individual during their tenure at 8WeekApp?\n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m486\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1965\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m486\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mAccording\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m specifically\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mWork\u001b[0m\u001b[1m\u001b[38;5;40m History\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m section\u001b[0m\u001b[1m\u001b[38;5;40m under\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mJan\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m9\u001b[0m\u001b[1m\u001b[38;5;40m -\u001b[0m\u001b[1m\u001b[38;5;40m Jun\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m202\u001b[0m\u001b[1m\u001b[38;5;40m0\u001b[0m\u001b[1m\u001b[38;5;40m\",\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m social\u001b[0m\u001b[1m\u001b[38;5;40m networking\u001b[0m\u001b[1m\u001b[38;5;40m app\u001b[0m\u001b[1m\u001b[38;5;40m developed\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m individual\u001b[0m\u001b[1m\u001b[38;5;40m during\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m tenure\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m8\u001b[0m\u001b[1m\u001b[38;5;40mWeek\u001b[0m\u001b[1m\u001b[38;5;40mApp\u001b[0m\u001b[1m\u001b[38;5;40m was\u001b[0m\u001b[1m\u001b[38;5;40m named\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mGrad\u001b[0m\u001b[1m\u001b[38;5;40mu\u001b[0m\u001b[1m\u001b[38;5;40mapp\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:37<00:00,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;40m\".\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m223\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2188\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.69s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m34.78ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;82m429.00ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.22s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m457\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m50\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m507\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/jethroestrada/Desktop/External_Projects/AI/repo-libs/llama_index/llama-index-core/llama_index/core/evaluation/dataset_generation.py:296: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "gpt4 = Ollama(model=\"llama3.1\", temperature=0.1)\n",
    "\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents,\n",
    "    llm=gpt4,\n",
    "    show_progress=True,\n",
    "    num_questions_per_chunk=2,\n",
    ")\n",
    "\n",
    "eval_dataset = dataset_generator.generate_dataset_from_nodes(num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_queries = list(eval_dataset.queries.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are two questions based on the provided context:']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(eval_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be consistent we will fix evaluation query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_query = \"How did the author describe their early attempts at writing code?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix GPT-3.5-TURBO LLM for generating response\n",
    "gpt35 = Ollama(temperature=0, model=\"llama3.2\")\n",
    "\n",
    "# Fix GPT-4 LLM for evaluation\n",
    "gpt4 = Ollama(temperature=0, model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# create vector index\n",
    "vector_index = VectorStoreIndex.from_documents(documents, llm=gpt35)\n",
    "\n",
    "# Query engine to generate response\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_index.as_retriever(similarity_top_k=3)\n",
    "nodes = retriever.retrieve(eval_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:20px\"># Web apps\n",
       "\n",
       "## Jules Procure\n",
       "\n",
       "### Achievements\n",
       "\n",
       "- Started as the sole client side developer, built enterprise web and mobile CRM apps starting from provided mockups to production\n",
       "- JulesAI CEO was impressed and acquired ownership of existing CRM\n",
       "- Successfully integrated existing CRM with JulesAI's workflow to be rebranded as \"Jules Procure\"\n",
       "- Key features: Contact dashboard, Data builder, Task calendar, Workflow boards, Form builders, Price list generator, Automated emails based on triggers, and more\n",
       "- Technologies used: React, React Native, AWS Lambdas, GraphQL, Docker, Serverless, Jest\n",
       "\n",
       "## Digital Cities PH\n",
       "\n",
       "### Achievements\n",
       "\n",
       "- As the lead developer, I worked on a portal that showcases the profiles of provinces and cities in the Philippines\n",
       "- Developed an interactive Philippine map with clickable provinces, enabling users to access detailed descriptions and statistics for each region\n",
       "- Key features: Interactive map, Search, Filtering, Fast loading, SEO-friendly\n",
       "- Technologies used: React, GraphQL, React Static, Headless CMS\n",
       "\n",
       "## ADEC Kenya, AMDATEX\n",
       "\n",
       "### Achievements\n",
       "\n",
       "- Built UI components from mockups using Photoshop to achieve pixel-perfect look\n",
       "- Key features: Responsive, Reusable components\n",
       "- Technologies used: React, jQuery, Wordpress</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(f'<p style=\"font-size:20px\">{nodes[1].get_text()}</p>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faithfullness Evaluator\n",
    "\n",
    " Measures if the response from a query engine matches any source nodes. This is useful for measuring if the response was hallucinated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Context information is below.\n",
      "---------------------\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: How did the author describe their early attempts at writing code?\n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m316\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1240\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m316\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mThere\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m It\u001b[0m\u001b[1m\u001b[38;5;40m appears\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m there\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m narrative\u001b[0m\u001b[1m\u001b[38;5;40m or\u001b[0m\u001b[1m\u001b[38;5;40m personal\u001b[0m\u001b[1m\u001b[38;5;40m anecd\u001b[0m\u001b[1m\u001b[38;5;40mote\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m coding\u001b[0m\u001b[1m\u001b[38;5;40m journey\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m230\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m1470\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m6.91s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m38.60ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m2.56s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.00s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m283\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m48\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m331\u001b[0m\n",
      "\n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: There is no mention of the author's early attempts at writing code in the provided context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: There is no mention of the author's early attempts at writing code in the provided context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m470\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m2145\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m470\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mRepeat\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThere\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m this\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m It\u001b[0m\u001b[1m\u001b[38;5;40m appears\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m there\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m narrative\u001b[0m\u001b[1m\u001b[38;5;40m or\u001b[0m\u001b[1m\u001b[38;5;40m personal\u001b[0m\u001b[1m\u001b[38;5;40m anecd\u001b[0m\u001b[1m\u001b[38;5;40mote\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m coding\u001b[0m\u001b[1m\u001b[38;5;40m journey\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m234\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2379\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m8.16s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m18.83ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.91s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.23s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m448\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m50\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m498\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "response_vector = query_engine.query(eval_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting tokens for string: Please tell if a given piece of information is supported by the context.\n",
      "You need to answer with either YES or NO.\n",
      "Answer YES if any of the context supports the information, even if most of the context is unrelated. Some examples are provided below. \n",
      "\n",
      "Information: Apple pie is generally double-crusted.\n",
      "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples. \n",
      "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
      "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
      "Answer: YES\n",
      "Information: Apple pies tastes bad.\n",
      "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples. \n",
      "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
      "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
      "Answer: NO\n",
      "Information: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "Context: \n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Please tell if a given piece of information is supported by the context.\n",
      "You need to answer with either YES or NO.\n",
      "Answer YES if any of the context supports the information, even if most of the context is unrelated. Some examples are provided below. \n",
      "\n",
      "Information: Apple pie is generally double-crusted.\n",
      "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples. \n",
      "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
      "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
      "Answer: YES\n",
      "Information: Apple pies tastes bad.\n",
      "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples. \n",
      "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
      "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
      "Answer: NO\n",
      "Information: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "Context: \n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m453\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1839\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m453\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mYES\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m3\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m1842\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.00s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m37.45ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.87s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;82m88.00ms\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m423\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m2\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m425\u001b[0m\n",
      "\n",
      "Counting tokens for string: We want to understand if the following information is present in the context information: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have provided an existing YES/NO answer: YES\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
      "\n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: We want to understand if the following information is present in the context information: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have provided an existing YES/NO answer: YES\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
      "\n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m425\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1973\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m425\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mYES\u001b[0m\u001b[1m\u001b[38;5;40m.\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThe\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m mentions\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mStarted\u001b[0m\u001b[1m\u001b[38;5;40m as\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m sole\u001b[0m\u001b[1m\u001b[38;5;40m client\u001b[0m\u001b[1m\u001b[38;5;40m side\u001b[0m\u001b[1m\u001b[38;5;40m developer\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m which\u001b[0m\u001b[1m\u001b[38;5;40m implies\u001b[0m\u001b[1m\u001b[38;5;40m that\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m had\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m before\u001b[0m\u001b[1m\u001b[38;5;40m becoming\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m successful\u001b[0m\u001b[1m\u001b[38;5;40m developer\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m175\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2148\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m6.33s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m37.91ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.61s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m2.68s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m401\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m32\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m433\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result = faithfulness_evaluator.evaluate_response(\n",
    "    response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(query=None, contexts=['# Skills\\n\\n## Frontend\\n\\n- React\\n- React Native\\n- Vanilla JS/CSS\\n- Expo\\n- GraphQL\\n- Redux\\n- Gatsby\\n- TypeScript\\n\\n## Backend\\n\\n- Node.js\\n- Python\\n\\n## Databases\\n\\n- PostgreSQL\\n- MongoDB\\n\\n## Platforms\\n\\n- Firebase\\n- AWS\\n- Google Cloud\\n\\n## Developer Tools\\n\\n- Photoshop\\n- Jest (Unit testing)\\n- Cypress (Integration testing)\\n- Selenium (E2E testing)\\n- Git\\n- Sentry bug tracker\\n- Android Studio\\n- Xcode\\n- Fastlane\\n- Serverless\\n- ChatGPT', '# Web apps\\n\\n## Jules Procure\\n\\n### Achievements\\n\\n- Started as the sole client side developer, built enterprise web and mobile CRM apps starting from provided mockups to production\\n- JulesAI CEO was impressed and acquired ownership of existing CRM\\n- Successfully integrated existing CRM with JulesAI\\'s workflow to be rebranded as \"Jules Procure\"\\n- Key features: Contact dashboard, Data builder, Task calendar, Workflow boards, Form builders, Price list generator, Automated emails based on triggers, and more\\n- Technologies used: React, React Native, AWS Lambdas, GraphQL, Docker, Serverless, Jest\\n\\n## Digital Cities PH\\n\\n### Achievements\\n\\n- As the lead developer, I worked on a portal that showcases the profiles of provinces and cities in the Philippines\\n- Developed an interactive Philippine map with clickable provinces, enabling users to access detailed descriptions and statistics for each region\\n- Key features: Interactive map, Search, Filtering, Fast loading, SEO-friendly\\n- Technologies used: React, GraphQL, React Static, Headless CMS\\n\\n## ADEC Kenya, AMDATEX\\n\\n### Achievements\\n\\n- Built UI components from mockups using Photoshop to achieve pixel-perfect look\\n- Key features: Responsive, Reusable components\\n- Technologies used: React, jQuery, Wordpress'], response=\"**Repeat**\\n\\nThere is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\", passing=True, feedback='YES.\\n\\nThe context mentions \"Started as the sole client side developer\" which implies that the author had early attempts at writing code before becoming a successful developer.', score=1.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevency Evaluation\n",
    "\n",
    "Measures if the response + source nodes match the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RelevancyEvaluator using GPT-4 LLM\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Context information is below.\n",
      "---------------------\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: How did the author describe their early attempts at writing code?\n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m316\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1240\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m316\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mThere\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m It\u001b[0m\u001b[1m\u001b[38;5;40m appears\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m there\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m narrative\u001b[0m\u001b[1m\u001b[38;5;40m or\u001b[0m\u001b[1m\u001b[38;5;40m personal\u001b[0m\u001b[1m\u001b[38;5;40m anecd\u001b[0m\u001b[1m\u001b[38;5;40mote\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m coding\u001b[0m\u001b[1m\u001b[38;5;40m journey\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m230\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m1470\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m6.54s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m38.64ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m2.55s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.95s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m283\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m48\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m331\u001b[0m\n",
      "\n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: There is no mention of the author's early attempts at writing code in the provided context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: There is no mention of the author's early attempts at writing code in the provided context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m470\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m2145\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m470\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mRepeat\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThere\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m this\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m It\u001b[0m\u001b[1m\u001b[38;5;40m appears\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m there\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m narrative\u001b[0m\u001b[1m\u001b[38;5;40m or\u001b[0m\u001b[1m\u001b[38;5;40m personal\u001b[0m\u001b[1m\u001b[38;5;40m anecd\u001b[0m\u001b[1m\u001b[38;5;40mote\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m coding\u001b[0m\u001b[1m\u001b[38;5;40m journey\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m234\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2379\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m8.18s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m37.26ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.92s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.22s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m448\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m50\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m498\u001b[0m\n",
      "\n",
      "Counting tokens for string: We want to understand if the following query and response isin line with the context information: \n",
      " Question: How did the author describe their early attempts at writing code?\n",
      "Response: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have provided an existing YES/NO answer: \n",
      " \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
      "\n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Your task is to evaluate if the response for the query     is in line with the context information provided.\n",
      "You have two options to answer. Either YES/ NO.\n",
      "Answer - YES, if the response for the query     is in line with context information otherwise NO.\n",
      "Query and Response: \n",
      " Question: How did the author describe their early attempts at writing code?\n",
      "Response: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "Context: \n",
      " \n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m288\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1126\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m288\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mNO\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m \n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThe\u001b[0m\u001b[1m\u001b[38;5;40m response\u001b[0m\u001b[1m\u001b[38;5;40m does\u001b[0m\u001b[1m\u001b[38;5;40m not\u001b[0m\u001b[1m\u001b[38;5;40m match\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m as\u001b[0m\u001b[1m\u001b[38;5;40m there\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m The\u001b[0m\u001b[1m\u001b[38;5;40m response\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mRepeat\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m seems\u001b[0m\u001b[1m\u001b[38;5;40m unrelated\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m query\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m coding\u001b[0m\u001b[1m\u001b[38;5;40m journey\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m244\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m1370\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m6.83s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m39.27ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m2.50s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.29s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m264\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m52\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m316\u001b[0m\n",
      "\n",
      "Counting tokens for string: We want to understand if the following query and response isin line with the context information: \n",
      " Question: How did the author describe their early attempts at writing code?\n",
      "Response: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have provided an existing YES/NO answer: \n",
      " NO. \n",
      "\n",
      "The response does not match the context, as there is no mention of the author's early attempts at writing code in the provided list of skills and tools. The response \"Repeat\" seems unrelated to the query about the author's coding journey.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
      "\n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: We want to understand if the following query and response isin line with the context information: \n",
      " Question: How did the author describe their early attempts at writing code?\n",
      "Response: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have provided an existing YES/NO answer: \n",
      " NO. \n",
      "\n",
      "The response does not match the context, as there is no mention of the author's early attempts at writing code in the provided list of skills and tools. The response \"Repeat\" seems unrelated to the query about the author's coding journey.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
      "\n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m503\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m2319\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m503\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mBased\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m I\u001b[0m\u001b[1m\u001b[38;5;40m would\u001b[0m\u001b[1m\u001b[38;5;40m keep\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m existing\u001b[0m\u001b[1m\u001b[38;5;40m answer\u001b[0m\u001b[1m\u001b[38;5;40m as\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mNO\u001b[0m\u001b[1m\u001b[38;5;40m.\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThe\u001b[0m\u001b[1m\u001b[38;5;40m response\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mRepeat\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m seems\u001b[0m\u001b[1m\u001b[38;5;40m unrelated\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m query\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m there\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m coding\u001b[0m\u001b[1m\u001b[38;5;40m journey\u001b[0m\u001b[1m\u001b[38;5;40m or\u001b[0m\u001b[1m\u001b[38;5;40m experiences\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m The\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m only\u001b[0m\u001b[1m\u001b[38;5;40m provides\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m achievements\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m technologies\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m various\u001b[0m\u001b[1m\u001b[38;5;40m projects\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m not\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m personal\u001b[0m\u001b[1m\u001b[38;5;40m experiences\u001b[0m\u001b[1m\u001b[38;5;40m with\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m444\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2763\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m11.32s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m35.82ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.17s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m7.11s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m471\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m83\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m554\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "response_vector = query_engine.query(eval_query)\n",
    "\n",
    "# Evaluation\n",
    "eval_result = relevancy_evaluator.evaluate_response(\n",
    "    query=eval_query, response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How did the author describe their early attempts at writing code?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Repeat**\\n\\nThere is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevancy evaluation with multiple source nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Context information is below.\n",
      "---------------------\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: How did the author describe their early attempts at writing code?\n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m316\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1240\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m316\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mThere\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m It\u001b[0m\u001b[1m\u001b[38;5;40m appears\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m there\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m narrative\u001b[0m\u001b[1m\u001b[38;5;40m or\u001b[0m\u001b[1m\u001b[38;5;40m personal\u001b[0m\u001b[1m\u001b[38;5;40m anecd\u001b[0m\u001b[1m\u001b[38;5;40mote\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m coding\u001b[0m\u001b[1m\u001b[38;5;40m journey\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m230\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m1470\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m6.56s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m38.37ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m2.52s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.99s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m283\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m48\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m331\u001b[0m\n",
      "\n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: There is no mention of the author's early attempts at writing code in the provided context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: There is no mention of the author's early attempts at writing code in the provided context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m470\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m2145\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m470\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mRepeat\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThere\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m this\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m It\u001b[0m\u001b[1m\u001b[38;5;40m appears\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m there\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m narrative\u001b[0m\u001b[1m\u001b[38;5;40m or\u001b[0m\u001b[1m\u001b[38;5;40m personal\u001b[0m\u001b[1m\u001b[38;5;40m anecd\u001b[0m\u001b[1m\u001b[38;5;40mote\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m coding\u001b[0m\u001b[1m\u001b[38;5;40m journey\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m234\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2379\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m8.17s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m39.02ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.90s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.24s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m448\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m50\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m498\u001b[0m\n",
      "\n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: The original query is as follows: How did the author describe their early attempts at writing code?\n",
      "We have provided an existing answer: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m482\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m2158\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m482\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mRepeat\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThere\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m this\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m It\u001b[0m\u001b[1m\u001b[38;5;40m appears\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m there\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m narrative\u001b[0m\u001b[1m\u001b[38;5;40m or\u001b[0m\u001b[1m\u001b[38;5;40m personal\u001b[0m\u001b[1m\u001b[38;5;40m anecd\u001b[0m\u001b[1m\u001b[38;5;40mote\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m coding\u001b[0m\u001b[1m\u001b[38;5;40m journey\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.1, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m234\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2392\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m7.64s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m37.52ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.35s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.25s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m461\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m50\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m511\u001b[0m\n",
      "\n",
      "Counting tokens for string: We want to understand if the following query and response isin line with the context information: \n",
      " Question: How did the author describe their early attempts at writing code?\n",
      "Response: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have provided an existing YES/NO answer: \n",
      " \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
      "\n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Your task is to evaluate if the response for the query     is in line with the context information provided.\n",
      "You have two options to answer. Either YES/ NO.\n",
      "Answer - YES, if the response for the query     is in line with context information otherwise NO.\n",
      "Query and Response: \n",
      " Question: How did the author describe their early attempts at writing code?\n",
      "Response: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "Context: \n",
      " \n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m288\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1126\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m288\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mNO\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m \n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThe\u001b[0m\u001b[1m\u001b[38;5;40m response\u001b[0m\u001b[1m\u001b[38;5;40m does\u001b[0m\u001b[1m\u001b[38;5;40m not\u001b[0m\u001b[1m\u001b[38;5;40m match\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m as\u001b[0m\u001b[1m\u001b[38;5;40m there\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m no\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m The\u001b[0m\u001b[1m\u001b[38;5;40m response\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mRepeat\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m seems\u001b[0m\u001b[1m\u001b[38;5;40m unrelated\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m query\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m coding\u001b[0m\u001b[1m\u001b[38;5;40m journey\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m244\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m1370\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m6.90s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m37.62ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m2.50s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m4.36s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m264\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m52\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m316\u001b[0m\n",
      "\n",
      "Counting tokens for string: We want to understand if the following query and response isin line with the context information: \n",
      " Question: How did the author describe their early attempts at writing code?\n",
      "Response: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have provided an existing YES/NO answer: \n",
      " \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
      "\n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Your task is to evaluate if the response for the query     is in line with the context information provided.\n",
      "You have two options to answer. Either YES/ NO.\n",
      "Answer - YES, if the response for the query     is in line with context information otherwise NO.\n",
      "Query and Response: \n",
      " Question: How did the author describe their early attempts at writing code?\n",
      "Response: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "Context: \n",
      " \n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m426\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1950\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m426\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mNO\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m The\u001b[0m\u001b[1m\u001b[38;5;40m response\u001b[0m\u001b[1m\u001b[38;5;40m does\u001b[0m\u001b[1m\u001b[38;5;40m not\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m instead\u001b[0m\u001b[1m\u001b[38;5;40m appears\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m list\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m tools\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m various\u001b[0m\u001b[1m\u001b[38;5;40m projects\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m171\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2121\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m5.65s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m33.61ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m2.56s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.06s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m403\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m36\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m439\u001b[0m\n",
      "\n",
      "Counting tokens for string: We want to understand if the following query and response isin line with the context information: \n",
      " Question: How did the author describe their early attempts at writing code?\n",
      "Response: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "We have provided an existing YES/NO answer: \n",
      " \n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "\n",
      "------------\n",
      "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
      "\n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "Counting tokens for string: Your task is to evaluate if the response for the query     is in line with the context information provided.\n",
      "You have two options to answer. Either YES/ NO.\n",
      "Answer - YES, if the response for the query     is in line with context information otherwise NO.\n",
      "Query and Response: \n",
      " Question: How did the author describe their early attempts at writing code?\n",
      "Response: **Repeat**\n",
      "\n",
      "There is no mention of the author's early attempts at writing code in this context. It appears to be a list of skills and tools used by the author, but there is no narrative or personal anecdote about their coding journey.\n",
      "Context: \n",
      " \n",
      "Answer: \n",
      "Estimating tokens in tools...\n",
      "Counting tokens for string: \n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m434\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1955\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m434\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mNO\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m The\u001b[0m\u001b[1m\u001b[38;5;40m response\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mRepeat\u001b[0m\u001b[1m\u001b[38;5;40m\"\u001b[0m\u001b[1m\u001b[38;5;40m does\u001b[0m\u001b[1m\u001b[38;5;40m not\u001b[0m\u001b[1m\u001b[38;5;40m match\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m provided\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m which\u001b[0m\u001b[1m\u001b[38;5;40m describes\u001b[0m\u001b[1m\u001b[38;5;40m various\u001b[0m\u001b[1m\u001b[38;5;40m achievements\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m technologies\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m author\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m projects\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m does\u001b[0m\u001b[1m\u001b[38;5;40m not\u001b[0m\u001b[1m\u001b[38;5;40m mention\u001b[0m\u001b[1m\u001b[38;5;40m anything\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m early\u001b[0m\u001b[1m\u001b[38;5;40m attempts\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m writing\u001b[0m\u001b[1m\u001b[38;5;40m code\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m234\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2189\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m6.14s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m37.64ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m2.55s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.55s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m414\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m42\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m456\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Query Engine with similarity_top_k=3\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "# Create response\n",
    "response_vector = query_engine.query(eval_query)\n",
    "\n",
    "# Evaluate with each source node\n",
    "eval_source_result_full = [\n",
    "    relevancy_evaluator.evaluate(\n",
    "        query=eval_query,\n",
    "        response=response_vector.response,\n",
    "        contexts=[source_node.get_content()],\n",
    "    )\n",
    "    for source_node in response_vector.source_nodes\n",
    "]\n",
    "\n",
    "# Evaluation result\n",
    "eval_source_result = [\n",
    "    \"Pass\" if result.passing else \"Fail\" for result in eval_source_result_full\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fail', 'Fail', 'Fail']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_source_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctness Evaluator\n",
    "\n",
    "Evaluates the relevance and correctness of a generated answer against a reference answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_evaluator = CorrectnessEvaluator(llm=gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you explain the theory of relativity proposed by Albert Einstein in detail?\"\n",
    "\n",
    "reference = \"\"\"\n",
    "Certainly! Albert Einstein's theory of relativity consists of two main components: special relativity and general relativity. Special relativity, published in 1905, introduced the concept that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is a constant, regardless of the motion of the source or observer. It also gave rise to the famous equation E=mc², which relates energy (E) and mass (m).\n",
    "\n",
    "General relativity, published in 1915, extended these ideas to include the effects of gravity. According to general relativity, gravity is not a force between masses, as described by Newton's theory of gravity, but rather the result of the warping of space and time by mass and energy. Massive objects, such as planets and stars, cause a curvature in spacetime, and smaller objects follow curved paths in response to this curvature. This concept is often illustrated using the analogy of a heavy ball placed on a rubber sheet, causing it to create a depression that other objects (representing smaller masses) naturally move towards.\n",
    "\n",
    "In essence, general relativity provided a new understanding of gravity, explaining phenomena like the bending of light by gravity (gravitational lensing) and the precession of the orbit of Mercury. It has been confirmed through numerous experiments and observations and has become a fundamental theory in modern physics.\n",
    "\"\"\"\n",
    "\n",
    "response = \"\"\"\n",
    "Certainly! Albert Einstein's theory of relativity consists of two main components: special relativity and general relativity. Special relativity, published in 1905, introduced the concept that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is a constant, regardless of the motion of the source or observer. It also gave rise to the famous equation E=mc², which relates energy (E) and mass (m).\n",
    "\n",
    "However, general relativity, published in 1915, extended these ideas to include the effects of magnetism. According to general relativity, gravity is not a force between masses but rather the result of the warping of space and time by magnetic fields generated by massive objects. Massive objects, such as planets and stars, create magnetic fields that cause a curvature in spacetime, and smaller objects follow curved paths in response to this magnetic curvature. This concept is often illustrated using the analogy of a heavy ball placed on a rubber sheet with magnets underneath, causing it to create a depression that other objects (representing smaller masses) naturally move towards due to magnetic attraction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m837\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m3918\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m837\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40m4\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m0\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mThe\u001b[0m\u001b[1m\u001b[38;5;40m generated\u001b[0m\u001b[1m\u001b[38;5;40m answer\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m highly\u001b[0m\u001b[1m\u001b[38;5;40m relevant\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m mostly\u001b[0m\u001b[1m\u001b[38;5;40m correct\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m contains\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m significant\u001b[0m\u001b[1m\u001b[38;5;40m mistake\u001b[0m\u001b[1m\u001b[38;5;40m regarding\u001b[0m\u001b[1m\u001b[38;5;40m general\u001b[0m\u001b[1m\u001b[38;5;40m rel\u001b[0m\u001b[1m\u001b[38;5;40mativity\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m The\u001b[0m\u001b[1m\u001b[38;5;40m reference\u001b[0m\u001b[1m\u001b[38;5;40m answer\u001b[0m\u001b[1m\u001b[38;5;40m clearly\u001b[0m\u001b[1m\u001b[38;5;40m states\u001b[0m\u001b[1m\u001b[38;5;40m that\u001b[0m\u001b[1m\u001b[38;5;40m general\u001b[0m\u001b[1m\u001b[38;5;40m rel\u001b[0m\u001b[1m\u001b[38;5;40mativity\u001b[0m\u001b[1m\u001b[38;5;40m includes\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m effects\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m gravity\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m not\u001b[0m\u001b[1m\u001b[38;5;40m magnet\u001b[0m\u001b[1m\u001b[38;5;40mism\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m However\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m generated\u001b[0m\u001b[1m\u001b[38;5;40m answer\u001b[0m\u001b[1m\u001b[38;5;40m correctly\u001b[0m\u001b[1m\u001b[38;5;40m describes\u001b[0m\u001b[1m\u001b[38;5;40m special\u001b[0m\u001b[1m\u001b[38;5;40m rel\u001b[0m\u001b[1m\u001b[38;5;40mativity\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m provides\u001b[0m\u001b[1m\u001b[38;5;40m some\u001b[0m\u001b[1m\u001b[38;5;40m accurate\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m about\u001b[0m\u001b[1m\u001b[38;5;40m general\u001b[0m\u001b[1m\u001b[38;5;40m rel\u001b[0m\u001b[1m\u001b[38;5;40mativity\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m The\u001b[0m\u001b[1m\u001b[38;5;40m inclusion\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m magnet\u001b[0m\u001b[1m\u001b[38;5;40mism\u001b[0m\u001b[1m\u001b[38;5;40m instead\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m gravity\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m explanation\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m general\u001b[0m\u001b[1m\u001b[38;5;40m rel\u001b[0m\u001b[1m\u001b[38;5;40mativity\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m major\u001b[0m\u001b[1m\u001b[38;5;40m error\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m but\u001b[0m\u001b[1m\u001b[38;5;40m it\u001b[0m\u001b[1m\u001b[38;5;40m does\u001b[0m\u001b[1m\u001b[38;5;40m not\u001b[0m\u001b[1m\u001b[38;5;40m completely\u001b[0m\u001b[1m\u001b[38;5;40m undermine\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m overall\u001b[0m\u001b[1m\u001b[38;5;40m relevance\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m correctness\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m generated\u001b[0m\u001b[1m\u001b[38;5;40m answer\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m575\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m4493\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m16.44s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m37.69ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m7.31s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m9.09s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m790\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m100\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m890\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correctness_result = correctness_evaluator.evaluate(\n",
    "    query=query,\n",
    "    response=response,\n",
    "    reference=reference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(query='Can you explain the theory of relativity proposed by Albert Einstein in detail?', contexts=None, response=\"\\nCertainly! Albert Einstein's theory of relativity consists of two main components: special relativity and general relativity. Special relativity, published in 1905, introduced the concept that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is a constant, regardless of the motion of the source or observer. It also gave rise to the famous equation E=mc², which relates energy (E) and mass (m).\\n\\nHowever, general relativity, published in 1915, extended these ideas to include the effects of magnetism. According to general relativity, gravity is not a force between masses but rather the result of the warping of space and time by magnetic fields generated by massive objects. Massive objects, such as planets and stars, create magnetic fields that cause a curvature in spacetime, and smaller objects follow curved paths in response to this magnetic curvature. This concept is often illustrated using the analogy of a heavy ball placed on a rubber sheet with magnets underneath, causing it to create a depression that other objects (representing smaller masses) naturally move towards due to magnetic attraction.\\n\", passing=True, feedback='The generated answer is highly relevant and mostly correct, but contains a significant mistake regarding general relativity. The reference answer clearly states that general relativity includes the effects of gravity, not magnetism. However, the generated answer correctly describes special relativity and provides some accurate information about general relativity. The inclusion of magnetism instead of gravity in the explanation of general relativity is a major error, but it does not completely undermine the overall relevance and correctness of the generated answer.', score=4.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_result.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_result.passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The generated answer is highly relevant and mostly correct, but contains a significant mistake regarding general relativity. The reference answer clearly states that general relativity includes the effects of gravity, not magnetism. However, the generated answer correctly describes special relativity and provides some accurate information about general relativity. The inclusion of magnetism instead of gravity in the explanation of general relativity is a major error, but it does not completely undermine the overall relevance and correctness of the generated answer.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_result.feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation\n",
    "\n",
    "Evaluates the quality of any Retriever module defined in LlamaIndex.\n",
    "\n",
    "To assess the quality of a Retriever module in LlamaIndex, we use metrics like hit-rate and MRR. These compare retrieved results to ground-truth context for any question. For simpler evaluation dataset creation, we utilize synthetic data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"/Users/jethroestrada/Desktop/External_Projects/Jet_Projects/JetScripts/data/jet-resume/data\")\n",
    "documents = reader.load_data()\n",
    "\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "\n",
    "# create parser and parse document into nodes\n",
    "parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "nodes = parser(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the retriever\n",
    "retriever = vector_index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;208mCalling OllamaEmbedding embed...\u001b[0m\n",
      "\u001b[38;5;250mEmbed model:\u001b[0m \u001b[1m\u001b[38;5;45mnomic-embed-text\u001b[0m \u001b[1m\u001b[38;5;45m(768)\u001b[0m\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "\u001b[1m\u001b[38;5;45mBatch Tokens:\u001b[0m \u001b[1m\u001b[38;5;40m768\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "retrieved_nodes = retriever.retrieve(eval_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e8c7a22c-c9fe-46b3-84c2-cff4ef42a541<br>**Similarity:** 0.5197927857574212<br>**Text:** # Skills\n",
       "\n",
       "## Frontend\n",
       "\n",
       "- React\n",
       "- React Native\n",
       "- Vanilla JS/CSS\n",
       "- Expo\n",
       "- GraphQL\n",
       "- Redux\n",
       "- Gatsby\n",
       "- TypeScript\n",
       "\n",
       "## Backend\n",
       "\n",
       "- Node.js\n",
       "- Python\n",
       "\n",
       "## Databases\n",
       "\n",
       "- PostgreSQL\n",
       "- MongoDB\n",
       "\n",
       "## Platforms\n",
       "\n",
       "- Firebase\n",
       "- AWS\n",
       "- Google Cloud\n",
       "\n",
       "## Developer Tools\n",
       "\n",
       "- Photoshop\n",
       "- Jest (Unit testing)\n",
       "- Cypress (Integration testing)\n",
       "- Selenium (E2E testing)\n",
       "- Git\n",
       "- Sentry bug tracker\n",
       "- Android Studio\n",
       "- Xcode\n",
       "- Fastlane\n",
       "- Serverless\n",
       "- ChatGPT<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** c1209e55-77e4-41f3-b980-82e93fbd8b06<br>**Similarity:** 0.5173908664780609<br>**Text:** # Web apps\n",
       "\n",
       "## Jules Procure\n",
       "\n",
       "### Achievements\n",
       "\n",
       "- Started as the sole client side developer, built enterprise web and mobile CRM apps starting from provided mockups to production\n",
       "- JulesAI CEO was impressed and acquired ownership of existing CRM\n",
       "- Successfully integrated existing CRM with JulesAI's workflow to be rebranded as \"Jules Procure\"\n",
       "- Key features: Contact dashboard, Data builder, Task calendar, Workflow boards, Form builders, Price list generator, Automated emails based on triggers, and more\n",
       "- Technologies used: React, React Native, AWS Lambdas, GraphQL, Docker, Serverless, Jest\n",
       "\n",
       "## Digital Cities PH\n",
       "\n",
       "### Achievements\n",
       "\n",
       "- As the lead developer, I worked on a portal that showcases the profiles of provinces and cities in the Philippines\n",
       "- Developed an interactive Philippine map with clickable provinces, enabling users to access detailed descriptions and statistics for each region\n",
       "- Key features: Interactive map, Search, Filtering, Fast loading, SEO-friendly\n",
       "- Technologies used: React, GraphQL, React Static, Headless CMS\n",
       "\n",
       "## ADEC Kenya, AMDATEX\n",
       "\n",
       "### Achievements\n",
       "\n",
       "- Built UI components from mockups using Photoshop to achieve pixel-perfect look\n",
       "- Key features: Responsive, Reusable components\n",
       "- Technologies used: React, jQuery, Wordpress<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m436\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1857\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m436\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mHere\u001b[0m\u001b[1m\u001b[38;5;40m are\u001b[0m\u001b[1m\u001b[38;5;40m two\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m based\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m1\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhat\u001b[0m\u001b[1m\u001b[38;5;40m was\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m name\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m social\u001b[0m\u001b[1m\u001b[38;5;40m networking\u001b[0m\u001b[1m\u001b[38;5;40m app\u001b[0m\u001b[1m\u001b[38;5;40m developed\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m individual\u001b[0m\u001b[1m\u001b[38;5;40m during\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m time\u001b[0m\u001b[1m\u001b[38;5;40m at\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m8\u001b[0m\u001b[1m\u001b[38;5;40mWeek\u001b[0m\u001b[1m\u001b[38;5;40mApp\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhich\u001b[0m\u001b[1m\u001b[38;5;40m company\u001b[0m\u001b[1m\u001b[38;5;40m did\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m individual\u001b[0m\u001b[1m\u001b[38;5;40m work\u001b[0m\u001b[1m\u001b[38;5;40m for\u001b[0m\u001b[1m\u001b[38;5;40m as\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m Web\u001b[0m\u001b[1m\u001b[38;5;40m Developer\u001b[0m\u001b[1m\u001b[38;5;40m from\u001b[0m\u001b[1m\u001b[38;5;40m June\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m November\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m201\u001b[0m\u001b[1m\u001b[38;5;40m4\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:18<01:52, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;40m?\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m288\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2145\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m16.43s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m591.38ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m10.28s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m5.55s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m403\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m65\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m468\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m260\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1014\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m260\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mHere\u001b[0m\u001b[1m\u001b[38;5;40m are\u001b[0m\u001b[1m\u001b[38;5;40m two\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m based\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m1\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m What\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m J\u001b[0m\u001b[1m\u001b[38;5;40meth\u001b[0m\u001b[1m\u001b[38;5;40mro\u001b[0m\u001b[1m\u001b[38;5;40m Re\u001b[0m\u001b[1m\u001b[38;5;40muel\u001b[0m\u001b[1m\u001b[38;5;40m A\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m Estr\u001b[0m\u001b[1m\u001b[38;5;40mada\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m preferred\u001b[0m\u001b[1m\u001b[38;5;40m name\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m(\u001b[0m\u001b[1m\u001b[38;5;40mAnswer\u001b[0m\u001b[1m\u001b[38;5;40m can\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m found\u001b[0m\u001b[1m\u001b[38;5;40m under\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m\u001b[1m\u001b[38;5;40mBasic\u001b[0m\u001b[1m\u001b[38;5;40m Details\u001b[0m\u001b[1m\u001b[38;5;40m\")\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m In\u001b[0m\u001b[1m\u001b[38;5;40m which\u001b[0m\u001b[1m\u001b[38;5;40m university\u001b[0m\u001b[1m\u001b[38;5;40m did\u001b[0m\u001b[1m\u001b[38;5;40m J\u001b[0m\u001b[1m\u001b[38;5;40meth\u001b[0m\u001b[1m\u001b[38;5;40mro\u001b[0m\u001b[1m\u001b[38;5;40m Re\u001b[0m\u001b[1m\u001b[38;5;40muel\u001b[0m\u001b[1m\u001b[38;5;40m A\u001b[0m\u001b[1m\u001b[38;5;40m.\u001b[0m\u001b[1m\u001b[38;5;40m Estr\u001b[0m\u001b[1m\u001b[38;5;40mada\u001b[0m\u001b[1m\u001b[38;5;40m earn\u001b[0m\u001b[1m\u001b[38;5;40m his\u001b[0m\u001b[1m\u001b[38;5;40m BS\u001b[0m\u001b[1m\u001b[38;5;40m Degree\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m Computer\u001b[0m\u001b[1m\u001b[38;5;40m Engineering\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m(\u001b[0m\u001b[1m\u001b[38;5;40mAnswer\u001b[0m\u001b[1m\u001b[38;5;40m can\u001b[0m\u001b[1m\u001b[38;5;40m be\u001b[0m\u001b[1m\u001b[38;5;40m found\u001b[0m\u001b[1m\u001b[38;5;40m under\u001b[0m\u001b[1m\u001b[38;5;40m \"\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:28<01:08, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;40mEducation\u001b[0m\u001b[1m\u001b[38;5;40m\")\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m292\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m1306\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m8.18s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m49.72ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m2.25s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m5.87s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m238\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m70\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m308\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m377\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1742\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m377\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mHere\u001b[0m\u001b[1m\u001b[38;5;40m are\u001b[0m\u001b[1m\u001b[38;5;40m two\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m based\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m1\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhat\u001b[0m\u001b[1m\u001b[38;5;40m technology\u001b[0m\u001b[1m\u001b[38;5;40m was\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m develop\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m iOS\u001b[0m\u001b[1m\u001b[38;5;40m app\u001b[0m\u001b[1m\u001b[38;5;40m for\u001b[0m\u001b[1m\u001b[38;5;40m J\u001b[0m\u001b[1m\u001b[38;5;40mABA\u001b[0m\u001b[1m\u001b[38;5;40m AI\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mA\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m React\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mB\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Node\u001b[0m\u001b[1m\u001b[38;5;40m.js\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mC\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Expo\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mD\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Firebase\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhat\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m a\u001b[0m\u001b[1m\u001b[38;5;40m key\u001b[0m\u001b[1m\u001b[38;5;40m feature\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m Grad\u001b[0m\u001b[1m\u001b[38;5;40mu\u001b[0m\u001b[1m\u001b[38;5;40mapp\u001b[0m\u001b[1m\u001b[38;5;40m mobile\u001b[0m\u001b[1m\u001b[38;5;40m app\u001b[0m\u001b[1m\u001b[38;5;40m that\u001b[0m\u001b[1m\u001b[38;5;40m allows\u001b[0m\u001b[1m\u001b[38;5;40m school\u001b[0m\u001b[1m\u001b[38;5;40m administrators\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m manage\u001b[0m\u001b[1m\u001b[38;5;40m content\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mA\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Real\u001b[0m\u001b[1m\u001b[38;5;40m-time\u001b[0m\u001b[1m\u001b[38;5;40m feeds\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mB\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Tag\u001b[0m\u001b[1m\u001b[38;5;40mging\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m Comments\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mC\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Gallery\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m Notifications\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mD\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Admin\u001b[0m\u001b[1m\u001b[38;5;40m site\u001b[0m\u001b[1m\u001b[38;5;40m for\u001b[0m\u001b[1m\u001b[38;5;40m managing\u001b[0m\u001b[1m\u001b[38;5;40m content\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:42<00:54, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m400\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2142\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m11.43s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m47.66ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.07s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m8.31s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m355\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m98\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m453\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m368\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1736\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m368\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mHere\u001b[0m\u001b[1m\u001b[38;5;40m are\u001b[0m\u001b[1m\u001b[38;5;40m two\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m based\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m1\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhat\u001b[0m\u001b[1m\u001b[38;5;40m technology\u001b[0m\u001b[1m\u001b[38;5;40m was\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m J\u001b[0m\u001b[1m\u001b[38;5;40mules\u001b[0m\u001b[1m\u001b[38;5;40m Proc\u001b[0m\u001b[1m\u001b[38;5;40mure\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m build\u001b[0m\u001b[1m\u001b[38;5;40m its\u001b[0m\u001b[1m\u001b[38;5;40m enterprise\u001b[0m\u001b[1m\u001b[38;5;40m web\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m mobile\u001b[0m\u001b[1m\u001b[38;5;40m CRM\u001b[0m\u001b[1m\u001b[38;5;40m apps\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mA\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m React\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m React\u001b[0m\u001b[1m\u001b[38;5;40m Native\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m AWS\u001b[0m\u001b[1m\u001b[38;5;40m Lamb\u001b[0m\u001b[1m\u001b[38;5;40mdas\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m GraphQL\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Docker\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Server\u001b[0m\u001b[1m\u001b[38;5;40mless\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mB\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Angular\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Vue\u001b[0m\u001b[1m\u001b[38;5;40m.js\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Node\u001b[0m\u001b[1m\u001b[38;5;40m.js\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Express\u001b[0m\u001b[1m\u001b[38;5;40m.js\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m MongoDB\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Redis\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mC\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Python\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Flask\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Django\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m PostgreSQL\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m MySQL\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Apache\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mAnswer\u001b[0m\u001b[1m\u001b[38;5;40m:**\u001b[0m\u001b[1m\u001b[38;5;40m A\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m React\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m React\u001b[0m\u001b[1m\u001b[38;5;40m Native\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m AWS\u001b[0m\u001b[1m\u001b[38;5;40m Lamb\u001b[0m\u001b[1m\u001b[38;5;40mdas\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m GraphQL\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Docker\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m Server\u001b[0m\u001b[1m\u001b[38;5;40mless\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhat\u001b[0m\u001b[1m\u001b[38;5;40m feature\u001b[0m\u001b[1m\u001b[38;5;40m was\u001b[0m\u001b[1m\u001b[38;5;40m developed\u001b[0m\u001b[1m\u001b[38;5;40m for\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m Digital\u001b[0m\u001b[1m\u001b[38;5;40m Cities\u001b[0m\u001b[1m\u001b[38;5;40m PH\u001b[0m\u001b[1m\u001b[38;5;40m portal\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m enable\u001b[0m\u001b[1m\u001b[38;5;40m users\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m access\u001b[0m\u001b[1m\u001b[38;5;40m detailed\u001b[0m\u001b[1m\u001b[38;5;40m descriptions\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m statistics\u001b[0m\u001b[1m\u001b[38;5;40m for\u001b[0m\u001b[1m\u001b[38;5;40m each\u001b[0m\u001b[1m\u001b[38;5;40m region\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mA\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Filtering\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mB\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Search\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mC\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Interactive\u001b[0m\u001b[1m\u001b[38;5;40m map\u001b[0m\u001b[1m\u001b[38;5;40m with\u001b[0m\u001b[1m\u001b[38;5;40m clickable\u001b[0m\u001b[1m\u001b[38;5;40m provinces\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mD\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Automated\u001b[0m\u001b[1m\u001b[38;5;40m emails\u001b[0m\u001b[1m\u001b[38;5;40m based\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m triggers\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mAnswer\u001b[0m\u001b[1m\u001b[38;5;40m:**\u001b[0m\u001b[1m\u001b[38;5;40m C\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Interactive\u001b[0m\u001b[1m\u001b[38;5;40m map\u001b[0m\u001b[1m\u001b[38;5;40m with\u001b[0m\u001b[1m\u001b[38;5;40m clickable\u001b[0m\u001b[1m\u001b[38;5;40m provinces\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [01:01<00:47, 15.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m734\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m2470\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m17.37s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m50.38ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m3.06s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m14.26s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m344\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m167\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m511\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m206\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1003\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m206\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mHere\u001b[0m\u001b[1m\u001b[38;5;40m are\u001b[0m\u001b[1m\u001b[38;5;40m two\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m based\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m given\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m1\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhat\u001b[0m\u001b[1m\u001b[38;5;40m programming\u001b[0m\u001b[1m\u001b[38;5;40m languages\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m frameworks\u001b[0m\u001b[1m\u001b[38;5;40m does\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m candidate\u001b[0m\u001b[1m\u001b[38;5;40m have\u001b[0m\u001b[1m\u001b[38;5;40m experience\u001b[0m\u001b[1m\u001b[38;5;40m with\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m aside\u001b[0m\u001b[1m\u001b[38;5;40m from\u001b[0m\u001b[1m\u001b[38;5;40m React\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m React\u001b[0m\u001b[1m\u001b[38;5;40m Native\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m Node\u001b[0m\u001b[1m\u001b[38;5;40m.js\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhat\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m one\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m goals\u001b[0m\u001b[1m\u001b[38;5;40m that\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m candidate\u001b[0m\u001b[1m\u001b[38;5;40m has\u001b[0m\u001b[1m\u001b[38;5;40m committed\u001b[0m\u001b[1m\u001b[38;5;40m to\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m their\u001b[0m\u001b[1m\u001b[38;5;40m job\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [01:09<00:25, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;40m pitch\u001b[0m\u001b[1m\u001b[38;5;40m?\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m291\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m1294\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m6.80s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m45.04ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m1.72s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m5.04s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m185\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m61\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m246\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m231\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m913\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m231\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mHere\u001b[0m\u001b[1m\u001b[38;5;40m are\u001b[0m\u001b[1m\u001b[38;5;40m two\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m based\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m1\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhat\u001b[0m\u001b[1m\u001b[38;5;40m programming\u001b[0m\u001b[1m\u001b[38;5;40m language\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m used\u001b[0m\u001b[1m\u001b[38;5;40m by\u001b[0m\u001b[1m\u001b[38;5;40m Node\u001b[0m\u001b[1m\u001b[38;5;40m.js\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m as\u001b[0m\u001b[1m\u001b[38;5;40m mentioned\u001b[0m\u001b[1m\u001b[38;5;40m in\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m Backend\u001b[0m\u001b[1m\u001b[38;5;40m section\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m skills\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mA\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Python\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mB\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m JavaScript\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mC\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m TypeScript\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mD\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Java\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhich\u001b[0m\u001b[1m\u001b[38;5;40m platform\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m listed\u001b[0m\u001b[1m\u001b[38;5;40m among\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m Developer\u001b[0m\u001b[1m\u001b[38;5;40m Tools\u001b[0m\u001b[1m\u001b[38;5;40m,\u001b[0m\u001b[1m\u001b[38;5;40m alongside\u001b[0m\u001b[1m\u001b[38;5;40m Photoshop\u001b[0m\u001b[1m\u001b[38;5;40m and\u001b[0m\u001b[1m\u001b[38;5;40m Jest\u001b[0m\u001b[1m\u001b[38;5;40m (\u001b[0m\u001b[1m\u001b[38;5;40mUnit\u001b[0m\u001b[1m\u001b[38;5;40m testing\u001b[0m\u001b[1m\u001b[38;5;40m)?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mA\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Jenkins\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mB\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Firebase\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mC\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m AWS\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mD\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [01:20<00:12, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Git\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m367\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m1280\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m9.09s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m35.84ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m1.72s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m7.34s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m205\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m88\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m293\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;208mCalling Ollama chat...\u001b[0m\n",
      "\u001b[38;5;250mLLM model:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m \u001b[1m\u001b[38;5;213m(4096)\u001b[0m \u001b[38;5;250m|\u001b[0m \u001b[1m\u001b[38;5;213mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m294\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;250mModel:\u001b[0m \u001b[1m\u001b[38;5;213mllama3.1\u001b[0m\n",
      "\u001b[38;5;250mPrompt:\u001b[0m \u001b[1m\u001b[38;5;213m1116\u001b[0m\n",
      "\u001b[38;5;250mTokens:\u001b[0m \u001b[1m\u001b[38;5;213m294\u001b[0m\n",
      "\u001b[38;5;250mStream:\u001b[0m \u001b[1m\u001b[38;5;213mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mGenerating response...\u001b[0m\n",
      "\u001b[1m\u001b[38;5;208mEvent: call_ollama_chat\u001b[0m\n",
      "\u001b[38;5;250mFile:\u001b[0m \u001b[1m\u001b[38;5;208mipykernel_launcher.py\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mLog-Filename:\u001b[0m \u001b[1m\u001b[38;5;45mipykernel_launcher\u001b[0m\n",
      "\u001b[1m\u001b[38;5;40mHere\u001b[0m\u001b[1m\u001b[38;5;40m are\u001b[0m\u001b[1m\u001b[38;5;40m two\u001b[0m\u001b[1m\u001b[38;5;40m questions\u001b[0m\u001b[1m\u001b[38;5;40m based\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m context\u001b[0m\u001b[1m\u001b[38;5;40m information\u001b[0m\u001b[1m\u001b[38;5;40m:\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m1\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhat\u001b[0m\u001b[1m\u001b[38;5;40m is\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m name\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m website\u001b[0m\u001b[1m\u001b[38;5;40m that\u001b[0m\u001b[1m\u001b[38;5;40m showcases\u001b[0m\u001b[1m\u001b[38;5;40m J\u001b[0m\u001b[1m\u001b[38;5;40mules\u001b[0m\u001b[1m\u001b[38;5;40m Proc\u001b[0m\u001b[1m\u001b[38;5;40mure\u001b[0m\u001b[1m\u001b[38;5;40m's\u001b[0m\u001b[1m\u001b[38;5;40m portfolio\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mA\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m https\u001b[0m\u001b[1m\u001b[38;5;40m://\u001b[0m\u001b[1m\u001b[38;5;40mwww\u001b[0m\u001b[1m\u001b[38;5;40m.j\u001b[0m\u001b[1m\u001b[38;5;40mules\u001b[0m\u001b[1m\u001b[38;5;40mai\u001b[0m\u001b[1m\u001b[38;5;40m.com\u001b[0m\u001b[1m\u001b[38;5;40m/#\u001b[0m\u001b[1m\u001b[38;5;40mproc\u001b[0m\u001b[1m\u001b[38;5;40mure\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mB\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m https\u001b[0m\u001b[1m\u001b[38;5;40m://\u001b[0m\u001b[1m\u001b[38;5;40mwww\u001b[0m\u001b[1m\u001b[38;5;40m.digital\u001b[0m\u001b[1m\u001b[38;5;40mcities\u001b[0m\u001b[1m\u001b[38;5;40mph\u001b[0m\u001b[1m\u001b[38;5;40m.com\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mC\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m https\u001b[0m\u001b[1m\u001b[38;5;40m://\u001b[0m\u001b[1m\u001b[38;5;40mwww\u001b[0m\u001b[1m\u001b[38;5;40m.ad\u001b[0m\u001b[1m\u001b[38;5;40mec\u001b[0m\u001b[1m\u001b[38;5;40m-\u001b[0m\u001b[1m\u001b[38;5;40mken\u001b[0m\u001b[1m\u001b[38;5;40mya\u001b[0m\u001b[1m\u001b[38;5;40m.com\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mD\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m https\u001b[0m\u001b[1m\u001b[38;5;40m://\u001b[0m\u001b[1m\u001b[38;5;40mj\u001b[0m\u001b[1m\u001b[38;5;40maba\u001b[0m\u001b[1m\u001b[38;5;40m.live\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mAnswer\u001b[0m\u001b[1m\u001b[38;5;40m:\u001b[0m\u001b[1m\u001b[38;5;40m A\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m https\u001b[0m\u001b[1m\u001b[38;5;40m://\u001b[0m\u001b[1m\u001b[38;5;40mwww\u001b[0m\u001b[1m\u001b[38;5;40m.j\u001b[0m\u001b[1m\u001b[38;5;40mules\u001b[0m\u001b[1m\u001b[38;5;40mai\u001b[0m\u001b[1m\u001b[38;5;40m.com\u001b[0m\u001b[1m\u001b[38;5;40m/#\u001b[0m\u001b[1m\u001b[38;5;40mproc\u001b[0m\u001b[1m\u001b[38;5;40mure\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40m**\u001b[0m\u001b[1m\u001b[38;5;40mQuestion\u001b[0m\u001b[1m\u001b[38;5;40m \u001b[0m\u001b[1m\u001b[38;5;40m2\u001b[0m\u001b[1m\u001b[38;5;40m**\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mWhich\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m\u001b[1m\u001b[38;5;40m following\u001b[0m\u001b[1m\u001b[38;5;40m apps\u001b[0m\u001b[1m\u001b[38;5;40m has\u001b[0m\u001b[1m\u001b[38;5;40m an\u001b[0m\u001b[1m\u001b[38;5;40m Android\u001b[0m\u001b[1m\u001b[38;5;40m link\u001b[0m\u001b[1m\u001b[38;5;40m on\u001b[0m\u001b[1m\u001b[38;5;40m Google\u001b[0m\u001b[1m\u001b[38;5;40m Play\u001b[0m\u001b[1m\u001b[38;5;40m Store\u001b[0m\u001b[1m\u001b[38;5;40m?\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mA\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m Grad\u001b[0m\u001b[1m\u001b[38;5;40mu\u001b[0m\u001b[1m\u001b[38;5;40mapp\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mB\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m EZ\u001b[0m\u001b[1m\u001b[38;5;40m My\u001b[0m\u001b[1m\u001b[38;5;40moma\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mC\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m J\u001b[0m\u001b[1m\u001b[38;5;40mABA\u001b[0m\u001b[1m\u001b[38;5;40m AI\u001b[0m\u001b[1m\u001b[38;5;40m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;40mD\u001b[0m\u001b[1m\u001b[38;5;40m)\u001b[0m\u001b[1m\u001b[38;5;40m All\u001b[0m\u001b[1m\u001b[38;5;40m of\u001b[0m\u001b[1m\u001b[38;5;40m the\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:35<00:00, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;40m above\u001b[0m\u001b[1m\u001b[38;5;40m\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;15mModel:\u001b[0m \u001b[1m\u001b[38;5;45mllama3.1\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mOptions:\u001b[0m \u001b[1m\u001b[38;5;45m{'num_ctx': 3900, 'seed': 42, 'temperature': 0.0, 'num_keep': 0, 'num_predict': -1}\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mStream:\u001b[0m \u001b[1m\u001b[38;5;45mTrue\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mResponse:\u001b[0m \u001b[1m\u001b[38;5;45m459\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mContent:\u001b[0m \u001b[1m\u001b[38;5;45m1575\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mDurations:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mtotal_duration:\u001b[0m \u001b[1m\u001b[38;5;220m13.57s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mload_duration:\u001b[0m \u001b[1m\u001b[38;5;82m35.86ms\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15mprompt_eval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m2.28s\u001b[0m\n",
      "\u001b[1m\u001b[38;5;15meval_duration:\u001b[0m \u001b[1m\u001b[38;5;220m11.25s\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[38;5;213mFinal tokens info:\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mPrompt tokens:\u001b[0m \u001b[1m\u001b[38;5;40m267\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mResponse tokens:\u001b[0m \u001b[1m\u001b[38;5;40m134\u001b[0m\n",
      "\u001b[1m\u001b[38;5;45mTotal tokens:\u001b[0m \u001b[1m\u001b[38;5;40m401\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes, llm=gpt4, num_questions_per_chunk=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Question 1**\n"
     ]
    }
   ],
   "source": [
    "queries = qa_dataset.queries.values()\n",
    "print(list(queries)[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "Query: Here are two questions based on the context information:\n",
      "Metrics: {'mrr': 0.0, 'hit_rate': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try it out on a sample query\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[0]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# try it out on an entire dataset\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"retrievers\": [name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top-2 eval</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retrievers  hit_rate       mrr\n",
       "0  top-2 eval  0.357143  0.285714"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results(\"top-2 eval\", eval_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
